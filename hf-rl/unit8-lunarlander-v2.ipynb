{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!apt install python-opengl -y\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyglet==1.5\n!pip3 install pyvirtualdisplay","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-06T21:13:40.117058Z","iopub.execute_input":"2023-04-06T21:13:40.117498Z","iopub.status.idle":"2023-04-06T21:14:10.643589Z","shell.execute_reply.started":"2023-04-06T21:13:40.117456Z","shell.execute_reply":"2023-04-06T21:14:10.642315Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-04-06T21:13:00.183408Z","iopub.execute_input":"2023-04-06T21:13:00.184654Z","iopub.status.idle":"2023-04-06T21:13:00.639341Z","shell.execute_reply.started":"2023-04-06T21:13:00.184583Z","shell.execute_reply":"2023-04-06T21:13:00.637882Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<pyvirtualdisplay.display.Display at 0x75da1bf2a250>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install gym==0.21\n!pip install imageio-ffmpeg\n!pip install huggingface_hub\n!pip install box2d","metadata":{"execution":{"iopub.status.busy":"2023-04-06T21:13:00.640787Z","iopub.execute_input":"2023-04-06T21:13:00.641127Z","iopub.status.idle":"2023-04-06T21:13:40.114996Z","shell.execute_reply.started":"2023-04-06T21:13:00.641095Z","shell.execute_reply":"2023-04-06T21:13:40.113850Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting gym==0.21\n  Downloading gym-0.21.0.tar.gz (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.21) (1.21.6)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.21) (2.2.1)\nRequirement already satisfied: importlib_metadata>=4.8.1 in /opt/conda/lib/python3.7/site-packages (from gym==0.21) (4.11.4)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib_metadata>=4.8.1->gym==0.21) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib_metadata>=4.8.1->gym==0.21) (3.11.0)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616825 sha256=85556500e241bda36b06bac9fb71e0befd844241450a8dd77c77e363858eb078\n  Stored in directory: /root/.cache/pip/wheels/d3/78/02/af51e23f21c31c0167d288296d764a22abb842ac6e8f52ebfa\nSuccessfully built gym\nInstalling collected packages: gym\n  Attempting uninstall: gym\n    Found existing installation: gym 0.23.1\n    Uninstalling gym-0.23.1:\n      Successfully uninstalled gym-0.23.1\nSuccessfully installed gym-0.21.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting imageio-ffmpeg\n  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: imageio-ffmpeg\nSuccessfully installed imageio-ffmpeg-0.4.8\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.7/site-packages (0.13.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (2.28.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.4.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.64.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (23.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (1.26.14)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting box2d\n  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: box2d\nSuccessfully installed box2d-2.3.10\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi, upload_folder\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\nmsg = Printer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding HuggingFace argument\nparser.add_argument(\"--repo-id\", type=str, default=\"bsenst/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def package_to_hub(repo_id, \n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30,\n                commit_message=\"Push agent to the Hub\",\n                token= None,\n                logs=None\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the hub\n  :param repo_id: id of the model repository from the Hugging Face Hub\n  :param model: trained model\n  :param eval_env: environment used to evaluate the agent\n  :param fps: number of fps for rendering the video\n  :param commit_message: commit message\n  :param logs: directory on local machine of tensorboard logs you'd like to upload\n  \"\"\"\n  msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n  # Step 1: Clone or create the repo\n  repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n  \n  with tempfile.TemporaryDirectory() as tmpdirname:\n    tmpdirname = Path(tmpdirname)\n\n    # Step 2: Save the model\n    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n  \n    # Step 3: Evaluate the model and build JSON\n    mean_reward, std_reward = _evaluate_agent(eval_env, \n                                           10, \n                                           model)\n\n    # First get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n        \"env_id\": hyperparameters.env_id, \n        \"mean_reward\": mean_reward,\n        \"std_reward\": std_reward,\n        \"n_evaluation_episodes\": 10,\n        \"eval_datetime\": eval_form_datetime,\n    }\n \n    # Write a JSON file\n    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n      json.dump(evaluate_data, outfile)\n\n    # Step 4: Generate a video\n    video_path =  tmpdirname / \"replay.mp4\"\n    record_video(eval_env, model, video_path, video_fps)\n  \n    # Step 5: Generate the model card\n    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n    _save_model_card(tmpdirname, generated_model_card, metadata)\n\n    # Step 6: Add logs if needed\n    if logs:\n      _add_logdir(tmpdirname, Path(logs))\n  \n    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n  \n    repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n  return repo_url\n\n\ndef _evaluate_agent(env, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    while done is False:\n      state = torch.Tensor(state).to(device)\n      action, _, _, _ = policy.get_action_and_value(state)\n      new_state, reward, done, info = env.step(action.cpu().numpy())\n      total_rewards_ep += reward    \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward\n\n\ndef record_video(env, policy, out_directory, fps=30):\n  images = []  \n  done = False\n  state = env.reset()\n  img = env.render(mode='rgb_array')\n  images.append(img)\n  while not done:\n    state = torch.Tensor(state).to(device)\n    # Take the action (index) that have the maximum expected future reward given that state\n    action, _, _, _  = policy.get_action_and_value(state)\n    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n    img = env.render(mode='rgb_array')\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n\n\ndef _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n  \"\"\"\n  Generate the model card for the Hub\n  :param model_name: name of the model\n  :env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  :hyperparameters: training arguments\n  \"\"\"\n  # Step 1: Select the tags\n  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n  # Transform the hyperparams namespace to string\n  converted_dict = vars(hyperparameters)\n  converted_str = str(converted_dict)\n  converted_str = converted_str.split(\", \")\n  converted_str = '\\n'.join(converted_str)\n \n  # Step 2: Generate the model card\n  model_card = f\"\"\"\n  # PPO Agent Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n    \n  # Hyperparameters\n  ```python\n  {converted_str}\n  ```\n  \"\"\"\n  return model_card, metadata\n\n\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n  \"\"\"\n  Define the tags for the model card\n  :param model_name: name of the model\n  :param env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  \"\"\"\n  metadata = {}\n  metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\"\n  ]\n\n  # Add metrics\n  eval = metadata_eval_result(\n      model_pretty_name=model_name,\n      task_pretty_name=\"reinforcement-learning\",\n      task_id=\"reinforcement-learning\",\n      metrics_pretty_name=\"mean_reward\",\n      metrics_id=\"mean_reward\",\n      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n      dataset_pretty_name=env_id,\n      dataset_id=env_id,\n  )\n\n  # Merges both dictionaries\n  metadata = {**metadata, **eval}\n\n  return metadata\n\n\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n\ndef _add_logdir(local_path: Path, logdir: Path):\n  \"\"\"Adds a logdir to the repository.\n  :param local_path: repository directory\n  :param logdir: logdir directory\n  \"\"\"\n  if logdir.exists() and logdir.is_dir():\n    # Add the logdir to the repository under new dir called logs\n    repo_logdir = local_path / \"logs\"\n    \n    # Delete current logs if they exist\n    if repo_logdir.exists():\n      shutil.rmtree(repo_logdir)\n\n    # Copy logdir into repo logdir\n    shutil.copytree(logdir, repo_logdir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy\n\nimport argparse\nimport os\nimport random\nimport time\nfrom distutils.util import strtobool\n\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions.categorical import Categorical\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom huggingface_hub import HfApi, upload_folder\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\nmsg = Printer()\n\ndef parse_args():\n    # fmt: off\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n        help=\"the name of this experiment\")\n    parser.add_argument(\"--seed\", type=int, default=1,\n        help=\"seed of the experiment\")\n    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, cuda will be enabled by default\")\n    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n        help=\"the wandb's project name\")\n    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n        help=\"the entity (team) of wandb's project\")\n    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n\n    # Algorithm specific arguments\n    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n        help=\"the id of the environment\")\n    parser.add_argument(\"--total-timesteps\", type=int, default=50000,\n        help=\"total timesteps of the experiments\")\n    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n        help=\"the learning rate of the optimizer\")\n    parser.add_argument(\"--num-envs\", type=int, default=4,\n        help=\"the number of parallel game environments\")\n    parser.add_argument(\"--num-steps\", type=int, default=128,\n        help=\"the number of steps to run in each environment per policy rollout\")\n    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggle learning rate annealing for policy and value networks\")\n    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Use GAE for advantage computation\")\n    parser.add_argument(\"--gamma\", type=float, default=0.99,\n        help=\"the discount factor gamma\")\n    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n        help=\"the lambda for the general advantage estimation\")\n    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n        help=\"the number of mini-batches\")\n    parser.add_argument(\"--update-epochs\", type=int, default=4,\n        help=\"the K epochs to update the policy\")\n    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles advantages normalization\")\n    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n        help=\"the surrogate clipping coefficient\")\n    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n        help=\"coefficient of the entropy\")\n    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n        help=\"coefficient of the value function\")\n    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n        help=\"the maximum norm for the gradient clipping\")\n    parser.add_argument(\"--target-kl\", type=float, default=None,\n        help=\"the target KL divergence threshold\")\n    \n    # Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args\n\ndef package_to_hub(repo_id, \n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30,\n                commit_message=\"Push agent to the Hub\",\n                token= None,\n                logs=None\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the hub\n  :param repo_id: id of the model repository from the Hugging Face Hub\n  :param model: trained model\n  :param eval_env: environment used to evaluate the agent\n  :param fps: number of fps for rendering the video\n  :param commit_message: commit message\n  :param logs: directory on local machine of tensorboard logs you'd like to upload\n  \"\"\"\n  msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n  # Step 1: Clone or create the repo\n  repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n  \n  with tempfile.TemporaryDirectory() as tmpdirname:\n    tmpdirname = Path(tmpdirname)\n\n    # Step 2: Save the model\n    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n  \n    # Step 3: Evaluate the model and build JSON\n    mean_reward, std_reward = _evaluate_agent(eval_env, \n                                           10, \n                                           model)\n\n    # First get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n        \"env_id\": hyperparameters.env_id, \n        \"mean_reward\": mean_reward,\n        \"std_reward\": std_reward,\n        \"n_evaluation_episodes\": 10,\n        \"eval_datetime\": eval_form_datetime,\n    }\n \n    # Write a JSON file\n    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n      json.dump(evaluate_data, outfile)\n\n    # Step 4: Generate a video\n    video_path =  tmpdirname / \"replay.mp4\"\n    record_video(eval_env, model, video_path, video_fps)\n  \n    # Step 5: Generate the model card\n    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n    _save_model_card(tmpdirname, generated_model_card, metadata)\n\n    # Step 6: Add logs if needed\n    if logs:\n      _add_logdir(tmpdirname, Path(logs))\n  \n    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n  \n    repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n  return repo_url\n\ndef _evaluate_agent(env, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    while done is False:\n      state = torch.Tensor(state).to(device)\n      action, _, _, _ = policy.get_action_and_value(state)\n      new_state, reward, done, info = env.step(action.cpu().numpy())\n      total_rewards_ep += reward    \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward\n\n\ndef record_video(env, policy, out_directory, fps=30):\n  images = []  \n  done = False\n  state = env.reset()\n  img = env.render(mode='rgb_array')\n  images.append(img)\n  while not done:\n    state = torch.Tensor(state).to(device)\n    # Take the action (index) that have the maximum expected future reward given that state\n    action, _, _, _  = policy.get_action_and_value(state)\n    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n    img = env.render(mode='rgb_array')\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n\n\ndef _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n  \"\"\"\n  Generate the model card for the Hub\n  :param model_name: name of the model\n  :env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  :hyperparameters: training arguments\n  \"\"\"\n  # Step 1: Select the tags\n  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n  # Transform the hyperparams namespace to string\n  converted_dict = vars(hyperparameters)\n  converted_str = str(converted_dict)\n  converted_str = converted_str.split(\", \")\n  converted_str = '\\n'.join(converted_str)\n \n  # Step 2: Generate the model card\n  model_card = f\"\"\"\n  # PPO Agent Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n    \n  # Hyperparameters\n  ```python\n  {converted_str}\n  ```\n  \"\"\"\n  return model_card, metadata\n\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n  \"\"\"\n  Define the tags for the model card\n  :param model_name: name of the model\n  :param env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  \"\"\"\n  metadata = {}\n  metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\"\n  ]\n\n  # Add metrics\n  eval = metadata_eval_result(\n      model_pretty_name=model_name,\n      task_pretty_name=\"reinforcement-learning\",\n      task_id=\"reinforcement-learning\",\n      metrics_pretty_name=\"mean_reward\",\n      metrics_id=\"mean_reward\",\n      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n      dataset_pretty_name=env_id,\n      dataset_id=env_id,\n  )\n\n  # Merges both dictionaries\n  metadata = {**metadata, **eval}\n\n  return metadata\n\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\ndef _add_logdir(local_path: Path, logdir: Path):\n  \"\"\"Adds a logdir to the repository.\n  :param local_path: repository directory\n  :param logdir: logdir directory\n  \"\"\"\n  if logdir.exists() and logdir.is_dir():\n    # Add the logdir to the repository under new dir called logs\n    repo_logdir = local_path / \"logs\"\n    \n    # Delete current logs if they exist\n    if repo_logdir.exists():\n      shutil.rmtree(repo_logdir)\n\n    # Copy logdir into repo logdir\n    shutil.copytree(logdir, repo_logdir)\n\ndef make_env(env_id, seed, idx, capture_video, run_name):\n    def thunk():\n        env = gym.make(env_id)\n        env = gym.wrappers.RecordEpisodeStatistics(env)\n        if capture_video:\n            if idx == 0:\n                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n        env.seed(seed)\n        env.action_space.seed(seed)\n        env.observation_space.seed(seed)\n        return env\n\n    return thunk\n\n\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\n\nclass Agent(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 1), std=1.0),\n        )\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n        )\n\n    def get_value(self, x):\n        return self.critic(x)\n\n    def get_action_and_value(self, x, action=None):\n        logits = self.actor(x)\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    if args.track:\n        import wandb\n\n        wandb.init(\n            project=args.wandb_project_name,\n            entity=args.wandb_entity,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    writer = SummaryWriter(f\"runs/{run_name}\")\n    writer.add_text(\n        \"hyperparameters\",\n        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n    )\n\n    # TRY NOT TO MODIFY: seeding\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n\n    # env setup\n    envs = gym.vector.SyncVectorEnv(\n        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n    )\n    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n\n    agent = Agent(envs).to(device)\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n\n    # ALGO Logic: Storage setup\n    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n\n    # TRY NOT TO MODIFY: start the game\n    global_step = 0\n    start_time = time.time()\n    next_obs = torch.Tensor(envs.reset()).to(device)\n    next_done = torch.zeros(args.num_envs).to(device)\n    num_updates = args.total_timesteps // args.batch_size\n\n    for update in range(1, num_updates + 1):\n        # Annealing the rate if instructed to do so.\n        if args.anneal_lr:\n            frac = 1.0 - (update - 1.0) / num_updates\n            lrnow = frac * args.learning_rate\n            optimizer.param_groups[0][\"lr\"] = lrnow\n\n        for step in range(0, args.num_steps):\n            global_step += 1 * args.num_envs\n            obs[step] = next_obs\n            dones[step] = next_done\n\n            # ALGO LOGIC: action logic\n            with torch.no_grad():\n                action, logprob, _, value = agent.get_action_and_value(next_obs)\n                values[step] = value.flatten()\n            actions[step] = action\n            logprobs[step] = logprob\n\n            # TRY NOT TO MODIFY: execute the game and log data.\n            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n            rewards[step] = torch.tensor(reward).to(device).view(-1)\n            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n\n            for item in info:\n                if \"episode\" in item.keys():\n                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n                    break\n\n        # bootstrap value if not done\n        with torch.no_grad():\n            next_value = agent.get_value(next_obs).reshape(1, -1)\n            if args.gae:\n                advantages = torch.zeros_like(rewards).to(device)\n                lastgaelam = 0\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        nextvalues = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        nextvalues = values[t + 1]\n                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n                returns = advantages + values\n            else:\n                returns = torch.zeros_like(rewards).to(device)\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        next_return = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        next_return = returns[t + 1]\n                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n                advantages = returns - values\n\n        # flatten the batch\n        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n        b_logprobs = logprobs.reshape(-1)\n        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n        b_advantages = advantages.reshape(-1)\n        b_returns = returns.reshape(-1)\n        b_values = values.reshape(-1)\n\n        # Optimizing the policy and value network\n        b_inds = np.arange(args.batch_size)\n        clipfracs = []\n        for epoch in range(args.update_epochs):\n            np.random.shuffle(b_inds)\n            for start in range(0, args.batch_size, args.minibatch_size):\n                end = start + args.minibatch_size\n                mb_inds = b_inds[start:end]\n\n                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n                logratio = newlogprob - b_logprobs[mb_inds]\n                ratio = logratio.exp()\n\n                with torch.no_grad():\n                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n                    old_approx_kl = (-logratio).mean()\n                    approx_kl = ((ratio - 1) - logratio).mean()\n                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n\n                mb_advantages = b_advantages[mb_inds]\n                if args.norm_adv:\n                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n\n                # Policy loss\n                pg_loss1 = -mb_advantages * ratio\n                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n\n                # Value loss\n                newvalue = newvalue.view(-1)\n                if args.clip_vloss:\n                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n                    v_clipped = b_values[mb_inds] + torch.clamp(\n                        newvalue - b_values[mb_inds],\n                        -args.clip_coef,\n                        args.clip_coef,\n                    )\n                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n                    v_loss = 0.5 * v_loss_max.mean()\n                else:\n                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n\n                entropy_loss = entropy.mean()\n                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n                optimizer.step()\n\n            if args.target_kl is not None:\n                if approx_kl > args.target_kl:\n                    break\n\n        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n        var_y = np.var(y_true)\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n\n        # TRY NOT TO MODIFY: record rewards for plotting purposes\n        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n\n    envs.close()\n    writer.close()\n\n    # Create the evaluation environment\n    eval_env = gym.make(args.env_id)\n\n    package_to_hub(repo_id = args.repo_id,\n                model = agent, # The model we want to save\n                hyperparameters = args,\n                eval_env = gym.make(args.env_id),\n                logs= f\"runs/{run_name}\",\n                )\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ppo.py --env-id=\"LunarLander-v2\" --repo-id=\"YOUR_REPO_ID\" --total-timesteps=50000","metadata":{},"execution_count":null,"outputs":[]}]}