{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt install python-opengl -y","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-13T02:54:44.110126Z","iopub.execute_input":"2023-04-13T02:54:44.110894Z","iopub.status.idle":"2023-04-13T02:54:58.554953Z","shell.execute_reply.started":"2023-04-13T02:54:44.110852Z","shell.execute_reply":"2023-04-13T02:54:58.553757Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  freeglut3 libglu1-mesa libpython2-stdlib libpython2.7-minimal\n  libpython2.7-stdlib python2 python2-minimal python2.7 python2.7-minimal\nSuggested packages:\n  python-tk python-numpy libgle3 python2-doc python2.7-doc binfmt-support\nThe following NEW packages will be installed:\n  freeglut3 libglu1-mesa libpython2-stdlib libpython2.7-minimal\n  libpython2.7-stdlib python-opengl python2 python2-minimal python2.7\n  python2.7-minimal\n0 upgraded, 10 newly installed, 0 to remove and 76 not upgraded.\nNeed to get 4540 kB of archives.\nAfter this operation, 22.7 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7-minimal amd64 2.7.18-1~20.04.3 [336 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python2.7-minimal amd64 2.7.18-1~20.04.3 [1280 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]\nGet:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7-stdlib amd64 2.7.18-1~20.04.3 [1888 kB]\nGet:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python2.7 amd64 2.7.18-1~20.04.3 [248 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7072 B]\nGet:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]\nGet:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libglu1-mesa amd64 9.0.1-1build1 [168 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-opengl all 3.1.0+dfsg-2build1 [486 kB]\nFetched 4540 kB in 2s (2724 kB/s)      \u001b[0m\u001b[33m\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libpython2.7-minimal:amd64.\n(Reading database ... 111522 files and directories currently installed.)\nPreparing to unpack .../0-libpython2.7-minimal_2.7.18-1~20.04.3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libpython2.7-minimal:amd64 (2.7.18-1~20.04.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package python2.7-minimal.\nPreparing to unpack .../1-python2.7-minimal_2.7.18-1~20.04.3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking python2.7-minimal (2.7.18-1~20.04.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package python2-minimal.\nPreparing to unpack .../2-python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libpython2.7-stdlib:amd64.\nPreparing to unpack .../3-libpython2.7-stdlib_2.7.18-1~20.04.3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libpython2.7-stdlib:amd64 (2.7.18-1~20.04.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package python2.7.\nPreparing to unpack .../4-python2.7_2.7.18-1~20.04.3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking python2.7 (2.7.18-1~20.04.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libpython2-stdlib:amd64.\nPreparing to unpack .../5-libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Setting up libpython2.7-minimal:amd64 (2.7.18-1~20.04.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Setting up python2.7-minimal (2.7.18-1~20.04.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Linking and byte-compiling packages for runtime python2.7...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Setting up python2-minimal (2.7.17-2ubuntu4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [########################..................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package python2.\n(Reading database ... 112269 files and directories currently installed.)\nPreparing to unpack .../python2_2.7.17-2ubuntu4_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking python2 (2.7.17-2ubuntu4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Selecting previously unselected package freeglut3:amd64.\nPreparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Unpacking freeglut3:amd64 (2.8.1-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Selecting previously unselected package libglu1-mesa:amd64.\nPreparing to unpack .../libglu1-mesa_9.0.1-1build1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Unpacking libglu1-mesa:amd64 (9.0.1-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Selecting previously unselected package python-opengl.\nPreparing to unpack .../python-opengl_3.1.0+dfsg-2build1_all.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Unpacking python-opengl (3.1.0+dfsg-2build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up freeglut3:amd64 (2.8.1-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libpython2.7-stdlib:amd64 (2.7.18-1~20.04.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libglu1-mesa:amd64 (9.0.1-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up python2.7 (2.7.18-1~20.04.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up python2 (2.7.17-2ubuntu4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up python-opengl (3.1.0+dfsg-2build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for man-db (2.9.1-1) ...\nProcessing triggers for mime-support (3.64ubuntu1) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.9) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J","output_type":"stream"}]},{"cell_type":"code","source":"!apt install ffmpeg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-13T02:54:58.557739Z","iopub.execute_input":"2023-04-13T02:54:58.558155Z","iopub.status.idle":"2023-04-13T02:55:01.025847Z","shell.execute_reply.started":"2023-04-13T02:54:58.558108Z","shell.execute_reply":"2023-04-13T02:55:01.024583Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n0 upgraded, 0 newly installed, 0 to remove and 76 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"!apt install xvfb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-13T02:55:01.028007Z","iopub.execute_input":"2023-04-13T02:55:01.028427Z","iopub.status.idle":"2023-04-13T02:55:03.386592Z","shell.execute_reply.started":"2023-04-13T02:55:01.028383Z","shell.execute_reply":"2023-04-13T02:55:03.385367Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nxvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n0 upgraded, 0 newly installed, 0 to remove and 76 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyvirtualdisplay","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-13T02:55:03.389871Z","iopub.execute_input":"2023-04-13T02:55:03.390555Z","iopub.status.idle":"2023-04-13T02:55:14.158452Z","shell.execute_reply.started":"2023-04-13T02:55:03.390505Z","shell.execute_reply":"2023-04-13T02:55:14.157233Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pyvirtualdisplay\n  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\nInstalling collected packages: pyvirtualdisplay\nSuccessfully installed pyvirtualdisplay-3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyglet==1.5.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-13T02:55:14.161153Z","iopub.execute_input":"2023-04-13T02:55:14.161859Z","iopub.status.idle":"2023-04-13T02:55:24.699014Z","shell.execute_reply.started":"2023-04-13T02:55:14.161809Z","shell.execute_reply":"2023-04-13T02:55:24.697784Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting pyglet==1.5.1\n  Downloading pyglet-1.5.1-py2.py3-none-any.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyglet\nSuccessfully installed pyglet-1.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:55:24.702865Z","iopub.execute_input":"2023-04-13T02:55:24.703209Z","iopub.status.idle":"2023-04-13T02:55:25.097540Z","shell.execute_reply.started":"2023-04-13T02:55:24.703174Z","shell.execute_reply":"2023-04-13T02:55:25.096415Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<pyvirtualdisplay.display.Display at 0x727a9aeeb390>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:55:25.100251Z","iopub.execute_input":"2023-04-13T02:55:25.101457Z","iopub.status.idle":"2023-04-13T02:55:53.276474Z","shell.execute_reply.started":"2023-04-13T02:55:25.101410Z","shell.execute_reply":"2023-04-13T02:55:53.275115Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-gzglw968\n  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-gzglw968\n  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting git+https://github.com/qlan3/gym-games.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3))\n  Cloning https://github.com/qlan3/gym-games.git to /tmp/pip-req-build-hpzuez_9\n  Running command git clone --filter=blob:none --quiet https://github.com/qlan3/gym-games.git /tmp/pip-req-build-hpzuez_9\n  Resolved https://github.com/qlan3/gym-games.git to commit 4e7fd78ec757bfdad2dde939f18e626bcfbfb967\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gym in /opt/conda/lib/python3.7/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (0.23.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.7/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (0.13.3)\nCollecting imageio-ffmpeg\n  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml==6.0 in /opt/conda/lib/python3.7/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 6)) (6.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from gym->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (0.0.8)\nRequirement already satisfied: importlib-metadata>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from gym->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (4.11.4)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from gym->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (1.21.6)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (9.4.0)\nCollecting MinAtar>=1.0.10\n  Downloading MinAtar-1.0.12-py3-none-any.whl (16 kB)\nCollecting setuptools>=65.5.1\n  Downloading setuptools-67.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pygame>=1.9.6\n  Downloading pygame-2.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (4.64.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (2.28.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (4.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (3.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (23.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.10.0->gym->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (3.11.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (1.16.0)\nRequirement already satisfied: pandas>=0.24.2 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (1.3.5)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.8.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.0.9)\nRequirement already satisfied: pytz>=2018.9 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2023.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (1.4.4)\nRequirement already satisfied: scipy>=1.2.1 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (1.7.3)\nRequirement already satisfied: matplotlib>=3.0.3 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.5.3)\nRequirement already satisfied: seaborn>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.12.2)\nRequirement already satisfied: cycler>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (2.1.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->MinAtar>=1.0.10->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.38.0)\nBuilding wheels for collected packages: ple, gym-games\n  Building wheel for ple (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ple: filename=ple-0.0.1-py3-none-any.whl size=50789 sha256=6e0b06f14a243f4cf6316f027970aa18b5029b932bf74e68e6f585fe09a4693e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0q_dfn68/wheels/cd/51/18/46ce3a7c7b4a75d9ba91594b40e028f98b2001414f6c1da798\n  Building wheel for gym-games (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym-games: filename=gym_games-1.0.4-py3-none-any.whl size=17329 sha256=f6aca7d37c889d58772a1db40298e72eced53526667ef78c5caeca5b2e0e8788\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0q_dfn68/wheels/4d/32/86/19e03a5c068f41f6f2dd6f4197d893f97a27ae182a66bf598e\nSuccessfully built ple gym-games\nInstalling collected packages: setuptools, pygame, ple, imageio-ffmpeg, MinAtar, gym-games\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 59.8.0\n    Uninstalling setuptools-59.8.0:\n      Successfully uninstalled setuptools-59.8.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf 21.12.2 requires cupy-cuda115, which is not installed.\ncudf 21.12.2 requires cupy-cuda115, which is not installed.\ntensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\ntensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\ntensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\nlibrosa 0.10.0.post2 requires soundfile>=0.12.1, but you have soundfile 0.11.0 which is incompatible.\ndistributed 2021.11.2 requires dask==2021.11.2, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.12.2 requires dask<=2021.11.2,>=2021.11.1, but you have dask 2022.2.0 which is incompatible.\ncloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.83.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MinAtar-1.0.12 gym-games-1.0.4 imageio-ffmpeg-0.4.8 ple-0.0.1 pygame-2.3.0 setuptools-67.6.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:55:53.280389Z","iopub.execute_input":"2023-04-13T02:55:53.280716Z","iopub.status.idle":"2023-04-13T02:55:56.390179Z","shell.execute_reply.started":"2023-04-13T02:55:53.280682Z","shell.execute_reply":"2023-04-13T02:55:56.389210Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:59:01.900456Z","iopub.execute_input":"2023-04-13T02:59:01.900884Z","iopub.status.idle":"2023-04-13T02:59:01.907622Z","shell.execute_reply.started":"2023-04-13T02:59:01.900848Z","shell.execute_reply":"2023-04-13T02:59:01.906422Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"env_id = \"CartPole-v1\"\n# Create the env\nenv = gym.make(env_id)\n\n# Create the evaluation env\neval_env = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:59:01.914124Z","iopub.execute_input":"2023-04-13T02:59:01.914453Z","iopub.status.idle":"2023-04-13T02:59:01.976784Z","shell.execute_reply.started":"2023-04-13T02:59:01.914406Z","shell.execute_reply":"2023-04-13T02:59:01.975751Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"_____OBSERVATION SPACE_____ \n\nThe State Space is:  4\nSample observation [ 2.5701585e+00 -9.6042878e+37 -2.1042790e-01  2.4027001e+38]\n\n _____ACTION SPACE_____ \n\nThe Action Space is:  2\nAction Space Sample 0\n","output_type":"stream"}]},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:59:01.978722Z","iopub.execute_input":"2023-04-13T02:59:01.979577Z","iopub.status.idle":"2023-04-13T02:59:01.988278Z","shell.execute_reply.started":"2023-04-13T02:59:01.979539Z","shell.execute_reply":"2023-04-13T02:59:01.987242Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"cartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1_000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:59:01.991339Z","iopub.execute_input":"2023-04-13T02:59:01.991670Z","iopub.status.idle":"2023-04-13T02:59:02.003363Z","shell.execute_reply.started":"2023-04-13T02:59:01.991642Z","shell.execute_reply":"2023-04-13T02:59:02.002348Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = env.step(action)\n            rewards.append(reward)\n            if done:\n                break \n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n        \n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t) \n        n_steps = len(rewards) \n        # Compute the discounted returns at each timestep,\n        # as \n        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n        #\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity \n        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n        \n        # Given this formulation, the returns at each timestep t can be computed \n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order \n        # to avoid computing them multiple times)\n        \n        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n        \n        \n        ## Given the above, we calculate the returns at timestep t as: \n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed \n        ## if we were to do it from first to last.\n        \n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns)>0 else 0)\n            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n            \n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n        ## eps is the smallest representable float, which is \n        # added to the standard deviation of the returns to avoid numerical instabilities        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n        \n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n        \n        # Line 8: PyTorch prefers gradient descent \n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n        \n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n        \n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:59:02.006742Z","iopub.execute_input":"2023-04-13T02:59:02.007008Z","iopub.status.idle":"2023-04-13T02:59:02.020370Z","shell.execute_reply.started":"2023-04-13T02:59:02.006982Z","shell.execute_reply":"2023-04-13T02:59:02.019271Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Create policy and place it to the device\ncartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])\n\nscores = reinforce(cartpole_policy,\n                   cartpole_optimizer,\n                   cartpole_hyperparameters[\"n_training_episodes\"], \n                   cartpole_hyperparameters[\"max_t\"],\n                   cartpole_hyperparameters[\"gamma\"], \n                   100)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T02:59:02.033945Z","iopub.execute_input":"2023-04-13T02:59:02.034232Z","iopub.status.idle":"2023-04-13T03:03:59.991210Z","shell.execute_reply.started":"2023-04-13T02:59:02.034205Z","shell.execute_reply":"2023-04-13T03:03:59.990097Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Episode 100\tAverage Score: 39.54\nEpisode 200\tAverage Score: 186.46\nEpisode 300\tAverage Score: 454.28\nEpisode 400\tAverage Score: 359.70\nEpisode 500\tAverage Score: 466.88\nEpisode 600\tAverage Score: 489.04\nEpisode 700\tAverage Score: 252.14\nEpisode 800\tAverage Score: 448.42\nEpisode 900\tAverage Score: 433.84\nEpisode 1000\tAverage Score: 301.50\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json\nimport imageio\n\nimport tempfile\n\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-04-13T03:03:59.994948Z","iopub.execute_input":"2023-04-13T03:03:59.995252Z","iopub.status.idle":"2023-04-13T03:04:00.019369Z","shell.execute_reply.started":"2023-04-13T03:03:59.995223Z","shell.execute_reply":"2023-04-13T03:04:00.018401Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"huggingface_rl_course\")","metadata":{"execution":{"iopub.status.busy":"2023-04-13T03:04:00.020975Z","iopub.execute_input":"2023-04-13T03:04:00.021423Z","iopub.status.idle":"2023-04-13T03:04:00.164300Z","shell.execute_reply.started":"2023-04-13T03:04:00.021381Z","shell.execute_reply":"2023-04-13T03:04:00.163264Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The Reinforce agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    for step in range(max_steps):\n      action, _ = policy.act(state)\n      new_state, reward, done, info = env.step(action)\n      total_rewards_ep += reward\n        \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2023-04-13T03:04:00.165855Z","iopub.execute_input":"2023-04-13T03:04:00.166803Z","iopub.status.idle":"2023-04-13T03:04:00.174454Z","shell.execute_reply.started":"2023-04-13T03:04:00.166772Z","shell.execute_reply":"2023-04-13T03:04:00.173281Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"evaluate_agent(eval_env, \n               cartpole_hyperparameters[\"max_t\"], \n               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n               cartpole_policy)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T03:04:00.177848Z","iopub.execute_input":"2023-04-13T03:04:00.178386Z","iopub.status.idle":"2023-04-13T03:04:02.746748Z","shell.execute_reply.started":"2023-04-13T03:04:00.178343Z","shell.execute_reply":"2023-04-13T03:04:02.745598Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(500.0, 0.0)"},"metadata":{}}]},{"cell_type":"code","source":"def push_to_hub(repo_id, \n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the Hub\n\n  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n  :param model: the pytorch model we want to save\n  :param hyperparameters: training hyperparameters\n  :param eval_env: evaluation environment\n  :param video_fps: how many frame per seconds to record our video replay \n  \"\"\"\n\n  _, repo_name = repo_id.split(\"/\")\n  api = HfApi(endpoint=\"https://huggingface.co\", user_agent=\"bsenst\", token=hf_token)\n  \n  # Step 1: Create the repo\n  repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n  )\n\n  with tempfile.TemporaryDirectory() as tmpdirname:\n    local_directory = Path(tmpdirname)\n  \n    # Step 2: Save the model\n    torch.save(model, local_directory / \"model.pt\")\n\n    # Step 3: Save the hyperparameters to JSON\n    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n      json.dump(hyperparameters, outfile)\n    \n    # Step 4: Evaluate the model and build JSON\n    mean_reward, std_reward = evaluate_agent(eval_env, \n                                            hyperparameters[\"max_t\"],\n                                            hyperparameters[\"n_evaluation_episodes\"], \n                                            model)\n    # Get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n          \"env_id\": hyperparameters[\"env_id\"], \n          \"mean_reward\": mean_reward,\n          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n          \"eval_datetime\": eval_form_datetime,\n    }\n\n    # Write a JSON file\n    with open(local_directory / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = hyperparameters[\"env_id\"]\n    \n    metadata = {}\n    metadata[\"tags\"] = [\n          env_name,\n          \"reinforce\",\n          \"reinforcement-learning\",\n          \"custom-implementation\",\n          \"deep-rl-class\"\n      ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n      )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Reinforce** Agent playing **{env_id}**\n  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n  \"\"\"\n\n    readme_path = local_directory / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n          readme = f.read()\n    else:\n      readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n      f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path =  local_directory / \"replay.mp4\"\n    record_video(env, model, video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n          repo_id=repo_id,\n          folder_path=local_directory,\n          path_in_repo=\".\",\n    )\n\n    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-13T03:04:02.748350Z","iopub.execute_input":"2023-04-13T03:04:02.748915Z","iopub.status.idle":"2023-04-13T03:04:02.764156Z","shell.execute_reply.started":"2023-04-13T03:04:02.748874Z","shell.execute_reply":"2023-04-13T03:04:02.763122Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def record_video(env, policy, out_directory, fps=30):\n  \"\"\"\n  Generate a replay video of the agent\n  :param env\n  :param Qtable: Qtable of our agent\n  :param out_directory\n  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n  \"\"\"\n  images = []  \n  done = False\n  state = env.reset()\n  img = env.render(mode='rgb_array')\n  images.append(img)\n  while not done:\n    # Take the action (index) that have the maximum expected future reward given that state\n    action, _ = policy.act(state)\n    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n    img = env.render(mode='rgb_array')\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T03:04:02.765709Z","iopub.execute_input":"2023-04-13T03:04:02.766228Z","iopub.status.idle":"2023-04-13T03:04:02.778254Z","shell.execute_reply.started":"2023-04-13T03:04:02.766186Z","shell.execute_reply":"2023-04-13T03:04:02.777191Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"repo_id = \"bsenst/Reinforce-Cartpole\" #TODO Define your repo id {username/Reinforce-{model-id}}\npush_to_hub(repo_id,\n                cartpole_policy, # The model we want to save\n                cartpole_hyperparameters, # Hyperparameters\n                eval_env, # Evaluation environment\n                video_fps=30\n                )","metadata":{"execution":{"iopub.status.busy":"2023-04-13T03:04:58.840736Z","iopub.execute_input":"2023-04-13T03:04:58.841824Z","iopub.status.idle":"2023-04-13T03:05:07.961231Z","shell.execute_reply.started":"2023-04-13T03:04:58.841775Z","shell.execute_reply":"2023-04-13T03:05:07.959859Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"[swscaler @ 0x7143600] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a7942bfd704732b9eb9732ee6ad10d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.pt:   0%|          | 0.00/2.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ecd3dabdc54f169ceadfb08e8235f1"}},"metadata":{}},{"name":"stdout","text":"Your model is pushed to the Hub. You can view your model here: https://huggingface.co/bsenst/Reinforce-Cartpole\n","output_type":"stream"}]}]}