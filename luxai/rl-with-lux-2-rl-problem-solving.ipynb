{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup Code\n\nBefore we start lets install some dependencies. This will also run some extra code that your local notebook may not need to due to how Kaggle Notebooks are setup. **Note that this tutorial is only using the CPU luxai_s2 engine, the jax version will be released later**","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade luxai_s2\n!pip install pettingzoo==1.12.0 gym==0.21.0 stable-baselines3\n!pip install --upgrade \"importlib_metadata<5.0\"","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-01T19:35:20.850624Z","iopub.execute_input":"2023-04-01T19:35:20.851204Z","iopub.status.idle":"2023-04-01T19:36:26.228655Z","shell.execute_reply.started":"2023-04-01T19:35:20.851082Z","shell.execute_reply":"2023-04-01T19:36:26.226932Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting luxai_s2\n  Downloading luxai_s2-2.1.9-py3-none-any.whl (63 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from luxai_s2) (1.21.6)\nCollecting vec-noise\n  Downloading vec_noise-1.1.4.zip (134 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.1/134.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting importlib-metadata<5.0\n  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from luxai_s2) (3.5.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from luxai_s2) (1.7.3)\nCollecting pettingzoo\n  Downloading PettingZoo-1.22.3-py3-none-any.whl (816 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gym==0.21.0\n  Downloading gym-0.21.0.tar.gz (1.5 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from luxai_s2) (1.1.0)\nCollecting pygame\n  Downloading pygame-2.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.21.0->luxai_s2) (2.1.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0->luxai_s2) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0->luxai_s2) (4.1.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (23.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (9.1.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (1.4.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (4.33.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (3.0.9)\nCollecting gymnasium>=0.26.0\n  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting jax-jumpy>=1.0.0\n  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\nCollecting farama-notifications>=0.0.1\n  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\nCollecting typing-extensions>=3.6.4\n  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->luxai_s2) (1.15.0)\nBuilding wheels for collected packages: gym, vec-noise\n  Building wheel for gym (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=1709309ac3f71ad5b79ab357155f127ba8d944a9592e732e9a8b90ccfa52382d\n  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n  Building wheel for vec-noise (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for vec-noise: filename=vec_noise-1.1.4-cp37-cp37m-linux_x86_64.whl size=85814 sha256=80645e8603f997f06848b1935071401708c5dc349674e0731c0411fcb615bff1\n  Stored in directory: /root/.cache/pip/wheels/fc/0c/19/5932b4834cf3204ed2ae845e788f07c79b3279c302d55d6fa8\nSuccessfully built gym vec-noise\nInstalling collected packages: farama-notifications, vec-noise, typing-extensions, pygame, jax-jumpy, importlib-metadata, gymnasium, gym, pettingzoo, luxai_s2\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 6.0.0\n    Uninstalling importlib-metadata-6.0.0:\n      Successfully uninstalled importlib-metadata-6.0.0\n  Attempting uninstall: gym\n    Found existing installation: gym 0.26.2\n    Uninstalling gym-0.26.2:\n      Successfully uninstalled gym-0.26.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nthinc 8.0.17 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.5.0 which is incompatible.\ntensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\nspacy 3.3.2 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.2 which is incompatible.\nflake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nconfection 0.0.4 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\ncmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 4.13.0 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.40.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\naiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.54 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed farama-notifications-0.0.4 gym-0.21.0 gymnasium-0.28.1 importlib-metadata-4.13.0 jax-jumpy-1.0.0 luxai_s2-2.1.9 pettingzoo-1.22.3 pygame-2.3.0 typing-extensions-4.5.0 vec-noise-1.1.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pettingzoo==1.12.0\n  Downloading PettingZoo-1.12.0.tar.gz (756 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m756.1/756.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gym==0.21.0 in /opt/conda/lib/python3.7/site-packages (0.21.0)\nCollecting stable-baselines3\n  Downloading stable_baselines3-1.7.0-py3-none-any.whl (171 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m171.8/171.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from pettingzoo==1.12.0) (1.21.6)\nRequirement already satisfied: importlib-metadata>=4.8.1 in /opt/conda/lib/python3.7/site-packages (from gym==0.21.0) (4.13.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.21.0) (2.1.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (3.5.2)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (1.11.0+cpu)\nRequirement already satisfied: typing-extensions<5,>=4.0 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (4.5.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (1.3.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym==0.21.0) (3.8.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (4.33.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (23.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (2.8.2)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (9.1.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (1.4.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (3.0.9)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->stable-baselines3) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.15.0)\nBuilding wheels for collected packages: pettingzoo\n  Building wheel for pettingzoo (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pettingzoo: filename=PettingZoo-1.12.0-py3-none-any.whl size=873583 sha256=d65573a191a147a31b0e1aa424567b4d732cd538dfdb10ce2f473cf65700515f\n  Stored in directory: /root/.cache/pip/wheels/50/1d/da/6a09fdb8c06333aa981d1239e40cc8aa5cbac80b30498b1e85\nSuccessfully built pettingzoo\nInstalling collected packages: stable-baselines3, pettingzoo\n  Attempting uninstall: pettingzoo\n    Found existing installation: PettingZoo 1.22.3\n    Uninstalling PettingZoo-1.22.3:\n      Successfully uninstalled PettingZoo-1.22.3\nSuccessfully installed pettingzoo-1.12.0 stable-baselines3-1.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: importlib_metadata<5.0 in /opt/conda/lib/python3.7/site-packages (4.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib_metadata<5.0) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib_metadata<5.0) (4.5.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py\n__version__ = \"\"\n# this code above is used for Kaggle Notebooks\n# You might not need to run this but if you get an attribute error about the gym package, run it","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-01T19:37:14.419053Z","iopub.execute_input":"2023-04-01T19:37:14.419549Z","iopub.status.idle":"2023-04-01T19:37:14.429323Z","shell.execute_reply.started":"2023-04-01T19:37:14.419512Z","shell.execute_reply":"2023-04-01T19:37:14.427984Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Overwriting /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import importlib\nimport importlib_metadata\n# kaggle has 6.0.0 installed but we need version <5.0\nimportlib.reload(importlib_metadata)\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-01T19:37:23.317205Z","iopub.execute_input":"2023-04-01T19:37:23.318325Z","iopub.status.idle":"2023-04-01T19:37:23.341387Z","shell.execute_reply.started":"2023-04-01T19:37:23.318272Z","shell.execute_reply":"2023-04-01T19:37:23.340177Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<module 'importlib_metadata' from '/opt/conda/lib/python3.7/site-packages/importlib_metadata/__init__.py'>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Reinforcement Learning for Lux AI Season 2 ðŸ¤–\n\nPart 2 of the RL series will now dig into building a working RL agent for the Lux AI Challenge, Season 2!\n\nLux AI is designed to be intuitive to understand, but heavily layered in complexity and interactions of game mechanics in an multi-agent cooperative and competitive environment. \n\nLux AI Season 2's rules can be found here: https://www.lux-ai.org/specs-s2. Make sure to read them to learn how to the game works, and the rest of this tutorial will be much easier to understand.\n\nPart 1 of the series covered the single-agent RL setup, but Lux AI Season 2 is multi-agent! Moreover, the environment has different phases and a complex action space which makes it difficult to learn or use of the box. \n\nThis tutorial will cover simple tools and tricks on how to reduce a complex problem into a easier one! We will primarily focus on three things: \n\n1. Simplifying the action space with controllers/action wrappers\n2. Simplifying observations\n3. Transforming the three phase Lux AI game into a single phase game\n\nUltimately this will modify the standard RL diagram into one that is \"single-agent\", with modified observations and actions:\n\n![](https://github.com/Lux-AI-Challenge/Lux-Design-S2/raw/main/docs/assets/anatomyluxrl.png)\n\n\nThis starter kit is also implemented in https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/rl/sb3\n\nWe highly **recommend running this code with more CPU cores** as RL training can be fairly slow and needs good tuning. A GPU can also speed up the optimization part of RL training, but the rollout/interaction phase is CPU heavy in this tutorial and is typically the bottleneck.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Simplifying the Action Space\n\nThe action space is quite complicated in Lux S2 as each robot can move, dig, transfer/pickup, all in addition to being able to combine any sequence of these primitives into an action queue of up to length 20. For machine learning, such a massive action space leads to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality), making any ML algorithm have a much harder time to learn something useful, especially in RL.\n\nTo handle this, we can program a custom Controller that translates actions from one action space to the original action space and adds a few tricks and heuristics to be integrated with RL training. Since the original lux action space is large, this controller can be a little complicated. For those who want to dive straight into training you can use the controller as is. \n\nFor a high-level overview this controller will\n- Define a massively simplified action space\n- Translate actions from the discrete action space into the Lux S2 action space `action_to_lux_action`\n- Add a heuristic factory action to build one Heavy robot\n- Generate action masks where False = an action is invalid\n\nOverall, the action space of the controller is a discrete action space with just 12 dimensions to control just one heavy robot. It allows for a robot's 4 directional movement, transferring ice in 4 directions in addition to center, picking up power, digging, and a no-op action. This doesn't include factory actions, self destruct, recharging, transferring other types of resources, or longer planned action queues in the action space, which are all open problems for you to potentially tackle!\n\nThe controller also includes a trick to allow agents to reduce power costs incurred by action queue updates. The controller skips updating action queues if the existing action queue is the same as the new one the agent wants to use for the robot.\n\nWhile this simplification doesn't include adding in more complex things like more heavy robots or planting lichen, it will train out a succesful policy that with simple modifications, will beat the majority of bots using the rule-based starter kits.\n\nMore advanced usages can consider how to model the actions of different types of units on a game board (e.g. heavy, light, or factory) by using a MultiDiscrete action space. A more practical and likely winning solution can be to use a image-like controller by generating actions for each tile on the board and only using the actions with friendly units on that tile. See [Season 1's solution by ToadBrigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) and our previous [research paper: Emergent Collective Intelligence from Massive-Agent Cooperation and Competition](https://arxiv.org/abs/2301.01609) for how a image-like controller can work.\n","metadata":{"tags":[]}},{"cell_type":"code","source":"import sys\nfrom typing import Any, Dict\n\nimport numpy as np\nimport numpy.typing as npt\nfrom gym import spaces\n\n\n# Controller class copied here since you won't have access to the luxai_s2 package directly on the competition server\nclass Controller:\n    def __init__(self, action_space: spaces.Space) -> None:\n        self.action_space = action_space\n\n    def action_to_lux_action(\n        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n    ):\n        \"\"\"\n        Takes as input the current \"raw observation\" and the parameterized action and returns\n        an action formatted for the Lux env\n        \"\"\"\n        raise NotImplementedError()\n\n    def action_masks(self, agent: str, obs: Dict[str, Any]):\n        \"\"\"\n        Generates a boolean action mask indicating in each discrete dimension whether it would be valid or not\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SimpleUnitDiscreteController(Controller):\n    def __init__(self, env_cfg) -> None:\n        \"\"\"\n        A simple controller that controls only the robot that will get spawned.\n        Moreover, it will always try to spawn one heavy robot if there are none regardless of action given\n\n        For the robot unit\n        - 4 cardinal direction movement (4 dims)\n        - a move center no-op action (1 dim)\n        - transfer action just for transferring ice in 4 cardinal directions or center (5)\n        - pickup action for power (1 dims)\n        - dig action (1 dim)\n        - no op action (1 dim) - equivalent to not submitting an action queue which costs power\n\n        It does not include\n        - self destruct action\n        - recharge action\n        - planning (via actions executing multiple times or repeating actions)\n        - factory actions\n        - transferring power or resources other than ice\n\n        To help understand how to this controller works to map one action space to the original lux action space,\n        see how the lux action space is defined in luxai_s2/spaces/action.py\n\n        \"\"\"\n        self.env_cfg = env_cfg\n        self.move_act_dims = 4\n        self.transfer_act_dims = 5\n        self.pickup_act_dims = 1\n        self.dig_act_dims = 1\n        self.no_op_dims = 1\n\n        self.move_dim_high = self.move_act_dims\n        self.transfer_dim_high = self.move_dim_high + self.transfer_act_dims\n        self.pickup_dim_high = self.transfer_dim_high + self.pickup_act_dims\n        self.dig_dim_high = self.pickup_dim_high + self.dig_act_dims\n        self.no_op_dim_high = self.dig_dim_high + self.no_op_dims\n\n        self.total_act_dims = self.no_op_dim_high\n        action_space = spaces.Discrete(self.total_act_dims)\n        super().__init__(action_space)\n\n    def _is_move_action(self, id):\n        return id < self.move_dim_high\n\n    def _get_move_action(self, id):\n        # move direction is id + 1 since we don't allow move center here\n        return np.array([0, id + 1, 0, 0, 0, 1])\n\n    def _is_transfer_action(self, id):\n        return id < self.transfer_dim_high\n\n    def _get_transfer_action(self, id):\n        id = id - self.move_dim_high\n        transfer_dir = id % 5\n        return np.array([1, transfer_dir, 0, self.env_cfg.max_transfer_amount, 0, 1])\n\n    def _is_pickup_action(self, id):\n        return id < self.pickup_dim_high\n\n    def _get_pickup_action(self, id):\n        return np.array([2, 0, 4, self.env_cfg.max_transfer_amount, 0, 1])\n\n    def _is_dig_action(self, id):\n        return id < self.dig_dim_high\n\n    def _get_dig_action(self, id):\n        return np.array([3, 0, 0, 0, 0, 1])\n\n    def action_to_lux_action(\n        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n    ):\n        shared_obs = obs[\"player_0\"]\n        lux_action = dict()\n        units = shared_obs[\"units\"][agent]\n        for unit_id in units.keys():\n            unit = units[unit_id]\n            choice = action\n            action_queue = []\n            no_op = False\n            if self._is_move_action(choice):\n                action_queue = [self._get_move_action(choice)]\n            elif self._is_transfer_action(choice):\n                action_queue = [self._get_transfer_action(choice)]\n            elif self._is_pickup_action(choice):\n                action_queue = [self._get_pickup_action(choice)]\n            elif self._is_dig_action(choice):\n                action_queue = [self._get_dig_action(choice)]\n            else:\n                # action is a no_op, so we don't update the action queue\n                no_op = True\n\n            # simple trick to help agents conserve power is to avoid updating the action queue\n            # if the agent was previously trying to do that particular action already\n            if len(unit[\"action_queue\"]) > 0 and len(action_queue) > 0:\n                same_actions = (unit[\"action_queue\"][0] == action_queue[0]).all()\n                if same_actions:\n                    no_op = True\n            if not no_op:\n                lux_action[unit_id] = action_queue\n\n            break\n\n        factories = shared_obs[\"factories\"][agent]\n        if len(units) == 0:\n            for unit_id in factories.keys():\n                lux_action[unit_id] = 1  # build a single heavy\n\n        return lux_action\n\n    def action_masks(self, agent: str, obs: Dict[str, Any]):\n        \"\"\"\n        Defines a simplified action mask for this controller's action space\n\n        Doesn't account for whether robot has enough power\n        \"\"\"\n\n        # compute a factory occupancy map that will be useful for checking if a board tile\n        # has a factory and which team's factory it is.\n        shared_obs = obs[agent]\n        factory_occupancy_map = (\n            np.ones_like(shared_obs[\"board\"][\"rubble\"], dtype=int) * -1\n        )\n        factories = dict()\n        for player in shared_obs[\"factories\"]:\n            factories[player] = dict()\n            for unit_id in shared_obs[\"factories\"][player]:\n                f_data = shared_obs[\"factories\"][player][unit_id]\n                f_pos = f_data[\"pos\"]\n                # store in a 3x3 space around the factory position it's strain id.\n                factory_occupancy_map[\n                    f_pos[0] - 1 : f_pos[0] + 2, f_pos[1] - 1 : f_pos[1] + 2\n                ] = f_data[\"strain_id\"]\n\n        units = shared_obs[\"units\"][agent]\n        action_mask = np.zeros((self.total_act_dims), dtype=bool)\n        for unit_id in units.keys():\n            action_mask = np.zeros(self.total_act_dims)\n            # movement is always valid\n            action_mask[:4] = True\n\n            # transferring is valid only if the target exists\n            unit = units[unit_id]\n            pos = np.array(unit[\"pos\"])\n            # a[1] = direction (0 = center, 1 = up, 2 = right, 3 = down, 4 = left)\n            move_deltas = np.array([[0, 0], [0, -1], [1, 0], [0, 1], [-1, 0]])\n            for i, move_delta in enumerate(move_deltas):\n                transfer_pos = np.array(\n                    [pos[0] + move_delta[0], pos[1] + move_delta[1]]\n                )\n                # check if theres a factory tile there\n                if (\n                    transfer_pos[0] < 0\n                    or transfer_pos[1] < 0\n                    or transfer_pos[0] >= len(factory_occupancy_map)\n                    or transfer_pos[1] >= len(factory_occupancy_map[0])\n                ):\n                    continue\n                factory_there = factory_occupancy_map[transfer_pos[0], transfer_pos[1]]\n                if factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]:\n                    action_mask[\n                        self.transfer_dim_high - self.transfer_act_dims + i\n                    ] = True\n\n            factory_there = factory_occupancy_map[pos[0], pos[1]]\n            on_top_of_factory = (\n                factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]\n            )\n\n            # dig is valid only if on top of tile with rubble or resources or lichen\n            board_sum = (\n                shared_obs[\"board\"][\"ice\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"ore\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"rubble\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"lichen\"][pos[0], pos[1]]\n            )\n            if board_sum > 0 and not on_top_of_factory:\n                action_mask[\n                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n                ] = True\n\n            # pickup is valid only if on top of factory tile\n            if on_top_of_factory:\n                action_mask[\n                    self.pickup_dim_high - self.pickup_act_dims : self.pickup_dim_high\n                ] = True\n                action_mask[\n                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n                ] = False\n\n            # no-op is always valid\n            action_mask[-1] = True\n            break\n        return action_mask\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:52:53.334289Z","iopub.execute_input":"2023-04-01T19:52:53.334792Z","iopub.status.idle":"2023-04-01T19:52:53.377461Z","shell.execute_reply.started":"2023-04-01T19:52:53.334757Z","shell.execute_reply":"2023-04-01T19:52:53.376043Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 2. Simplifying the Observation Space\n\nLux S2 is fully observable which means you can see everything on the map, the opponents units etc. However, this is very high dimensional and not necessarily easy to learn from due to the curse of dimensionality (again!). We want to simplify this observation space in a way that contains sufficient information to learn a good policy but is also easy to learn from.\n\nFor this tutorial, we will create a state-based observation space (no image like features e.g. the rubble, ice, ore maps) with some feature engineering that includes useful information such as the distance to the closest factory and ice tile. The wrapper we provide below will use the `gym.ObservationWrapper` interface. Note that since we are focusing on just controlling one heavy robot, the observation wrapper is written to only support one heavy robot (and returns 0 if there are none).\n\n\nMore advanced solutions can look into using the full set of observations and designing the appropriate neural net architecture to process them. One idea would be to use convolutional neural networks to process board features like images. See [Season 1's solution by ToadBrigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) and our previous [research paper: Emergent Collective Intelligence from Massive-Agent Cooperation and Competition](https://arxiv.org/abs/2301.01609) for example architectures and feature engineering choices.\n","metadata":{"tags":[]}},{"cell_type":"code","source":"from typing import Any, Dict\n\nimport gym\nimport numpy as np\nimport numpy.typing as npt\nfrom gym import spaces\n\n\nclass SimpleUnitObservationWrapper(gym.ObservationWrapper):\n    \"\"\"\n    A simple state based observation to work with in pair with the SimpleUnitDiscreteController\n\n    It contains info only on the first robot, the first factory you own, and some useful features. If there are no owned robots the observation is just zero.\n    No information about the opponent is included. This will generate observations for all teams.\n\n    Included features:\n    - First robot's stats\n    - distance vector to closest ice tile\n    - distance vector to first factory\n\n    \"\"\"\n\n    def __init__(self, env: gym.Env) -> None:\n        super().__init__(env)\n        self.observation_space = spaces.Box(-999, 999, shape=(13,))\n\n    def observation(self, obs):\n        return SimpleUnitObservationWrapper.convert_obs(obs, self.env.state.env_cfg)\n\n    # we make this method static so the submission/evaluation code can use this as well\n    @staticmethod\n    def convert_obs(obs: Dict[str, Any], env_cfg: Any) -> Dict[str, npt.NDArray]:\n        observation = dict()\n        shared_obs = obs[\"player_0\"]\n        ice_map = shared_obs[\"board\"][\"ice\"]\n        ice_tile_locations = np.argwhere(ice_map == 1)\n\n        for agent in obs.keys():\n            obs_vec = np.zeros(\n                13,\n            )\n\n            factories = shared_obs[\"factories\"][agent]\n            factory_vec = np.zeros(2)\n            for k in factories.keys():\n                # here we track a normalized position of the first friendly factory\n                factory = factories[k]\n                factory_vec = np.array(factory[\"pos\"]) / env_cfg.map_size\n                break\n            units = shared_obs[\"units\"][agent]\n            for k in units.keys():\n                unit = units[k]\n\n                # store cargo+power values scaled to [0, 1]\n                cargo_space = env_cfg.ROBOTS[unit[\"unit_type\"]].CARGO_SPACE\n                battery_cap = env_cfg.ROBOTS[unit[\"unit_type\"]].BATTERY_CAPACITY\n                cargo_vec = np.array(\n                    [\n                        unit[\"power\"] / battery_cap,\n                        unit[\"cargo\"][\"ice\"] / cargo_space,\n                        unit[\"cargo\"][\"ore\"] / cargo_space,\n                        unit[\"cargo\"][\"water\"] / cargo_space,\n                        unit[\"cargo\"][\"metal\"] / cargo_space,\n                    ]\n                )\n                unit_type = (\n                    0 if unit[\"unit_type\"] == \"LIGHT\" else 1\n                )  # note that build actions use 0 to encode Light\n                # normalize the unit position\n                pos = np.array(unit[\"pos\"]) / env_cfg.map_size\n                unit_vec = np.concatenate(\n                    [pos, [unit_type], cargo_vec, [unit[\"team_id\"]]], axis=-1\n                )\n\n                # we add some engineered features down here\n                # compute closest ice tile\n                ice_tile_distances = np.mean(\n                    (ice_tile_locations - np.array(unit[\"pos\"])) ** 2, 1\n                )\n                # normalize the ice tile location\n                closest_ice_tile = (\n                    ice_tile_locations[np.argmin(ice_tile_distances)] / env_cfg.map_size\n                )\n                obs_vec = np.concatenate(\n                    [unit_vec, factory_vec - pos, closest_ice_tile - pos], axis=-1\n                )\n                break\n            observation[agent] = obs_vec\n\n        return observation","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:53:03.333283Z","iopub.execute_input":"2023-04-01T19:53:03.333738Z","iopub.status.idle":"2023-04-01T19:53:03.350585Z","shell.execute_reply.started":"2023-04-01T19:53:03.333702Z","shell.execute_reply":"2023-04-01T19:53:03.349766Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 3. Transforming Lux S2 into a Single Phase\n\nNormally RL frameworks like Stable Baselines 3, RLlib, Tianshou etc. expect the action space and observation space to be consistent throughout an episode. Lux S2 does not conform to this as we add some additional complexity like bidding and factory placement phases. A simple way to get around this is to **upgrade the reset function.**\n\nPreviously we saw that `env.reset()` resets an environment to a clean slate. We will upgrade this function by building a environment wrapper that not only resets to the clean slate, but also handles the bidding and factory placement phases so effectively agents that are learning start from game states with factories already placed.\n\nBelow will build a wrapper that works with the SB3 package. To do this, we want to provide the wrapper a bidding policy and factory placement policy which will be used by all teams to handle the first two phases in the reset function. The code below does just that by overriding the environment's reset function in the wrapper. \n\nFurthermore, we want to use the Controller we defined earlier, so that is also an argument to the SB3Wrapper and we use it to transform actions inside the `env.step` function","metadata":{"tags":[]}},{"cell_type":"code","source":"from typing import Callable, Dict\n\nimport gym\nimport numpy as np\nimport numpy.typing as npt\nfrom gym import spaces\n\nimport luxai_s2.env\nfrom luxai_s2.env import LuxAI_S2\nfrom luxai_s2.state import ObservationStateDict\nfrom luxai_s2.unit import ActionType, BidActionType, FactoryPlacementActionType\nfrom luxai_s2.utils import my_turn_to_place_factory\nfrom luxai_s2.wrappers.controllers import (\n    Controller,\n)\n\n\nclass SB3Wrapper(gym.Wrapper):\n    def __init__(\n        self,\n        env: LuxAI_S2,\n        bid_policy: Callable[\n            [str, ObservationStateDict], Dict[str, BidActionType]\n        ] = None,\n        factory_placement_policy: Callable[\n            [str, ObservationStateDict], Dict[str, FactoryPlacementActionType]\n        ] = None,\n        controller: Controller = None,\n    ) -> None:\n        \"\"\"\n        A environment wrapper for Stable Baselines 3. It reduces the LuxAI_S2 env\n        into a single phase game and places the first two phases (bidding and factory placement) into the env.reset function so that\n        interacting agents directly start generating actions to play the third phase of the game.\n\n        It also accepts a Controller that translates action's in one action space to a Lux S2 compatible action\n\n        Parameters\n        ----------\n        bid_policy: Function\n            A function accepting player: str and obs: ObservationStateDict as input that returns a bid action\n            such as dict(bid=10, faction=\"AlphaStrike\"). By default will bid 0\n        factory_placement_policy: Function\n            A function accepting player: str and obs: ObservationStateDict as input that returns a factory placement action\n            such as dict(spawn=np.array([2, 4]), metal=150, water=150). By default will spawn in a random valid location with metal=150, water=150\n        controller : Controller\n            A controller that parameterizes the action space into something more usable and converts parameterized actions to lux actions.\n            See luxai_s2/wrappers/controllers.py for available controllers and how to make your own\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.env = env\n        \n        assert controller is not None\n        \n        # set our controller and replace the action space\n        self.controller = controller\n        self.action_space = controller.action_space\n\n        # The simplified wrapper removes the first two phases of the game by using predefined policies (trained or heuristic)\n        # to handle those two phases during each reset\n        if factory_placement_policy is None:\n            def factory_placement_policy(player, obs: ObservationStateDict):\n                potential_spawns = np.array(\n                    list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n                )\n                spawn_loc = potential_spawns[\n                    np.random.randint(0, len(potential_spawns))\n                ]\n                return dict(spawn=spawn_loc, metal=150, water=150)\n\n        self.factory_placement_policy = factory_placement_policy\n        if bid_policy is None:\n            def bid_policy(player, obs: ObservationStateDict):\n                faction = \"AlphaStrike\"\n                if player == \"player_1\":\n                    faction = \"MotherMars\"\n                return dict(bid=0, faction=faction)\n\n        self.bid_policy = bid_policy\n\n        self.prev_obs = None\n\n    def step(self, action: Dict[str, npt.NDArray]):\n        \n        # here, for each agent in the game we translate their action into a Lux S2 action\n        lux_action = dict()\n        for agent in self.env.agents:\n            if agent in action:\n                lux_action[agent] = self.controller.action_to_lux_action(\n                    agent=agent, obs=self.prev_obs, action=action[agent]\n                )\n            else:\n                lux_action[agent] = dict()\n        \n        # lux_action is now a dict mapping agent name to an action\n        obs, reward, done, info = self.env.step(lux_action)\n        self.prev_obs = obs\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        # we upgrade the reset function here\n        \n        # we call the original reset function first\n        obs = self.env.reset(**kwargs)\n        \n        # then use the bid policy to go through the bidding phase\n        action = dict()\n        for agent in self.env.agents:\n            action[agent] = self.bid_policy(agent, obs[agent])\n        obs, _, _, _ = self.env.step(action)\n        \n        # while real_env_steps < 0, we are in the factory placement phase\n        # so we use the factory placement policy to step through this\n        while self.env.state.real_env_steps < 0:\n            action = dict()\n            for agent in self.env.agents:\n                if my_turn_to_place_factory(\n                    obs[\"player_0\"][\"teams\"][agent][\"place_first\"],\n                    self.env.state.env_steps,\n                ):\n                    action[agent] = self.factory_placement_policy(agent, obs[agent])\n                else:\n                    action[agent] = dict()\n            obs, _, _, _ = self.env.step(action)\n        self.prev_obs = obs\n        \n        return obs\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:53:10.015548Z","iopub.execute_input":"2023-04-01T19:53:10.016543Z","iopub.status.idle":"2023-04-01T19:53:10.434239Z","shell.execute_reply.started":"2023-04-01T19:53:10.016507Z","shell.execute_reply":"2023-04-01T19:53:10.432686Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"pygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Defining a Bid and Factory Placement policy\n\nTo test the code above, we can program some heuristic bid and factory placement policies","metadata":{}},{"cell_type":"code","source":"def zero_bid(player, obs):\n    # a policy that always bids 0\n    faction = \"AlphaStrike\"\n    if player == \"player_1\":\n        faction = \"MotherMars\"\n    return dict(bid=0, faction=faction)\n\ndef place_near_random_ice(player, obs):\n    \"\"\"\n    This policy will place a single factory with all the starting resources\n    near a random ice tile\n    \"\"\"\n    if obs[\"teams\"][player][\"metal\"] == 0:\n        return dict()\n    potential_spawns = list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n    potential_spawns_set = set(potential_spawns)\n    done_search = False\n    \n    # simple numpy trick to find locations adjacent to ice tiles.\n    ice_diff = np.diff(obs[\"board\"][\"ice\"])\n    pot_ice_spots = np.argwhere(ice_diff == 1)\n    if len(pot_ice_spots) == 0:\n        pot_ice_spots = potential_spawns\n    \n    # pick a random ice spot and search around it for spawnable locations.\n    trials = 5\n    while trials > 0:\n        pos_idx = np.random.randint(0, len(pot_ice_spots))\n        pos = pot_ice_spots[pos_idx]\n        area = 3\n        for x in range(area):\n            for y in range(area):\n                check_pos = [pos[0] + x - area // 2, pos[1] + y - area // 2]\n                if tuple(check_pos) in potential_spawns_set:\n                    done_search = True\n                    pos = check_pos\n                    break\n            if done_search:\n                break\n        if done_search:\n            break\n        trials -= 1\n    \n    if not done_search:\n        spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]\n        pos = spawn_loc\n    \n    # this will spawn a factory at pos and with all the starting metal and water\n    metal = obs[\"teams\"][player][\"metal\"]\n    return dict(spawn=pos, metal=metal, water=metal)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:53:23.198238Z","iopub.execute_input":"2023-04-01T19:53:23.198956Z","iopub.status.idle":"2023-04-01T19:53:23.212528Z","shell.execute_reply.started":"2023-04-01T19:53:23.198920Z","shell.execute_reply":"2023-04-01T19:53:23.210666Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"So **without the wrapper**, when we reset the environment it looks like this:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nenv = gym.make(\"LuxAI_S2-v0\")\nenv.reset(seed=0)\nimg = env.render(\"rgb_array\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:53:34.347292Z","iopub.execute_input":"2023-04-01T19:53:34.347707Z","iopub.status.idle":"2023-04-01T19:53:34.655303Z","shell.execute_reply.started":"2023-04-01T19:53:34.347676Z","shell.execute_reply":"2023-04-01T19:53:34.653988Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f0d640443d0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"**With the wrapper**, when we reset the environment it looks like this:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nenv = gym.make(\"LuxAI_S2-v0\")\nenv = SB3Wrapper(env, zero_bid, place_near_random_ice, controller=SimpleUnitDiscreteController(env.env_cfg))\nenv.reset(seed=0)\nimg = env.render(\"rgb_array\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:53:42.528387Z","iopub.execute_input":"2023-04-01T19:53:42.529720Z","iopub.status.idle":"2023-04-01T19:53:43.829420Z","shell.execute_reply.started":"2023-04-01T19:53:42.529654Z","shell.execute_reply":"2023-04-01T19:53:43.827992Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f0d39207450>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Success! Our upgraded reset function makes the environment now start from the start of the normal game phase, meaning the action space can be consistently the same throughout the game.","metadata":{}},{"cell_type":"markdown","source":"## 3. Training with RL\n\nIn the previous tutorial, we saw how to train an agent with SB3 in single-agent environments. Handling true multi-agent via training separate or shared policies to control all agents requires a few extra things so instead, for the purpose of a tutorial we will treat Lux S2 like a single agent environment by training a policy for one team and letting the other team simply do nothing.\n\nMoreover, we want to define our own reward function to encourage our robots to seek ice, dig it, and return to a factory so it can generate water and survive longer. To do this all, we will just create a custom environment wrapper.\n\n\n","metadata":{}},{"cell_type":"code","source":"import copy\nclass CustomEnvWrapper(gym.Wrapper):\n    def __init__(self, env: gym.Env) -> None:\n        \"\"\"\n        Adds a custom reward and turns the LuxAI_S2 environment into a single-agent environment for easy training\n        \"\"\"\n        super().__init__(env)\n        self.prev_step_metrics = None\n\n    def step(self, action):\n        agent = \"player_0\"\n        opp_agent = \"player_1\"\n\n        opp_factories = self.env.state.factories[opp_agent]\n        for k in opp_factories.keys():\n            factory = opp_factories[k]\n             # set enemy factories to have 1000 water to keep them alive the whole around and treat the game as single-agent\n            factory.cargo.water = 1000\n\n        # submit actions for just one agent to make it single-agent\n        # and save single-agent versions of the data below\n        action = {agent: action}\n        obs, _, done, info = self.env.step(action)\n        obs = obs[agent]\n        done = done[agent]\n        \n        # we collect stats on teams here. These are useful stats that can be used to help generate reward functions\n        stats: StatsStateDict = self.env.state.stats[agent]\n\n        info = dict()\n        metrics = dict()\n        metrics[\"ice_dug\"] = (\n            stats[\"generation\"][\"ice\"][\"HEAVY\"] + stats[\"generation\"][\"ice\"][\"LIGHT\"]\n        )\n        metrics[\"water_produced\"] = stats[\"generation\"][\"water\"]\n\n        # we save these two to see often the agent updates robot action queues and how often enough\n        # power to do so and succeed (less frequent updates = more power is saved)\n        metrics[\"action_queue_updates_success\"] = stats[\"action_queue_updates_success\"]\n        metrics[\"action_queue_updates_total\"] = stats[\"action_queue_updates_total\"]\n\n        # we can save the metrics to info so we can use tensorboard to log them to get a glimpse into how our agent is behaving\n        info[\"metrics\"] = metrics\n\n        reward = 0\n        if self.prev_step_metrics is not None:\n            # we check how much ice and water is produced and reward the agent for generating both\n            ice_dug_this_step = metrics[\"ice_dug\"] - self.prev_step_metrics[\"ice_dug\"]\n            water_produced_this_step = (\n                metrics[\"water_produced\"] - self.prev_step_metrics[\"water_produced\"]\n            )\n            # we reward water production more as it is the most important resource for survival\n            reward = ice_dug_this_step / 100 + water_produced_this_step\n\n        self.prev_step_metrics = copy.deepcopy(metrics)\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        obs = self.env.reset(**kwargs)[\"player_0\"]\n        self.prev_step_metrics = None\n        return obs","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:53:58.266378Z","iopub.execute_input":"2023-04-01T19:53:58.266844Z","iopub.status.idle":"2023-04-01T19:53:58.281199Z","shell.execute_reply.started":"2023-04-01T19:53:58.266806Z","shell.execute_reply":"2023-04-01T19:53:58.280205Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 Defining the Environment and using Wrappers","metadata":{}},{"cell_type":"markdown","source":"Next, we will define a `make_env` function and use it with SB3 to create multiple environments in parallel that scale with the number of CPU cores you have. A future tutorial will show a variant that creates a single jax-powered environment to achieve the same functionality but scaling with GPU.\n\nWe will use the SB3Wrapper, the controller and observation wrapper we defined, and the custom env wrapper as well. These put together will give us an environment that resets to the start of the normal game phase, has a consistent and simplified observation and action space, and contains our reward function.","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.monitor import Monitor\nfrom gym.wrappers import TimeLimit\ndef make_env(env_id: str, rank: int, seed: int = 0, max_episode_steps=200):\n    def _init() -> gym.Env:\n        # verbose = 0\n        # collect_stats=True lets us track stats like total ice dug during an episode to help create reward functions\n        # max factories set to 2 for simplification and keeping returns consistent as we survive longer \n        # if there are more initial resources\n        env = gym.make(env_id, verbose=0, collect_stats=True, MAX_FACTORIES=2)\n\n        # Add a SB3 wrapper to make it work with SB3 and simplify the action space with the controller\n        # this will remove the bidding phase and factory placement phase. For factory placement we use\n        # the provided place_near_random_ice function which will randomly select an ice tile and place a factory near it.\n        env = SB3Wrapper(\n            env,\n            factory_placement_policy=place_near_random_ice,\n            controller=SimpleUnitDiscreteController(env.env_cfg),\n        )\n        \n        # changes observation to include a few simple features\n        env = SimpleUnitObservationWrapper(\n            env\n        )\n        \n        # convert to single agent, adds our reward\n        env = CustomEnvWrapper(env)  \n        \n        # Add a timelimit to the environment, which can truncate episodes, speed up training\n        env = TimeLimit(\n            env, max_episode_steps=max_episode_steps\n        )\n        env = Monitor(env) # for SB3 to allow it to record metrics\n        env.reset(seed=seed + rank)\n        set_random_seed(seed)\n        return env\n\n    return _init","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:56:01.546806Z","iopub.execute_input":"2023-04-01T19:56:01.547674Z","iopub.status.idle":"2023-04-01T19:56:01.558032Z","shell.execute_reply.started":"2023-04-01T19:56:01.547635Z","shell.execute_reply":"2023-04-01T19:56:01.556833Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Next we will define a useful callback function to log some of the custom metrics we defined earlier in the CustomEnvWrapper","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\nclass TensorboardCallback(BaseCallback):\n    def __init__(self, tag: str, verbose=0):\n        super().__init__(verbose)\n        self.tag = tag\n\n    def _on_step(self) -> bool:\n        c = 0\n\n        for i, done in enumerate(self.locals[\"dones\"]):\n            if done:\n                info = self.locals[\"infos\"][i]\n                c += 1\n                for k in info[\"metrics\"]:\n                    stat = info[\"metrics\"][k]\n                    self.logger.record_mean(f\"{self.tag}/{k}\", stat)\n        return True","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:56:06.479461Z","iopub.execute_input":"2023-04-01T19:56:06.480012Z","iopub.status.idle":"2023-04-01T19:56:06.489260Z","shell.execute_reply.started":"2023-04-01T19:56:06.479972Z","shell.execute_reply":"2023-04-01T19:56:06.487794Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Training Setup\n\nNow we can prepare for training by creating training and evaluation environments, as well as defining our algorithm and model.","metadata":{}},{"cell_type":"code","source":"import os.path as osp\nfrom stable_baselines3.common.utils import set_random_seed\nfrom stable_baselines3.ppo import PPO\n\nset_random_seed(42)\nlog_path = \"logs/exp_1\"\nnum_envs = 4\n\n# set max episode steps to 200 for training environments to train faster\nenv = SubprocVecEnv([make_env(\"LuxAI_S2-v0\", i, max_episode_steps=200) for i in range(num_envs)])\nenv.reset()\n# set max episode steps to 1000 to match original environment\neval_env = SubprocVecEnv([make_env(\"LuxAI_S2-v0\", i, max_episode_steps=1000) for i in range(4)])\neval_env.reset()\nrollout_steps = 4000\npolicy_kwargs = dict(net_arch=(128, 128))\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    n_steps=rollout_steps // num_envs,\n    batch_size=800,\n    learning_rate=3e-4,\n    policy_kwargs=policy_kwargs,\n    verbose=1,\n    n_epochs=2,\n    target_kl=0.05,\n    gamma=0.99,\n    tensorboard_log=osp.join(log_path),\n)\n\neval_callback = EvalCallback(\n    eval_env,\n    best_model_save_path=osp.join(log_path, \"models\"),\n    log_path=osp.join(log_path, \"eval_logs\"),\n    eval_freq=24_000,\n    deterministic=False,\n    render=False,\n    n_eval_episodes=5,\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:56:09.306241Z","iopub.execute_input":"2023-04-01T19:56:09.306908Z","iopub.status.idle":"2023-04-01T19:56:19.740661Z","shell.execute_reply.started":"2023-04-01T19:56:09.306870Z","shell.execute_reply":"2023-04-01T19:56:19.739514Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Using cpu device\n","output_type":"stream"}]},{"cell_type":"markdown","source":"With our callback functions and model defined, we can now begin training using `model.learn`. On CPU this training can take around 3-4 hours to train, on GPU it can take 30min to an hour to train. The hyperparameters and reward function can be improved to make it train much faster. A simple way to also increase training speed is to train on a machine with more CPU cores and increasing `num_envs` above. Kaggle notebooks by default only have 4, but with e.g. 10 you can easily train a policy in around 30 minutes.\n\nIf you want to skip this training you can also just use the pretrained model that's in the downloaded dataset for the RL kit called `best_model.dontunzipme`. (kaggle auto unzips files but we need to keep it as a zip so the file extention is called .dontunzipme but for submission just change it to a .zip)\n\nTo track the progress we recommend using tensorboard which you can run with\n```\ntensorboard --logdir logs\n```","metadata":{}},{"cell_type":"code","source":"total_timesteps = 10_000_000\nmodel.learn(\n    total_timesteps,\n    callback=[TensorboardCallback(tag=\"train_metrics\"), eval_callback],\n)\nmodel.save(osp.join(log_path, \"models/latest_model\"))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-01T19:56:34.570041Z","iopub.execute_input":"2023-04-01T19:56:34.570699Z","iopub.status.idle":"2023-04-01T20:36:13.414404Z","shell.execute_reply.started":"2023-04-01T19:56:34.570649Z","shell.execute_reply":"2023-04-01T20:36:13.411996Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Logging to logs/exp_1/PPO_1\n----------------------------------------------\n| rollout/                        |          |\n|    ep_len_mean                  | 200      |\n|    ep_rew_mean                  | 0        |\n| time/                           |          |\n|    fps                          | 465      |\n|    iterations                   | 1        |\n|    time_elapsed                 | 8        |\n|    total_timesteps              | 4000     |\n| train_metrics/                  |          |\n|    action_queue_updates_success | 145      |\n|    action_queue_updates_total   | 177      |\n|    ice_dug                      | 0        |\n|    water_produced               | 0        |\n----------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.02          |\n| time/                           |               |\n|    fps                          | 623           |\n|    iterations                   | 2             |\n|    time_elapsed                 | 12            |\n|    total_timesteps              | 8000          |\n| train/                          |               |\n|    approx_kl                    | 0.00025716395 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.843         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.000655     |\n|    n_updates                    | 2             |\n|    policy_gradient_loss         | -0.000534     |\n|    value_loss                   | 0.000729      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 4             |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.0133        |\n| time/                           |               |\n|    fps                          | 711           |\n|    iterations                   | 3             |\n|    time_elapsed                 | 16            |\n|    total_timesteps              | 12000         |\n| train/                          |               |\n|    approx_kl                    | 9.9918936e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.363         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00112      |\n|    n_updates                    | 4             |\n|    policy_gradient_loss         | -0.000431     |\n|    value_loss                   | 0.000874      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.075        |\n| time/                           |              |\n|    fps                          | 755          |\n|    iterations                   | 4            |\n|    time_elapsed                 | 21           |\n|    total_timesteps              | 16000        |\n| train/                          |              |\n|    approx_kl                    | 0.0001520467 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.48        |\n|    explained_variance           | 0.945        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00105     |\n|    n_updates                    | 6            |\n|    policy_gradient_loss         | -0.000384    |\n|    value_loss                   | 2.93e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 1            |\n|    water_produced               | 0.25         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.06          |\n| time/                           |               |\n|    fps                          | 784           |\n|    iterations                   | 5             |\n|    time_elapsed                 | 25            |\n|    total_timesteps              | 20000         |\n| train/                          |               |\n|    approx_kl                    | 0.00014432374 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.000437      |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.0395        |\n|    n_updates                    | 8             |\n|    policy_gradient_loss         | -0.00028      |\n|    value_loss                   | 0.0566        |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.06         |\n| time/                           |              |\n|    fps                          | 805          |\n|    iterations                   | 6            |\n|    time_elapsed                 | 29           |\n|    total_timesteps              | 24000        |\n| train/                          |              |\n|    approx_kl                    | 7.736915e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.48        |\n|    explained_variance           | 0.926        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00122     |\n|    n_updates                    | 10           |\n|    policy_gradient_loss         | -0.000353    |\n|    value_loss                   | 0.000268     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 149          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.052         |\n| time/                           |               |\n|    fps                          | 817           |\n|    iterations                   | 7             |\n|    time_elapsed                 | 34            |\n|    total_timesteps              | 28000         |\n| train/                          |               |\n|    approx_kl                    | 0.00023715764 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.824         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00399      |\n|    n_updates                    | 12            |\n|    policy_gradient_loss         | -0.00151      |\n|    value_loss                   | 5.99e-05      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.056        |\n| time/                           |              |\n|    fps                          | 831          |\n|    iterations                   | 8            |\n|    time_elapsed                 | 38           |\n|    total_timesteps              | 32000        |\n| train/                          |              |\n|    approx_kl                    | 0.0005054677 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.48        |\n|    explained_variance           | 0.924        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00543     |\n|    n_updates                    | 14           |\n|    policy_gradient_loss         | -0.00253     |\n|    value_loss                   | 1.45e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 175          |\n|    ice_dug                      | 2            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.008         |\n| time/                           |               |\n|    fps                          | 842           |\n|    iterations                   | 9             |\n|    time_elapsed                 | 42            |\n|    total_timesteps              | 36000         |\n| train/                          |               |\n|    approx_kl                    | 0.00033363188 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.33          |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00186      |\n|    n_updates                    | 16            |\n|    policy_gradient_loss         | -0.000427     |\n|    value_loss                   | 0.000187      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 175           |\n|    ice_dug                      | 2             |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.008         |\n| time/                           |               |\n|    fps                          | 851           |\n|    iterations                   | 10            |\n|    time_elapsed                 | 46            |\n|    total_timesteps              | 40000         |\n| train/                          |               |\n|    approx_kl                    | 0.00030649855 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.2           |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.0022       |\n|    n_updates                    | 18            |\n|    policy_gradient_loss         | -0.000971     |\n|    value_loss                   | 0.000312      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 175           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.01        |\n| time/                           |             |\n|    fps                          | 856         |\n|    iterations                   | 11          |\n|    time_elapsed                 | 51          |\n|    total_timesteps              | 44000       |\n| train/                          |             |\n|    approx_kl                    | 0.000877353 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.47       |\n|    explained_variance           | 0.931       |\n|    learning_rate                | 0.0003      |\n|    loss                         | -0.00712    |\n|    n_updates                    | 20          |\n|    policy_gradient_loss         | -0.00295    |\n|    value_loss                   | 6.81e-06    |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 132         |\n|    action_queue_updates_total   | 174         |\n|    ice_dug                      | 1           |\n|    water_produced               | 0           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.01         |\n| time/                           |              |\n|    fps                          | 862          |\n|    iterations                   | 12           |\n|    time_elapsed                 | 55           |\n|    total_timesteps              | 48000        |\n| train/                          |              |\n|    approx_kl                    | 0.0011578674 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.46        |\n|    explained_variance           | 0.371        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00181     |\n|    n_updates                    | 22           |\n|    policy_gradient_loss         | -0.000925    |\n|    value_loss                   | 7.56e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 174          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.006       |\n| time/                           |             |\n|    fps                          | 866         |\n|    iterations                   | 13          |\n|    time_elapsed                 | 59          |\n|    total_timesteps              | 52000       |\n| train/                          |             |\n|    approx_kl                    | 0.002232566 |\n|    clip_fraction                | 0.003       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.44       |\n|    explained_variance           | 0.898       |\n|    learning_rate                | 0.0003      |\n|    loss                         | -0.0102     |\n|    n_updates                    | 24          |\n|    policy_gradient_loss         | -0.00382    |\n|    value_loss                   | 5.88e-06    |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 132         |\n|    action_queue_updates_total   | 175         |\n|    ice_dug                      | 0           |\n|    water_produced               | 0           |\n-------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 0.002      |\n| time/                           |            |\n|    fps                          | 863        |\n|    iterations                   | 14         |\n|    time_elapsed                 | 64         |\n|    total_timesteps              | 56000      |\n| train/                          |            |\n|    approx_kl                    | 0.00616081 |\n|    clip_fraction                | 0.0591     |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -2.4       |\n|    explained_variance           | 0.908      |\n|    learning_rate                | 0.0003     |\n|    loss                         | -0.00325   |\n|    n_updates                    | 26         |\n|    policy_gradient_loss         | -0.00513   |\n|    value_loss                   | 3.88e-06   |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 121        |\n|    action_queue_updates_total   | 171        |\n|    ice_dug                      | 0          |\n|    water_produced               | 0          |\n------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.004        |\n| time/                           |              |\n|    fps                          | 869          |\n|    iterations                   | 15           |\n|    time_elapsed                 | 68           |\n|    total_timesteps              | 60000        |\n| train/                          |              |\n|    approx_kl                    | 0.0025576144 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.36        |\n|    explained_variance           | 0.844        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00746     |\n|    n_updates                    | 28           |\n|    policy_gradient_loss         | -0.00235     |\n|    value_loss                   | 4e-06        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 118          |\n|    action_queue_updates_total   | 168          |\n|    ice_dug                      | 1            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.056        |\n| time/                           |              |\n|    fps                          | 873          |\n|    iterations                   | 16           |\n|    time_elapsed                 | 73           |\n|    total_timesteps              | 64000        |\n| train/                          |              |\n|    approx_kl                    | 0.0026448176 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.29        |\n|    explained_variance           | 0.163        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.00305      |\n|    n_updates                    | 30           |\n|    policy_gradient_loss         | 0.000345     |\n|    value_loss                   | 8.15e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 120          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 2            |\n|    water_produced               | 0.25         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.056         |\n| time/                           |               |\n|    fps                          | 875           |\n|    iterations                   | 17            |\n|    time_elapsed                 | 77            |\n|    total_timesteps              | 68000         |\n| train/                          |               |\n|    approx_kl                    | 0.00060837384 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.000745      |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.0319        |\n|    n_updates                    | 32            |\n|    policy_gradient_loss         | -0.00047      |\n|    value_loss                   | 0.0473        |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 118           |\n|    action_queue_updates_total   | 164           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.056        |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 18           |\n|    time_elapsed                 | 82           |\n|    total_timesteps              | 72000        |\n| train/                          |              |\n|    approx_kl                    | 0.0003276844 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.24        |\n|    explained_variance           | 0.891        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.000166    |\n|    n_updates                    | 34           |\n|    policy_gradient_loss         | -3.8e-05     |\n|    value_loss                   | 0.000133     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 117          |\n|    action_queue_updates_total   | 165          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.056        |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 19           |\n|    time_elapsed                 | 86           |\n|    total_timesteps              | 76000        |\n| train/                          |              |\n|    approx_kl                    | 0.0017416099 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.31        |\n|    explained_variance           | 0.722        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00485     |\n|    n_updates                    | 36           |\n|    policy_gradient_loss         | -0.00195     |\n|    value_loss                   | 6.42e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 127          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.054       |\n| time/                           |             |\n|    fps                          | 875         |\n|    iterations                   | 20          |\n|    time_elapsed                 | 91          |\n|    total_timesteps              | 80000       |\n| train/                          |             |\n|    approx_kl                    | 0.003034997 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.32       |\n|    explained_variance           | 0.86        |\n|    learning_rate                | 0.0003      |\n|    loss                         | -0.00174    |\n|    n_updates                    | 38          |\n|    policy_gradient_loss         | -0.00257    |\n|    value_loss                   | 2.84e-05    |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 125         |\n|    action_queue_updates_total   | 168         |\n|    ice_dug                      | 0           |\n|    water_produced               | 0           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.002        |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 21           |\n|    time_elapsed                 | 95           |\n|    total_timesteps              | 84000        |\n| train/                          |              |\n|    approx_kl                    | 0.0010488753 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.37        |\n|    explained_variance           | 0.903        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.0017       |\n|    n_updates                    | 40           |\n|    policy_gradient_loss         | 0.00017      |\n|    value_loss                   | 1.63e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 1            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.002        |\n| time/                           |              |\n|    fps                          | 877          |\n|    iterations                   | 22           |\n|    time_elapsed                 | 100          |\n|    total_timesteps              | 88000        |\n| train/                          |              |\n|    approx_kl                    | 0.0002110318 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.37        |\n|    explained_variance           | 0.491        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.000476    |\n|    n_updates                    | 42           |\n|    policy_gradient_loss         | -0.000178    |\n|    value_loss                   | 8.75e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.006         |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 23            |\n|    time_elapsed                 | 104           |\n|    total_timesteps              | 92000         |\n| train/                          |               |\n|    approx_kl                    | 0.00034651073 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.37         |\n|    explained_variance           | 0.894         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00256      |\n|    n_updates                    | 44            |\n|    policy_gradient_loss         | -0.00077      |\n|    value_loss                   | 8.5e-06       |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 168           |\n|    ice_dug                      | 2             |\n|    water_produced               | 0             |\n---------------------------------------------------\nEval num_timesteps=96000, episode_reward=0.04 +/- 0.08\nEpisode length: 301.00 +/- 0.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 301           |\n|    mean_reward                  | 0.04          |\n| time/                           |               |\n|    total_timesteps              | 96000         |\n| train/                          |               |\n|    approx_kl                    | 0.00035094516 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.35         |\n|    explained_variance           | 0.468         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00106      |\n|    n_updates                    | 46            |\n|    policy_gradient_loss         | -0.000159     |\n|    value_loss                   | 0.000198      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 168           |\n|    ice_dug                      | 1             |\n|    water_produced               | 0             |\n---------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 0.008    |\n| time/              |          |\n|    fps             | 856      |\n|    iterations      | 24       |\n|    time_elapsed    | 112      |\n|    total_timesteps | 96000    |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.028        |\n| time/                           |              |\n|    fps                          | 857          |\n|    iterations                   | 25           |\n|    time_elapsed                 | 116          |\n|    total_timesteps              | 100000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008710647 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.37        |\n|    explained_variance           | 0.245        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00485     |\n|    n_updates                    | 48           |\n|    policy_gradient_loss         | -0.00137     |\n|    value_loss                   | 8.9e-05      |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 10           |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.034        |\n| time/                           |              |\n|    fps                          | 860          |\n|    iterations                   | 26           |\n|    time_elapsed                 | 120          |\n|    total_timesteps              | 104000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008234091 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.38        |\n|    explained_variance           | 0.131        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.00152      |\n|    n_updates                    | 50           |\n|    policy_gradient_loss         | -0.000816    |\n|    value_loss                   | 0.00182      |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 4            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.046        |\n| time/                           |              |\n|    fps                          | 860          |\n|    iterations                   | 27           |\n|    time_elapsed                 | 125          |\n|    total_timesteps              | 108000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008310125 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.37        |\n|    explained_variance           | 0.237        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00501     |\n|    n_updates                    | 52           |\n|    policy_gradient_loss         | -0.00122     |\n|    value_loss                   | 0.000898     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 6            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.05         |\n| time/                           |              |\n|    fps                          | 860          |\n|    iterations                   | 28           |\n|    time_elapsed                 | 130          |\n|    total_timesteps              | 112000       |\n| train/                          |              |\n|    approx_kl                    | 0.0013095809 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.35        |\n|    explained_variance           | 0.289        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.000849     |\n|    n_updates                    | 54           |\n|    policy_gradient_loss         | -0.00138     |\n|    value_loss                   | 0.000843     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 122          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 4            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.07         |\n| time/                           |              |\n|    fps                          | 860          |\n|    iterations                   | 29           |\n|    time_elapsed                 | 134          |\n|    total_timesteps              | 116000       |\n| train/                          |              |\n|    approx_kl                    | 0.0017493216 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.32        |\n|    explained_variance           | 0.195        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.000718     |\n|    n_updates                    | 56           |\n|    policy_gradient_loss         | -0.000558    |\n|    value_loss                   | 0.000536     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 11           |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.052        |\n| time/                           |              |\n|    fps                          | 860          |\n|    iterations                   | 30           |\n|    time_elapsed                 | 139          |\n|    total_timesteps              | 120000       |\n| train/                          |              |\n|    approx_kl                    | 0.0013841867 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.28        |\n|    explained_variance           | 0.134        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.00214      |\n|    n_updates                    | 58           |\n|    policy_gradient_loss         | -0.000754    |\n|    value_loss                   | 0.00445      |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 1            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.104        |\n| time/                           |              |\n|    fps                          | 861          |\n|    iterations                   | 31           |\n|    time_elapsed                 | 143          |\n|    total_timesteps              | 124000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016302329 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.27        |\n|    explained_variance           | 0.772        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00248     |\n|    n_updates                    | 60           |\n|    policy_gradient_loss         | -0.00223     |\n|    value_loss                   | 0.000326     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 163          |\n|    ice_dug                      | 5            |\n|    water_produced               | 0.25         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.18        |\n| time/                           |             |\n|    fps                          | 861         |\n|    iterations                   | 32          |\n|    time_elapsed                 | 148         |\n|    total_timesteps              | 128000      |\n| train/                          |             |\n|    approx_kl                    | 0.000806186 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.23       |\n|    explained_variance           | 0.0181      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 0.0375      |\n|    n_updates                    | 62          |\n|    policy_gradient_loss         | 0.000137    |\n|    value_loss                   | 0.0598      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 140         |\n|    action_queue_updates_total   | 164         |\n|    ice_dug                      | 19          |\n|    water_produced               | 0.25        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.318       |\n| time/                           |             |\n|    fps                          | 861         |\n|    iterations                   | 33          |\n|    time_elapsed                 | 153         |\n|    total_timesteps              | 132000      |\n| train/                          |             |\n|    approx_kl                    | 0.001065569 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.22       |\n|    explained_variance           | 0.0271      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 0.0328      |\n|    n_updates                    | 64          |\n|    policy_gradient_loss         | -0.00134    |\n|    value_loss                   | 0.0623      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 135         |\n|    action_queue_updates_total   | 163         |\n|    ice_dug                      | 48          |\n|    water_produced               | 0.25        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.36         |\n| time/                           |              |\n|    fps                          | 861          |\n|    iterations                   | 34           |\n|    time_elapsed                 | 157          |\n|    total_timesteps              | 136000       |\n| train/                          |              |\n|    approx_kl                    | 0.0014423163 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.21        |\n|    explained_variance           | -0.0109      |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.0462       |\n|    n_updates                    | 66           |\n|    policy_gradient_loss         | -7.75e-05    |\n|    value_loss                   | 0.0869       |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 32           |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.602        |\n| time/                           |              |\n|    fps                          | 861          |\n|    iterations                   | 35           |\n|    time_elapsed                 | 162          |\n|    total_timesteps              | 140000       |\n| train/                          |              |\n|    approx_kl                    | 0.0017468471 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.16        |\n|    explained_variance           | 0.202        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.00301      |\n|    n_updates                    | 68           |\n|    policy_gradient_loss         | -0.00102     |\n|    value_loss                   | 0.00888      |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 22           |\n|    water_produced               | 1            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.682        |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 36           |\n|    time_elapsed                 | 166          |\n|    total_timesteps              | 144000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016584389 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.09        |\n|    explained_variance           | 0.0387       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.208        |\n|    n_updates                    | 70           |\n|    policy_gradient_loss         | -0.000939    |\n|    value_loss                   | 0.343        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 20           |\n|    water_produced               | 0.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.742        |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 37           |\n|    time_elapsed                 | 171          |\n|    total_timesteps              | 148000       |\n| train/                          |              |\n|    approx_kl                    | 0.0010056595 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.05        |\n|    explained_variance           | 0.201        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.0601       |\n|    n_updates                    | 72           |\n|    policy_gradient_loss         | -0.000795    |\n|    value_loss                   | 0.128        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 24           |\n|    water_produced               | 0.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.848        |\n| time/                           |              |\n|    fps                          | 864          |\n|    iterations                   | 38           |\n|    time_elapsed                 | 175          |\n|    total_timesteps              | 152000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016303925 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.06        |\n|    explained_variance           | 0.0882       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.15         |\n|    n_updates                    | 74           |\n|    policy_gradient_loss         | -0.000931    |\n|    value_loss                   | 0.203        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 26           |\n|    water_produced               | 1            |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 0.898      |\n| time/                           |            |\n|    fps                          | 865        |\n|    iterations                   | 39         |\n|    time_elapsed                 | 180        |\n|    total_timesteps              | 156000     |\n| train/                          |            |\n|    approx_kl                    | 0.00164708 |\n|    clip_fraction                | 0          |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -2.03      |\n|    explained_variance           | 0.02       |\n|    learning_rate                | 0.0003     |\n|    loss                         | 0.107      |\n|    n_updates                    | 76         |\n|    policy_gradient_loss         | -0.00143   |\n|    value_loss                   | 0.235      |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 138        |\n|    action_queue_updates_total   | 157        |\n|    ice_dug                      | 7          |\n|    water_produced               | 0.5        |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 1.8         |\n| time/                           |             |\n|    fps                          | 867         |\n|    iterations                   | 40          |\n|    time_elapsed                 | 184         |\n|    total_timesteps              | 160000      |\n| train/                          |             |\n|    approx_kl                    | 0.003974599 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.92       |\n|    explained_variance           | 0.0994      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 0.103       |\n|    n_updates                    | 78          |\n|    policy_gradient_loss         | -0.00125    |\n|    value_loss                   | 0.23        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 134         |\n|    action_queue_updates_total   | 152         |\n|    ice_dug                      | 75          |\n|    water_produced               | 5           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 3.27         |\n| time/                           |              |\n|    fps                          | 867          |\n|    iterations                   | 41           |\n|    time_elapsed                 | 189          |\n|    total_timesteps              | 164000       |\n| train/                          |              |\n|    approx_kl                    | 0.0026435042 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.88        |\n|    explained_variance           | 0.0301       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.19         |\n|    n_updates                    | 80           |\n|    policy_gradient_loss         | -0.00119     |\n|    value_loss                   | 5.14         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 151          |\n|    water_produced               | 6.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 4.82         |\n| time/                           |              |\n|    fps                          | 868          |\n|    iterations                   | 42           |\n|    time_elapsed                 | 193          |\n|    total_timesteps              | 168000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008459856 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.0403       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 3.78         |\n|    n_updates                    | 82           |\n|    policy_gradient_loss         | -0.000208    |\n|    value_loss                   | 8.27         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 123          |\n|    action_queue_updates_total   | 138          |\n|    ice_dug                      | 101          |\n|    water_produced               | 7.5          |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 5.5        |\n| time/                           |            |\n|    fps                          | 868        |\n|    iterations                   | 43         |\n|    time_elapsed                 | 198        |\n|    total_timesteps              | 172000     |\n| train/                          |            |\n|    approx_kl                    | 0.00048582 |\n|    clip_fraction                | 0          |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.73      |\n|    explained_variance           | 0.0321     |\n|    learning_rate                | 0.0003     |\n|    loss                         | 5.85       |\n|    n_updates                    | 84         |\n|    policy_gradient_loss         | -0.000748  |\n|    value_loss                   | 12.4       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 120        |\n|    action_queue_updates_total   | 144        |\n|    ice_dug                      | 64         |\n|    water_produced               | 4          |\n------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.76          |\n| time/                           |               |\n|    fps                          | 869           |\n|    iterations                   | 44            |\n|    time_elapsed                 | 202           |\n|    total_timesteps              | 176000        |\n| train/                          |               |\n|    approx_kl                    | 0.00069236057 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.106         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.9           |\n|    n_updates                    | 86            |\n|    policy_gradient_loss         | -0.000522     |\n|    value_loss                   | 4.81          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 40            |\n|    water_produced               | 1.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 8.15         |\n| time/                           |              |\n|    fps                          | 870          |\n|    iterations                   | 45           |\n|    time_elapsed                 | 206          |\n|    total_timesteps              | 180000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015396948 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.82        |\n|    explained_variance           | 0.17         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.29         |\n|    n_updates                    | 88           |\n|    policy_gradient_loss         | -0.000787    |\n|    value_loss                   | 2.14         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 147          |\n|    ice_dug                      | 145          |\n|    water_produced               | 16.2         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 7.65        |\n| time/                           |             |\n|    fps                          | 871         |\n|    iterations                   | 46          |\n|    time_elapsed                 | 211         |\n|    total_timesteps              | 184000      |\n| train/                          |             |\n|    approx_kl                    | 0.000665737 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.86       |\n|    explained_variance           | 0.0594      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 23.4        |\n|    n_updates                    | 90          |\n|    policy_gradient_loss         | -0.000483   |\n|    value_loss                   | 38.3        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 140         |\n|    action_queue_updates_total   | 154         |\n|    ice_dug                      | 75          |\n|    water_produced               | 4.75        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.28          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 47            |\n|    time_elapsed                 | 215           |\n|    total_timesteps              | 188000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020301346 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.86         |\n|    explained_variance           | 0.0657        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.67          |\n|    n_updates                    | 92            |\n|    policy_gradient_loss         | -0.00022      |\n|    value_loss                   | 7.82          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 141           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 64            |\n|    water_produced               | 1             |\n---------------------------------------------------\nEval num_timesteps=192000, episode_reward=13.16 +/- 20.69\nEpisode length: 313.00 +/- 19.39\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 313           |\n|    mean_reward                  | 13.2          |\n| time/                           |               |\n|    total_timesteps              | 192000        |\n| train/                          |               |\n|    approx_kl                    | 0.00025811998 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.88         |\n|    explained_variance           | 0.466         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.14          |\n|    n_updates                    | 94            |\n|    policy_gradient_loss         | -0.00049      |\n|    value_loss                   | 0.755         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 64            |\n|    water_produced               | 8             |\n---------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 7.08     |\n| time/              |          |\n|    fps             | 861      |\n|    iterations      | 48       |\n|    time_elapsed    | 222      |\n|    total_timesteps | 192000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7.95         |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 49           |\n|    time_elapsed                 | 227          |\n|    total_timesteps              | 196000       |\n| train/                          |              |\n|    approx_kl                    | 8.083284e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.88        |\n|    explained_variance           | 0.0969       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 5.68         |\n|    n_updates                    | 96           |\n|    policy_gradient_loss         | 8.58e-05     |\n|    value_loss                   | 12.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 75           |\n|    water_produced               | 5.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.44          |\n| time/                           |               |\n|    fps                          | 864           |\n|    iterations                   | 50            |\n|    time_elapsed                 | 231           |\n|    total_timesteps              | 200000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021829375 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.86         |\n|    explained_variance           | 0.121         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.2           |\n|    n_updates                    | 98            |\n|    policy_gradient_loss         | -0.000146     |\n|    value_loss                   | 8.82          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 17            |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.32          |\n| time/                           |               |\n|    fps                          | 866           |\n|    iterations                   | 51            |\n|    time_elapsed                 | 235           |\n|    total_timesteps              | 204000        |\n| train/                          |               |\n|    approx_kl                    | 0.00073647796 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.835         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.109         |\n|    n_updates                    | 100           |\n|    policy_gradient_loss         | 0.000133      |\n|    value_loss                   | 0.237         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 139           |\n|    water_produced               | 13.5          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 7.09        |\n| time/                           |             |\n|    fps                          | 867         |\n|    iterations                   | 52          |\n|    time_elapsed                 | 239         |\n|    total_timesteps              | 208000      |\n| train/                          |             |\n|    approx_kl                    | 0.000326183 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.84       |\n|    explained_variance           | 0.0994      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 9.65        |\n|    n_updates                    | 102         |\n|    policy_gradient_loss         | -0.000251   |\n|    value_loss                   | 26.3        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 136         |\n|    action_queue_updates_total   | 150         |\n|    ice_dug                      | 75          |\n|    water_produced               | 4.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 6.27         |\n| time/                           |              |\n|    fps                          | 868          |\n|    iterations                   | 53           |\n|    time_elapsed                 | 244          |\n|    total_timesteps              | 212000       |\n| train/                          |              |\n|    approx_kl                    | 0.0004068938 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.78        |\n|    explained_variance           | 0.176        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.15         |\n|    n_updates                    | 104          |\n|    policy_gradient_loss         | -0.000438    |\n|    value_loss                   | 4.45         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 77           |\n|    water_produced               | 3.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 5.03         |\n| time/                           |              |\n|    fps                          | 869          |\n|    iterations                   | 54           |\n|    time_elapsed                 | 248          |\n|    total_timesteps              | 216000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001394609 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.81        |\n|    explained_variance           | 0.213        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.19         |\n|    n_updates                    | 106          |\n|    policy_gradient_loss         | -0.000283    |\n|    value_loss                   | 4.37         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 6            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.75          |\n| time/                           |               |\n|    fps                          | 868           |\n|    iterations                   | 55            |\n|    time_elapsed                 | 253           |\n|    total_timesteps              | 220000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019217537 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.826         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.0725        |\n|    n_updates                    | 108           |\n|    policy_gradient_loss         | -0.000117     |\n|    value_loss                   | 0.261         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 54            |\n|    water_produced               | 3.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 3.51          |\n| time/                           |               |\n|    fps                          | 868           |\n|    iterations                   | 56            |\n|    time_elapsed                 | 257           |\n|    total_timesteps              | 224000        |\n| train/                          |               |\n|    approx_kl                    | 0.00031904533 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.264         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.16          |\n|    n_updates                    | 110           |\n|    policy_gradient_loss         | -0.0005       |\n|    value_loss                   | 2.38          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 41            |\n|    water_produced               | 3.25          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 3.84        |\n| time/                           |             |\n|    fps                          | 868         |\n|    iterations                   | 57          |\n|    time_elapsed                 | 262         |\n|    total_timesteps              | 228000      |\n| train/                          |             |\n|    approx_kl                    | 0.001044607 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.74       |\n|    explained_variance           | 0.113       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 2.15        |\n|    n_updates                    | 112         |\n|    policy_gradient_loss         | -0.000722   |\n|    value_loss                   | 3.46        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 137         |\n|    action_queue_updates_total   | 148         |\n|    ice_dug                      | 69          |\n|    water_produced               | 6.5         |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.89          |\n| time/                           |               |\n|    fps                          | 869           |\n|    iterations                   | 58            |\n|    time_elapsed                 | 266           |\n|    total_timesteps              | 232000        |\n| train/                          |               |\n|    approx_kl                    | 0.00084565266 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.104         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.46          |\n|    n_updates                    | 114           |\n|    policy_gradient_loss         | -0.00025      |\n|    value_loss                   | 10.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 173           |\n|    water_produced               | 28            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.9           |\n| time/                           |               |\n|    fps                          | 870           |\n|    iterations                   | 59            |\n|    time_elapsed                 | 271           |\n|    total_timesteps              | 236000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020317236 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.73         |\n|    explained_variance           | 0.0496        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 32.1          |\n|    n_updates                    | 116           |\n|    policy_gradient_loss         | -0.000433     |\n|    value_loss                   | 50.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 11            |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.6          |\n| time/                           |               |\n|    fps                          | 870           |\n|    iterations                   | 60            |\n|    time_elapsed                 | 275           |\n|    total_timesteps              | 240000        |\n| train/                          |               |\n|    approx_kl                    | 3.3405795e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.72          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.211         |\n|    n_updates                    | 118           |\n|    policy_gradient_loss         | 4.94e-05      |\n|    value_loss                   | 0.487         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 177           |\n|    water_produced               | 15.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.3          |\n| time/                           |               |\n|    fps                          | 871           |\n|    iterations                   | 61            |\n|    time_elapsed                 | 280           |\n|    total_timesteps              | 244000        |\n| train/                          |               |\n|    approx_kl                    | 1.6553016e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.75         |\n|    explained_variance           | 0.00712       |\n|    learning_rate                | 0.0003        |\n|    loss                         | 8.46          |\n|    n_updates                    | 120           |\n|    policy_gradient_loss         | 2.28e-05      |\n|    value_loss                   | 22.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 68            |\n|    water_produced               | 1.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.4          |\n| time/                           |               |\n|    fps                          | 870           |\n|    iterations                   | 62            |\n|    time_elapsed                 | 284           |\n|    total_timesteps              | 248000        |\n| train/                          |               |\n|    approx_kl                    | 1.1464402e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.361         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.467         |\n|    n_updates                    | 122           |\n|    policy_gradient_loss         | -5.63e-05     |\n|    value_loss                   | 1.36          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 58            |\n|    water_produced               | 7             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 6.77         |\n| time/                           |              |\n|    fps                          | 871          |\n|    iterations                   | 63           |\n|    time_elapsed                 | 289          |\n|    total_timesteps              | 252000       |\n| train/                          |              |\n|    approx_kl                    | 7.817574e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.75        |\n|    explained_variance           | 0.12         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 3.75         |\n|    n_updates                    | 124          |\n|    policy_gradient_loss         | 3.66e-05     |\n|    value_loss                   | 7.74         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 48           |\n|    water_produced               | 6            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 9.99          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 64            |\n|    time_elapsed                 | 293           |\n|    total_timesteps              | 256000        |\n| train/                          |               |\n|    approx_kl                    | 0.00025784745 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.0496        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.8           |\n|    n_updates                    | 126           |\n|    policy_gradient_loss         | 1.64e-05      |\n|    value_loss                   | 7.35          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 96            |\n|    water_produced               | 15.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 9.29          |\n| time/                           |               |\n|    fps                          | 873           |\n|    iterations                   | 65            |\n|    time_elapsed                 | 297           |\n|    total_timesteps              | 260000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026554256 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.0365        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.8          |\n|    n_updates                    | 128           |\n|    policy_gradient_loss         | -0.000323     |\n|    value_loss                   | 29.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 77            |\n|    water_produced               | 13.3          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 9.34         |\n| time/                           |              |\n|    fps                          | 874          |\n|    iterations                   | 66           |\n|    time_elapsed                 | 301          |\n|    total_timesteps              | 264000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001891165 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.0612       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.8         |\n|    n_updates                    | 130          |\n|    policy_gradient_loss         | -0.000197    |\n|    value_loss                   | 22.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 16           |\n|    water_produced               | 2.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 9.57         |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 67           |\n|    time_elapsed                 | 306          |\n|    total_timesteps              | 268000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006065965 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.0635       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.844        |\n|    n_updates                    | 132          |\n|    policy_gradient_loss         | -0.00108     |\n|    value_loss                   | 2.49         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 75           |\n|    water_produced               | 8            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.1          |\n| time/                           |               |\n|    fps                          | 875           |\n|    iterations                   | 68            |\n|    time_elapsed                 | 310           |\n|    total_timesteps              | 272000        |\n| train/                          |               |\n|    approx_kl                    | 0.00037927998 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.0888        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.53          |\n|    n_updates                    | 134           |\n|    policy_gradient_loss         | -0.00023      |\n|    value_loss                   | 11.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 69            |\n|    water_produced               | 13.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.1          |\n| time/                           |               |\n|    fps                          | 875           |\n|    iterations                   | 69            |\n|    time_elapsed                 | 315           |\n|    total_timesteps              | 276000        |\n| train/                          |               |\n|    approx_kl                    | 0.00044146608 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.76         |\n|    explained_variance           | 0.0432        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.2           |\n|    n_updates                    | 136           |\n|    policy_gradient_loss         | -0.000636     |\n|    value_loss                   | 23.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 160           |\n|    ice_dug                      | 76            |\n|    water_produced               | 10.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 9.51          |\n| time/                           |               |\n|    fps                          | 875           |\n|    iterations                   | 70            |\n|    time_elapsed                 | 319           |\n|    total_timesteps              | 280000        |\n| train/                          |               |\n|    approx_kl                    | 0.00086314604 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.0969        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 5.88          |\n|    n_updates                    | 138           |\n|    policy_gradient_loss         | -0.00128      |\n|    value_loss                   | 15.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 151           |\n|    action_queue_updates_total   | 160           |\n|    ice_dug                      | 69            |\n|    water_produced               | 10.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.6         |\n| time/                           |              |\n|    fps                          | 876          |\n|    iterations                   | 71           |\n|    time_elapsed                 | 323          |\n|    total_timesteps              | 284000       |\n| train/                          |              |\n|    approx_kl                    | 0.0010566121 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.73        |\n|    explained_variance           | 0.0822       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 10.9         |\n|    n_updates                    | 140          |\n|    policy_gradient_loss         | -0.000962    |\n|    value_loss                   | 19.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 154          |\n|    water_produced               | 21.2         |\n--------------------------------------------------\nEval num_timesteps=288000, episode_reward=0.36 +/- 0.72\nEpisode length: 301.00 +/- 0.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 301          |\n|    mean_reward                  | 0.36         |\n| time/                           |              |\n|    total_timesteps              | 288000       |\n| train/                          |              |\n|    approx_kl                    | 0.0009082459 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.69        |\n|    explained_variance           | 0.0683       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21.2         |\n|    n_updates                    | 142          |\n|    policy_gradient_loss         | 0.000231     |\n|    value_loss                   | 42           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 48           |\n|    water_produced               | 4            |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 12.7     |\n| time/              |          |\n|    fps             | 871      |\n|    iterations      | 72       |\n|    time_elapsed    | 330      |\n|    total_timesteps | 288000   |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.3          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 73            |\n|    time_elapsed                 | 334           |\n|    total_timesteps              | 292000        |\n| train/                          |               |\n|    approx_kl                    | 0.00011291199 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.149         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.1           |\n|    n_updates                    | 144           |\n|    policy_gradient_loss         | -9.41e-06     |\n|    value_loss                   | 7.26          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 118           |\n|    water_produced               | 15.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.2          |\n| time/                           |               |\n|    fps                          | 873           |\n|    iterations                   | 74            |\n|    time_elapsed                 | 338           |\n|    total_timesteps              | 296000        |\n| train/                          |               |\n|    approx_kl                    | 1.2346834e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.67         |\n|    explained_variance           | 0.138         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.3          |\n|    n_updates                    | 146           |\n|    policy_gradient_loss         | 5.54e-05      |\n|    value_loss                   | 23.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 75            |\n|    water_produced               | 10            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.9          |\n| time/                           |               |\n|    fps                          | 874           |\n|    iterations                   | 75            |\n|    time_elapsed                 | 343           |\n|    total_timesteps              | 300000        |\n| train/                          |               |\n|    approx_kl                    | 6.1173334e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.127         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.3          |\n|    n_updates                    | 148           |\n|    policy_gradient_loss         | -0.000117     |\n|    value_loss                   | 18.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 68            |\n|    water_produced               | 4             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.81          |\n| time/                           |               |\n|    fps                          | 874           |\n|    iterations                   | 76            |\n|    time_elapsed                 | 347           |\n|    total_timesteps              | 304000        |\n| train/                          |               |\n|    approx_kl                    | 0.00012447716 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.67         |\n|    explained_variance           | 0.194         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 4.16          |\n|    n_updates                    | 150           |\n|    policy_gradient_loss         | 1.04e-05      |\n|    value_loss                   | 6.78          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 48            |\n|    water_produced               | 6.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 9.66         |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 77           |\n|    time_elapsed                 | 351          |\n|    total_timesteps              | 308000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003696676 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.72        |\n|    explained_variance           | 0.197        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 7.28         |\n|    n_updates                    | 152          |\n|    policy_gradient_loss         | -0.000495    |\n|    value_loss                   | 11.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 159          |\n|    ice_dug                      | 96           |\n|    water_produced               | 7.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 10.2         |\n| time/                           |              |\n|    fps                          | 876          |\n|    iterations                   | 78           |\n|    time_elapsed                 | 356          |\n|    total_timesteps              | 312000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006211189 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.158        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 7.65         |\n|    n_updates                    | 154          |\n|    policy_gradient_loss         | 2.98e-05     |\n|    value_loss                   | 12.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 114          |\n|    water_produced               | 18.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.2          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 79            |\n|    time_elapsed                 | 360           |\n|    total_timesteps              | 316000        |\n| train/                          |               |\n|    approx_kl                    | 0.00042644804 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.0973        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 18.7          |\n|    n_updates                    | 156           |\n|    policy_gradient_loss         | -0.000432     |\n|    value_loss                   | 31.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 165           |\n|    ice_dug                      | 148           |\n|    water_produced               | 24.5          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 16.9        |\n| time/                           |             |\n|    fps                          | 877         |\n|    iterations                   | 80          |\n|    time_elapsed                 | 364         |\n|    total_timesteps              | 320000      |\n| train/                          |             |\n|    approx_kl                    | 0.000703286 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.75       |\n|    explained_variance           | 0.0567      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 18.5        |\n|    n_updates                    | 158         |\n|    policy_gradient_loss         | 0.000147    |\n|    value_loss                   | 50.4        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 148         |\n|    action_queue_updates_total   | 167         |\n|    ice_dug                      | 224         |\n|    water_produced               | 20.5        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.7         |\n| time/                           |              |\n|    fps                          | 878          |\n|    iterations                   | 81           |\n|    time_elapsed                 | 368          |\n|    total_timesteps              | 324000       |\n| train/                          |              |\n|    approx_kl                    | 0.0004339232 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.74        |\n|    explained_variance           | 0.159        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.8         |\n|    n_updates                    | 160          |\n|    policy_gradient_loss         | 4.66e-05     |\n|    value_loss                   | 28.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 153          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 137          |\n|    water_produced               | 20.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23.4          |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 82            |\n|    time_elapsed                 | 373           |\n|    total_timesteps              | 328000        |\n| train/                          |               |\n|    approx_kl                    | 0.00022797105 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.0665        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 19.8          |\n|    n_updates                    | 162           |\n|    policy_gradient_loss         | 8.45e-05      |\n|    value_loss                   | 40.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 163           |\n|    ice_dug                      | 181           |\n|    water_produced               | 25.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.2          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 83            |\n|    time_elapsed                 | 378           |\n|    total_timesteps              | 332000        |\n| train/                          |               |\n|    approx_kl                    | 0.00024342074 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.0773        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.9          |\n|    n_updates                    | 164           |\n|    policy_gradient_loss         | 0.000389      |\n|    value_loss                   | 53.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 166           |\n|    ice_dug                      | 44            |\n|    water_produced               | 8.25          |\n---------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 16.9           |\n| time/                           |                |\n|    fps                          | 878            |\n|    iterations                   | 84             |\n|    time_elapsed                 | 382            |\n|    total_timesteps              | 336000         |\n| train/                          |                |\n|    approx_kl                    | 0.000119701355 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.72          |\n|    explained_variance           | 0.187          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 4.48           |\n|    n_updates                    | 166            |\n|    policy_gradient_loss         | -9.64e-05      |\n|    value_loss                   | 14.4           |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 151            |\n|    action_queue_updates_total   | 167            |\n|    ice_dug                      | 76             |\n|    water_produced               | 3.75           |\n----------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.1          |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 85            |\n|    time_elapsed                 | 387           |\n|    total_timesteps              | 340000        |\n| train/                          |               |\n|    approx_kl                    | 0.00040250184 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.74         |\n|    explained_variance           | 0.377         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.91          |\n|    n_updates                    | 168           |\n|    policy_gradient_loss         | -0.000171     |\n|    value_loss                   | 3.99          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 148           |\n|    action_queue_updates_total   | 163           |\n|    ice_dug                      | 143           |\n|    water_produced               | 17.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.8          |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 86            |\n|    time_elapsed                 | 391           |\n|    total_timesteps              | 344000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015786414 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.73         |\n|    explained_variance           | 0.157         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.6          |\n|    n_updates                    | 170           |\n|    policy_gradient_loss         | -1.67e-05     |\n|    value_loss                   | 30.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 167           |\n|    ice_dug                      | 69            |\n|    water_produced               | 9.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.1          |\n| time/                           |               |\n|    fps                          | 879           |\n|    iterations                   | 87            |\n|    time_elapsed                 | 395           |\n|    total_timesteps              | 348000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017424766 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.75         |\n|    explained_variance           | 0.159         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.59          |\n|    n_updates                    | 172           |\n|    policy_gradient_loss         | -0.000115     |\n|    value_loss                   | 20.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 165           |\n|    ice_dug                      | 117           |\n|    water_produced               | 12.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 12.4         |\n| time/                           |              |\n|    fps                          | 879          |\n|    iterations                   | 88           |\n|    time_elapsed                 | 400          |\n|    total_timesteps              | 352000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005022868 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.75        |\n|    explained_variance           | 0.233        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.07         |\n|    n_updates                    | 174          |\n|    policy_gradient_loss         | -0.000248    |\n|    value_loss                   | 15.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 114          |\n|    water_produced               | 13.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 12.3          |\n| time/                           |               |\n|    fps                          | 880           |\n|    iterations                   | 89            |\n|    time_elapsed                 | 404           |\n|    total_timesteps              | 356000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014429663 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.313         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.87          |\n|    n_updates                    | 176           |\n|    policy_gradient_loss         | -1.52e-06     |\n|    value_loss                   | 15.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 148           |\n|    action_queue_updates_total   | 163           |\n|    ice_dug                      | 21            |\n|    water_produced               | 4             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 12.3          |\n| time/                           |               |\n|    fps                          | 880           |\n|    iterations                   | 90            |\n|    time_elapsed                 | 408           |\n|    total_timesteps              | 360000        |\n| train/                          |               |\n|    approx_kl                    | 0.00085658545 |\n|    clip_fraction                | 0.000125      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.76         |\n|    explained_variance           | 0.47          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.69          |\n|    n_updates                    | 178           |\n|    policy_gradient_loss         | -0.000119     |\n|    value_loss                   | 5.06          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 159           |\n|    ice_dug                      | 127           |\n|    water_produced               | 17.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.8          |\n| time/                           |               |\n|    fps                          | 880           |\n|    iterations                   | 91            |\n|    time_elapsed                 | 413           |\n|    total_timesteps              | 364000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020486079 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.227         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.9          |\n|    n_updates                    | 180           |\n|    policy_gradient_loss         | 0.000172      |\n|    value_loss                   | 32.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 72            |\n|    water_produced               | 1.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.7          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 92            |\n|    time_elapsed                 | 417           |\n|    total_timesteps              | 368000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015487775 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.734         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.16          |\n|    n_updates                    | 182           |\n|    policy_gradient_loss         | -9.3e-06      |\n|    value_loss                   | 2.33          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 154           |\n|    action_queue_updates_total   | 166           |\n|    ice_dug                      | 113           |\n|    water_produced               | 12.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 10.6         |\n| time/                           |              |\n|    fps                          | 882          |\n|    iterations                   | 93           |\n|    time_elapsed                 | 421          |\n|    total_timesteps              | 372000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016817044 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.266        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.1          |\n|    n_updates                    | 184          |\n|    policy_gradient_loss         | -0.0011      |\n|    value_loss                   | 17           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 122          |\n|    water_produced               | 13           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 11.7         |\n| time/                           |              |\n|    fps                          | 882          |\n|    iterations                   | 94           |\n|    time_elapsed                 | 425          |\n|    total_timesteps              | 376000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006763857 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.15         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 17.4         |\n|    n_updates                    | 186          |\n|    policy_gradient_loss         | -7.96e-05    |\n|    value_loss                   | 22.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 79           |\n|    water_produced               | 9            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11            |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 95            |\n|    time_elapsed                 | 430           |\n|    total_timesteps              | 380000        |\n| train/                          |               |\n|    approx_kl                    | 0.00032742237 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.224         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.9          |\n|    n_updates                    | 188           |\n|    policy_gradient_loss         | -2.02e-05     |\n|    value_loss                   | 20.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 79            |\n|    water_produced               | 14.2          |\n---------------------------------------------------\nEval num_timesteps=384000, episode_reward=36.56 +/- 73.02\nEpisode length: 336.00 +/- 70.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 336           |\n|    mean_reward                  | 36.6          |\n| time/                           |               |\n|    total_timesteps              | 384000        |\n| train/                          |               |\n|    approx_kl                    | 0.00034279362 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.75         |\n|    explained_variance           | 0.197         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.8          |\n|    n_updates                    | 190           |\n|    policy_gradient_loss         | -0.000371     |\n|    value_loss                   | 32.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 153           |\n|    water_produced               | 27.8          |\n---------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 16.3     |\n| time/              |          |\n|    fps             | 877      |\n|    iterations      | 96       |\n|    time_elapsed    | 437      |\n|    total_timesteps | 384000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.9         |\n| time/                           |              |\n|    fps                          | 877          |\n|    iterations                   | 97           |\n|    time_elapsed                 | 442          |\n|    total_timesteps              | 388000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005015042 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.144        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 30.4         |\n|    n_updates                    | 192          |\n|    policy_gradient_loss         | -0.000358    |\n|    value_loss                   | 57.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 144          |\n|    ice_dug                      | 84           |\n|    water_produced               | 15.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.7          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 98            |\n|    time_elapsed                 | 446           |\n|    total_timesteps              | 392000        |\n| train/                          |               |\n|    approx_kl                    | 0.00044463776 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.57         |\n|    explained_variance           | 0.182         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.2          |\n|    n_updates                    | 194           |\n|    policy_gradient_loss         | 1.12e-05      |\n|    value_loss                   | 28            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 141           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 78            |\n|    water_produced               | 12.3          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.4          |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 99            |\n|    time_elapsed                 | 450           |\n|    total_timesteps              | 396000        |\n| train/                          |               |\n|    approx_kl                    | 0.00016963489 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.151         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.3          |\n|    n_updates                    | 196           |\n|    policy_gradient_loss         | 6.89e-06      |\n|    value_loss                   | 28.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 106           |\n|    water_produced               | 12.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.8         |\n| time/                           |              |\n|    fps                          | 878          |\n|    iterations                   | 100          |\n|    time_elapsed                 | 455          |\n|    total_timesteps              | 400000       |\n| train/                          |              |\n|    approx_kl                    | 0.0009872374 |\n|    clip_fraction                | 0.00188      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.69        |\n|    explained_variance           | 0.244        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8            |\n|    n_updates                    | 198          |\n|    policy_gradient_loss         | -0.00119     |\n|    value_loss                   | 17.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 95           |\n|    water_produced               | 15.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.6         |\n| time/                           |              |\n|    fps                          | 878          |\n|    iterations                   | 101          |\n|    time_elapsed                 | 459          |\n|    total_timesteps              | 404000       |\n| train/                          |              |\n|    approx_kl                    | 0.0030415982 |\n|    clip_fraction                | 0.00388      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.65        |\n|    explained_variance           | 0.167        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15.4         |\n|    n_updates                    | 200          |\n|    policy_gradient_loss         | -0.0019      |\n|    value_loss                   | 28.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 135          |\n|    ice_dug                      | 38           |\n|    water_produced               | 8            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14.5         |\n| time/                           |              |\n|    fps                          | 879          |\n|    iterations                   | 102          |\n|    time_elapsed                 | 464          |\n|    total_timesteps              | 408000       |\n| train/                          |              |\n|    approx_kl                    | 0.0013733355 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.53        |\n|    explained_variance           | 0.136        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.2         |\n|    n_updates                    | 202          |\n|    policy_gradient_loss         | 0.000127     |\n|    value_loss                   | 28.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 119          |\n|    action_queue_updates_total   | 132          |\n|    ice_dug                      | 148          |\n|    water_produced               | 19.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 12.6          |\n| time/                           |               |\n|    fps                          | 879           |\n|    iterations                   | 103           |\n|    time_elapsed                 | 468           |\n|    total_timesteps              | 412000        |\n| train/                          |               |\n|    approx_kl                    | 0.00025131094 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.56         |\n|    explained_variance           | 0.153         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 23.7          |\n|    n_updates                    | 204           |\n|    policy_gradient_loss         | -0.00014      |\n|    value_loss                   | 42.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 47            |\n|    water_produced               | 3             |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 12.7        |\n| time/                           |             |\n|    fps                          | 878         |\n|    iterations                   | 104         |\n|    time_elapsed                 | 473         |\n|    total_timesteps              | 416000      |\n| train/                          |             |\n|    approx_kl                    | 3.66032e-05 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.6        |\n|    explained_variance           | 0.49        |\n|    learning_rate                | 0.0003      |\n|    loss                         | 2.54        |\n|    n_updates                    | 206         |\n|    policy_gradient_loss         | 2.05e-05    |\n|    value_loss                   | 4.84        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 120         |\n|    action_queue_updates_total   | 131         |\n|    ice_dug                      | 84          |\n|    water_produced               | 13.2        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.7         |\n| time/                           |              |\n|    fps                          | 879          |\n|    iterations                   | 105          |\n|    time_elapsed                 | 477          |\n|    total_timesteps              | 420000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001189569 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.101        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.7         |\n|    n_updates                    | 208          |\n|    policy_gradient_loss         | -0.000202    |\n|    value_loss                   | 34.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 135          |\n|    ice_dug                      | 276          |\n|    water_produced               | 44           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.6          |\n| time/                           |               |\n|    fps                          | 879           |\n|    iterations                   | 106           |\n|    time_elapsed                 | 481           |\n|    total_timesteps              | 424000        |\n| train/                          |               |\n|    approx_kl                    | 0.00049734936 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.5          |\n|    explained_variance           | 0.0782        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 57.7          |\n|    n_updates                    | 210           |\n|    policy_gradient_loss         | -0.000153     |\n|    value_loss                   | 114           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 118           |\n|    action_queue_updates_total   | 132           |\n|    ice_dug                      | 145           |\n|    water_produced               | 21.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.6         |\n| time/                           |              |\n|    fps                          | 880          |\n|    iterations                   | 107          |\n|    time_elapsed                 | 486          |\n|    total_timesteps              | 428000       |\n| train/                          |              |\n|    approx_kl                    | 0.0004019789 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.51        |\n|    explained_variance           | 0.0964       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 36.6         |\n|    n_updates                    | 212          |\n|    policy_gradient_loss         | -0.000363    |\n|    value_loss                   | 66           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 133          |\n|    ice_dug                      | 78           |\n|    water_produced               | 9.75         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.4          |\n| time/                           |               |\n|    fps                          | 880           |\n|    iterations                   | 108           |\n|    time_elapsed                 | 490           |\n|    total_timesteps              | 432000        |\n| train/                          |               |\n|    approx_kl                    | 3.4632547e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.44         |\n|    explained_variance           | 0.12          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.6           |\n|    n_updates                    | 214           |\n|    policy_gradient_loss         | 0.000113      |\n|    value_loss                   | 19.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 117           |\n|    action_queue_updates_total   | 127           |\n|    ice_dug                      | 179           |\n|    water_produced               | 35.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.5          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 109           |\n|    time_elapsed                 | 494           |\n|    total_timesteps              | 436000        |\n| train/                          |               |\n|    approx_kl                    | 4.0246592e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.51         |\n|    explained_variance           | 0.127         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.6          |\n|    n_updates                    | 216           |\n|    policy_gradient_loss         | -0.000128     |\n|    value_loss                   | 84.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 116           |\n|    action_queue_updates_total   | 121           |\n|    ice_dug                      | 97            |\n|    water_produced               | 13.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.7          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 110           |\n|    time_elapsed                 | 498           |\n|    total_timesteps              | 440000        |\n| train/                          |               |\n|    approx_kl                    | 0.00016541663 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.43         |\n|    explained_variance           | 0.0892        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.6          |\n|    n_updates                    | 218           |\n|    policy_gradient_loss         | -0.000184     |\n|    value_loss                   | 25.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 136           |\n|    ice_dug                      | 116           |\n|    water_produced               | 16.7          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.2          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 111           |\n|    time_elapsed                 | 503           |\n|    total_timesteps              | 444000        |\n| train/                          |               |\n|    approx_kl                    | 0.00048110232 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.47         |\n|    explained_variance           | 0.119         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 16.8          |\n|    n_updates                    | 220           |\n|    policy_gradient_loss         | 7.64e-05      |\n|    value_loss                   | 32.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 124           |\n|    action_queue_updates_total   | 135           |\n|    ice_dug                      | 61            |\n|    water_produced               | 9.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.1          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 112           |\n|    time_elapsed                 | 507           |\n|    total_timesteps              | 448000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021812467 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.53         |\n|    explained_variance           | 0.129         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.3           |\n|    n_updates                    | 222           |\n|    policy_gradient_loss         | -0.00015      |\n|    value_loss                   | 20.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 136           |\n|    ice_dug                      | 26            |\n|    water_produced               | 4.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.4          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 113           |\n|    time_elapsed                 | 512           |\n|    total_timesteps              | 452000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019556319 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.49         |\n|    explained_variance           | 0.199         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 5.6           |\n|    n_updates                    | 224           |\n|    policy_gradient_loss         | -8.39e-05     |\n|    value_loss                   | 10.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 120           |\n|    action_queue_updates_total   | 132           |\n|    ice_dug                      | 107           |\n|    water_produced               | 18            |\n---------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 13.9           |\n| time/                           |                |\n|    fps                          | 882            |\n|    iterations                   | 114            |\n|    time_elapsed                 | 516            |\n|    total_timesteps              | 456000         |\n| train/                          |                |\n|    approx_kl                    | 0.000107978274 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.49          |\n|    explained_variance           | 0.158          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 18             |\n|    n_updates                    | 226            |\n|    policy_gradient_loss         | -5.5e-05       |\n|    value_loss                   | 40.1           |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 121            |\n|    action_queue_updates_total   | 127            |\n|    ice_dug                      | 89             |\n|    water_produced               | 16.5           |\n----------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.8          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 115           |\n|    time_elapsed                 | 521           |\n|    total_timesteps              | 460000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019832849 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.44         |\n|    explained_variance           | 0.0649        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 18            |\n|    n_updates                    | 228           |\n|    policy_gradient_loss         | -0.000104     |\n|    value_loss                   | 43            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 120           |\n|    action_queue_updates_total   | 133           |\n|    ice_dug                      | 107           |\n|    water_produced               | 16.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.8         |\n| time/                           |              |\n|    fps                          | 883          |\n|    iterations                   | 116          |\n|    time_elapsed                 | 525          |\n|    total_timesteps              | 464000       |\n| train/                          |              |\n|    approx_kl                    | 7.485457e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.46        |\n|    explained_variance           | 0.131        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 19.3         |\n|    n_updates                    | 230          |\n|    policy_gradient_loss         | -6.18e-05    |\n|    value_loss                   | 32.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 242          |\n|    water_produced               | 48           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23            |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 117           |\n|    time_elapsed                 | 529           |\n|    total_timesteps              | 468000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026079142 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.47         |\n|    explained_variance           | 0.0796        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 52.5          |\n|    n_updates                    | 232           |\n|    policy_gradient_loss         | -0.000543     |\n|    value_loss                   | 120           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 53            |\n|    water_produced               | 10            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 24.8         |\n| time/                           |              |\n|    fps                          | 882          |\n|    iterations                   | 118          |\n|    time_elapsed                 | 534          |\n|    total_timesteps              | 472000       |\n| train/                          |              |\n|    approx_kl                    | 2.621132e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.46        |\n|    explained_variance           | 0.116        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.3         |\n|    n_updates                    | 234          |\n|    policy_gradient_loss         | 2.88e-05     |\n|    value_loss                   | 22           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 165          |\n|    water_produced               | 26.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 24.6          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 119           |\n|    time_elapsed                 | 539           |\n|    total_timesteps              | 476000        |\n| train/                          |               |\n|    approx_kl                    | 4.6345533e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.48         |\n|    explained_variance           | 0.0935        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 25.1          |\n|    n_updates                    | 236           |\n|    policy_gradient_loss         | -0.000165     |\n|    value_loss                   | 53.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 119           |\n|    water_produced               | 15.2          |\n---------------------------------------------------\nEval num_timesteps=480000, episode_reward=13.28 +/- 19.98\nEpisode length: 312.00 +/- 19.60\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 312          |\n|    mean_reward                  | 13.3         |\n| time/                           |              |\n|    total_timesteps              | 480000       |\n| train/                          |              |\n|    approx_kl                    | 0.0002150936 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.56        |\n|    explained_variance           | 0.152        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.7         |\n|    n_updates                    | 238          |\n|    policy_gradient_loss         | -0.000182    |\n|    value_loss                   | 26.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 147          |\n|    ice_dug                      | 36           |\n|    water_produced               | 5.25         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 22.2     |\n| time/              |          |\n|    fps             | 879      |\n|    iterations      | 120      |\n|    time_elapsed    | 546      |\n|    total_timesteps | 480000   |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.8          |\n| time/                           |               |\n|    fps                          | 879           |\n|    iterations                   | 121           |\n|    time_elapsed                 | 550           |\n|    total_timesteps              | 484000        |\n| train/                          |               |\n|    approx_kl                    | 0.00047319802 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.53         |\n|    explained_variance           | 0.14          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.24          |\n|    n_updates                    | 240           |\n|    policy_gradient_loss         | 0.000175      |\n|    value_loss                   | 9.69          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 137           |\n|    ice_dug                      | 43            |\n|    water_produced               | 7.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16            |\n| time/                           |               |\n|    fps                          | 880           |\n|    iterations                   | 122           |\n|    time_elapsed                 | 554           |\n|    total_timesteps              | 488000        |\n| train/                          |               |\n|    approx_kl                    | 0.00042868807 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.55         |\n|    explained_variance           | 0.233         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.49          |\n|    n_updates                    | 242           |\n|    policy_gradient_loss         | -0.000221     |\n|    value_loss                   | 12.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 129           |\n|    action_queue_updates_total   | 139           |\n|    ice_dug                      | 151           |\n|    water_produced               | 20            |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 14.5        |\n| time/                           |             |\n|    fps                          | 880         |\n|    iterations                   | 123         |\n|    time_elapsed                 | 558         |\n|    total_timesteps              | 492000      |\n| train/                          |             |\n|    approx_kl                    | 3.25426e-05 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.52       |\n|    explained_variance           | 0.155       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 25.2        |\n|    n_updates                    | 244         |\n|    policy_gradient_loss         | -3.36e-05   |\n|    value_loss                   | 47.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 127         |\n|    action_queue_updates_total   | 138         |\n|    ice_dug                      | 92          |\n|    water_produced               | 19.8        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.8          |\n| time/                           |               |\n|    fps                          | 880           |\n|    iterations                   | 124           |\n|    time_elapsed                 | 563           |\n|    total_timesteps              | 496000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014352065 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.53         |\n|    explained_variance           | 0.0731        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 19.1          |\n|    n_updates                    | 246           |\n|    policy_gradient_loss         | -0.000165     |\n|    value_loss                   | 46.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 212           |\n|    water_produced               | 46            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.4         |\n| time/                           |              |\n|    fps                          | 880          |\n|    iterations                   | 125          |\n|    time_elapsed                 | 567          |\n|    total_timesteps              | 500000       |\n| train/                          |              |\n|    approx_kl                    | 0.0002800067 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.54        |\n|    explained_variance           | 0.0908       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 61.6         |\n|    n_updates                    | 248          |\n|    policy_gradient_loss         | -0.000172    |\n|    value_loss                   | 114          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 62           |\n|    water_produced               | 8            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.6         |\n| time/                           |              |\n|    fps                          | 881          |\n|    iterations                   | 126          |\n|    time_elapsed                 | 572          |\n|    total_timesteps              | 504000       |\n| train/                          |              |\n|    approx_kl                    | 7.029156e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.223        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.77         |\n|    n_updates                    | 250          |\n|    policy_gradient_loss         | -0.000259    |\n|    value_loss                   | 15.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 93           |\n|    water_produced               | 13           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.9          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 127           |\n|    time_elapsed                 | 576           |\n|    total_timesteps              | 508000        |\n| train/                          |               |\n|    approx_kl                    | 0.00052950566 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.168         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 18.8          |\n|    n_updates                    | 252           |\n|    policy_gradient_loss         | 4.88e-05      |\n|    value_loss                   | 24.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 40            |\n|    water_produced               | 2.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.1         |\n| time/                           |              |\n|    fps                          | 881          |\n|    iterations                   | 128          |\n|    time_elapsed                 | 580          |\n|    total_timesteps              | 512000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003762257 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.57        |\n|    explained_variance           | 0.44         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.53         |\n|    n_updates                    | 254          |\n|    policy_gradient_loss         | 0.000105     |\n|    value_loss                   | 3.49         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 141          |\n|    ice_dug                      | 127          |\n|    water_produced               | 25.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 14            |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 129           |\n|    time_elapsed                 | 584           |\n|    total_timesteps              | 516000        |\n| train/                          |               |\n|    approx_kl                    | 5.9887097e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.102         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 23.8          |\n|    n_updates                    | 256           |\n|    policy_gradient_loss         | 0.000126      |\n|    value_loss                   | 57.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 140           |\n|    water_produced               | 16.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.2          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 130           |\n|    time_elapsed                 | 589           |\n|    total_timesteps              | 520000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017327846 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.131         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.6          |\n|    n_updates                    | 258           |\n|    policy_gradient_loss         | -0.000129     |\n|    value_loss                   | 34.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 219           |\n|    water_produced               | 37.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.2          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 131           |\n|    time_elapsed                 | 593           |\n|    total_timesteps              | 524000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021337035 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.62         |\n|    explained_variance           | 0.118         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 35.5          |\n|    n_updates                    | 260           |\n|    policy_gradient_loss         | -7.44e-05     |\n|    value_loss                   | 66.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 85            |\n|    water_produced               | 13            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 26.6         |\n| time/                           |              |\n|    fps                          | 882          |\n|    iterations                   | 132          |\n|    time_elapsed                 | 598          |\n|    total_timesteps              | 528000       |\n| train/                          |              |\n|    approx_kl                    | 2.789703e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.151        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.3         |\n|    n_updates                    | 262          |\n|    policy_gradient_loss         | -1.9e-05     |\n|    value_loss                   | 30.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 154          |\n|    water_produced               | 33.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 22.8          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 133           |\n|    time_elapsed                 | 602           |\n|    total_timesteps              | 532000        |\n| train/                          |               |\n|    approx_kl                    | 4.4058357e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.57         |\n|    explained_variance           | 0.109         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 38.5          |\n|    n_updates                    | 264           |\n|    policy_gradient_loss         | -4e-05        |\n|    value_loss                   | 76            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 71            |\n|    water_produced               | 6.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 28.2          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 134           |\n|    time_elapsed                 | 606           |\n|    total_timesteps              | 536000        |\n| train/                          |               |\n|    approx_kl                    | 0.00013034629 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.68         |\n|    explained_variance           | 0.242         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.84          |\n|    n_updates                    | 266           |\n|    policy_gradient_loss         | -2.43e-05     |\n|    value_loss                   | 12.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 223           |\n|    water_produced               | 42.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.5          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 135           |\n|    time_elapsed                 | 610           |\n|    total_timesteps              | 540000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014052202 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.17          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 49.4          |\n|    n_updates                    | 268           |\n|    policy_gradient_loss         | 0.000323      |\n|    value_loss                   | 97.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 27            |\n|    water_produced               | 6.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.5         |\n| time/                           |              |\n|    fps                          | 884          |\n|    iterations                   | 136          |\n|    time_elapsed                 | 615          |\n|    total_timesteps              | 544000       |\n| train/                          |              |\n|    approx_kl                    | 9.267153e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.224        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.89         |\n|    n_updates                    | 270          |\n|    policy_gradient_loss         | 6.66e-05     |\n|    value_loss                   | 17.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 98           |\n|    water_produced               | 13           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.8          |\n| time/                           |               |\n|    fps                          | 884           |\n|    iterations                   | 137           |\n|    time_elapsed                 | 619           |\n|    total_timesteps              | 548000        |\n| train/                          |               |\n|    approx_kl                    | 0.00077834737 |\n|    clip_fraction                | 0.000125      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.66         |\n|    explained_variance           | 0.203         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.7          |\n|    n_updates                    | 272           |\n|    policy_gradient_loss         | -0.000113     |\n|    value_loss                   | 30.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 31            |\n|    water_produced               | 5.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.7         |\n| time/                           |              |\n|    fps                          | 884          |\n|    iterations                   | 138          |\n|    time_elapsed                 | 624          |\n|    total_timesteps              | 552000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006762225 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.68        |\n|    explained_variance           | 0.311        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.78         |\n|    n_updates                    | 274          |\n|    policy_gradient_loss         | -6.98e-05    |\n|    value_loss                   | 11.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 159          |\n|    ice_dug                      | 57           |\n|    water_produced               | 6.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 10.1         |\n| time/                           |              |\n|    fps                          | 884          |\n|    iterations                   | 139          |\n|    time_elapsed                 | 628          |\n|    total_timesteps              | 556000       |\n| train/                          |              |\n|    approx_kl                    | 0.0012264508 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.88        |\n|    explained_variance           | 0.308        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.9          |\n|    n_updates                    | 276          |\n|    policy_gradient_loss         | -0.000155    |\n|    value_loss                   | 13.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 91           |\n|    water_produced               | 15.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14           |\n| time/                           |              |\n|    fps                          | 884          |\n|    iterations                   | 140          |\n|    time_elapsed                 | 633          |\n|    total_timesteps              | 560000       |\n| train/                          |              |\n|    approx_kl                    | 7.645832e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.85        |\n|    explained_variance           | 0.319        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.5         |\n|    n_updates                    | 278          |\n|    policy_gradient_loss         | 4.77e-05     |\n|    value_loss                   | 28.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 143          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 151          |\n|    water_produced               | 24.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.3          |\n| time/                           |               |\n|    fps                          | 885           |\n|    iterations                   | 141           |\n|    time_elapsed                 | 637           |\n|    total_timesteps              | 564000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021975087 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.73         |\n|    explained_variance           | 0.214         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.2          |\n|    n_updates                    | 280           |\n|    policy_gradient_loss         | -0.000121     |\n|    value_loss                   | 39.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 104           |\n|    water_produced               | 19.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.6          |\n| time/                           |               |\n|    fps                          | 885           |\n|    iterations                   | 142           |\n|    time_elapsed                 | 641           |\n|    total_timesteps              | 568000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017151487 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.261         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.1          |\n|    n_updates                    | 282           |\n|    policy_gradient_loss         | -0.000131     |\n|    value_loss                   | 42.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 116           |\n|    water_produced               | 16.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.1          |\n| time/                           |               |\n|    fps                          | 885           |\n|    iterations                   | 143           |\n|    time_elapsed                 | 646           |\n|    total_timesteps              | 572000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019878987 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.23          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 19.2          |\n|    n_updates                    | 284           |\n|    policy_gradient_loss         | -0.000131     |\n|    value_loss                   | 35.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 104           |\n|    water_produced               | 14            |\n---------------------------------------------------\nEval num_timesteps=576000, episode_reward=0.00 +/- 0.00\nEpisode length: 301.00 +/- 0.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 301           |\n|    mean_reward                  | 0             |\n| time/                           |               |\n|    total_timesteps              | 576000        |\n| train/                          |               |\n|    approx_kl                    | 0.00042357476 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.296         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.4          |\n|    n_updates                    | 286           |\n|    policy_gradient_loss         | -0.000252     |\n|    value_loss                   | 31.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 94            |\n|    water_produced               | 8.5           |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 17.7     |\n| time/              |          |\n|    fps             | 882      |\n|    iterations      | 144      |\n|    time_elapsed    | 652      |\n|    total_timesteps | 576000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.6         |\n| time/                           |              |\n|    fps                          | 882          |\n|    iterations                   | 145          |\n|    time_elapsed                 | 657          |\n|    total_timesteps              | 580000       |\n| train/                          |              |\n|    approx_kl                    | 0.0014275485 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.85        |\n|    explained_variance           | 0.499        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.3          |\n|    n_updates                    | 288          |\n|    policy_gradient_loss         | -0.000848    |\n|    value_loss                   | 10.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 91           |\n|    water_produced               | 14.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16           |\n| time/                           |              |\n|    fps                          | 882          |\n|    iterations                   | 146          |\n|    time_elapsed                 | 661          |\n|    total_timesteps              | 584000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005172007 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.65        |\n|    explained_variance           | 0.301        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 18.3         |\n|    n_updates                    | 290          |\n|    policy_gradient_loss         | -0.000417    |\n|    value_loss                   | 28.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 155          |\n|    ice_dug                      | 188          |\n|    water_produced               | 21           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.6         |\n| time/                           |              |\n|    fps                          | 883          |\n|    iterations                   | 147          |\n|    time_elapsed                 | 665          |\n|    total_timesteps              | 588000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008119313 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.279        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 24.7         |\n|    n_updates                    | 292          |\n|    policy_gradient_loss         | -0.000112    |\n|    value_loss                   | 44.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 130          |\n|    water_produced               | 28.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.2         |\n| time/                           |              |\n|    fps                          | 883          |\n|    iterations                   | 148          |\n|    time_elapsed                 | 670          |\n|    total_timesteps              | 592000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005473661 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.74        |\n|    explained_variance           | 0.247        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 31.2         |\n|    n_updates                    | 294          |\n|    policy_gradient_loss         | -0.000323    |\n|    value_loss                   | 56.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 40           |\n|    water_produced               | 7.75         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.7          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 149           |\n|    time_elapsed                 | 674           |\n|    total_timesteps              | 596000        |\n| train/                          |               |\n|    approx_kl                    | 0.00036622933 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.402         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.83          |\n|    n_updates                    | 296           |\n|    policy_gradient_loss         | 0.000144      |\n|    value_loss                   | 20            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 136           |\n|    water_produced               | 20.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21           |\n| time/                           |              |\n|    fps                          | 883          |\n|    iterations                   | 150          |\n|    time_elapsed                 | 678          |\n|    total_timesteps              | 600000       |\n| train/                          |              |\n|    approx_kl                    | 0.0002547471 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.77        |\n|    explained_variance           | 0.301        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 20.6         |\n|    n_updates                    | 298          |\n|    policy_gradient_loss         | 0.000133     |\n|    value_loss                   | 50.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 116          |\n|    water_produced               | 21           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.7          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 151           |\n|    time_elapsed                 | 683           |\n|    total_timesteps              | 604000        |\n| train/                          |               |\n|    approx_kl                    | 0.00018713754 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.31          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.9          |\n|    n_updates                    | 300           |\n|    policy_gradient_loss         | -5.62e-05     |\n|    value_loss                   | 49.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 121           |\n|    water_produced               | 25            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.2          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 152           |\n|    time_elapsed                 | 688           |\n|    total_timesteps              | 608000        |\n| train/                          |               |\n|    approx_kl                    | 0.00012084496 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.288         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 44.4          |\n|    n_updates                    | 302           |\n|    policy_gradient_loss         | -9.99e-05     |\n|    value_loss                   | 68.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 129           |\n|    action_queue_updates_total   | 145           |\n|    ice_dug                      | 186           |\n|    water_produced               | 20.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.9          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 153           |\n|    time_elapsed                 | 693           |\n|    total_timesteps              | 612000        |\n| train/                          |               |\n|    approx_kl                    | 0.00012577753 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.36          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.3          |\n|    n_updates                    | 304           |\n|    policy_gradient_loss         | 1.96e-05      |\n|    value_loss                   | 39.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 146           |\n|    water_produced               | 15.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.8          |\n| time/                           |               |\n|    fps                          | 883           |\n|    iterations                   | 154           |\n|    time_elapsed                 | 697           |\n|    total_timesteps              | 616000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021348354 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.4           |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.2          |\n|    n_updates                    | 306           |\n|    policy_gradient_loss         | 0.000114      |\n|    value_loss                   | 29.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 140           |\n|    water_produced               | 19.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.4         |\n| time/                           |              |\n|    fps                          | 883          |\n|    iterations                   | 155          |\n|    time_elapsed                 | 702          |\n|    total_timesteps              | 620000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011422962 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.439        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.3         |\n|    n_updates                    | 308          |\n|    policy_gradient_loss         | -0.000333    |\n|    value_loss                   | 30.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 86           |\n|    water_produced               | 14.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.4         |\n| time/                           |              |\n|    fps                          | 882          |\n|    iterations                   | 156          |\n|    time_elapsed                 | 706          |\n|    total_timesteps              | 624000       |\n| train/                          |              |\n|    approx_kl                    | 0.0018914065 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.86        |\n|    explained_variance           | 0.511        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.6         |\n|    n_updates                    | 310          |\n|    policy_gradient_loss         | 1.37e-05     |\n|    value_loss                   | 25.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 51           |\n|    water_produced               | 10.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.9          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 157           |\n|    time_elapsed                 | 711           |\n|    total_timesteps              | 628000        |\n| train/                          |               |\n|    approx_kl                    | 0.00040925355 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.92         |\n|    explained_variance           | 0.603         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.77          |\n|    n_updates                    | 312           |\n|    policy_gradient_loss         | -0.000366     |\n|    value_loss                   | 15.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 132           |\n|    water_produced               | 23.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.6         |\n| time/                           |              |\n|    fps                          | 882          |\n|    iterations                   | 158          |\n|    time_elapsed                 | 716          |\n|    total_timesteps              | 632000       |\n| train/                          |              |\n|    approx_kl                    | 0.0014935824 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.73        |\n|    explained_variance           | 0.427        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21.4         |\n|    n_updates                    | 314          |\n|    policy_gradient_loss         | -0.000123    |\n|    value_loss                   | 50           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 114          |\n|    water_produced               | 19           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.2          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 159           |\n|    time_elapsed                 | 721           |\n|    total_timesteps              | 636000        |\n| train/                          |               |\n|    approx_kl                    | 5.0288927e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.64         |\n|    explained_variance           | 0.321         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.3          |\n|    n_updates                    | 316           |\n|    policy_gradient_loss         | 1.61e-05      |\n|    value_loss                   | 37.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 23            |\n|    water_produced               | 4             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.5          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 160           |\n|    time_elapsed                 | 725           |\n|    total_timesteps              | 640000        |\n| train/                          |               |\n|    approx_kl                    | 0.00052962865 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.564         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.88          |\n|    n_updates                    | 318           |\n|    policy_gradient_loss         | -0.00019      |\n|    value_loss                   | 10.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 147           |\n|    ice_dug                      | 88            |\n|    water_produced               | 15.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.9          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 161           |\n|    time_elapsed                 | 730           |\n|    total_timesteps              | 644000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015061807 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.69         |\n|    explained_variance           | 0.367         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.5          |\n|    n_updates                    | 320           |\n|    policy_gradient_loss         | -0.000371     |\n|    value_loss                   | 36.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 143           |\n|    ice_dug                      | 76            |\n|    water_produced               | 12.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 14.2          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 162           |\n|    time_elapsed                 | 734           |\n|    total_timesteps              | 648000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020718158 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.339         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 12.1          |\n|    n_updates                    | 322           |\n|    policy_gradient_loss         | -0.000387     |\n|    value_loss                   | 31            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 75            |\n|    water_produced               | 15.7          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.7          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 163           |\n|    time_elapsed                 | 739           |\n|    total_timesteps              | 652000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020122866 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.69         |\n|    explained_variance           | 0.43          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 17.4          |\n|    n_updates                    | 324           |\n|    policy_gradient_loss         | -0.000158     |\n|    value_loss                   | 31            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 139           |\n|    ice_dug                      | 55            |\n|    water_produced               | 7.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.7          |\n| time/                           |               |\n|    fps                          | 881           |\n|    iterations                   | 164           |\n|    time_elapsed                 | 743           |\n|    total_timesteps              | 656000        |\n| train/                          |               |\n|    approx_kl                    | 0.00018896077 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.63         |\n|    explained_variance           | 0.513         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 5.26          |\n|    n_updates                    | 326           |\n|    policy_gradient_loss         | 2.01e-05      |\n|    value_loss                   | 10.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 159           |\n|    water_produced               | 22.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.6          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 165           |\n|    time_elapsed                 | 748           |\n|    total_timesteps              | 660000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017486085 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.67         |\n|    explained_variance           | 0.266         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 34.5          |\n|    n_updates                    | 328           |\n|    policy_gradient_loss         | -0.000219     |\n|    value_loss                   | 62.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 85            |\n|    water_produced               | 20.2          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 17          |\n| time/                           |             |\n|    fps                          | 881         |\n|    iterations                   | 166         |\n|    time_elapsed                 | 752         |\n|    total_timesteps              | 664000      |\n| train/                          |             |\n|    approx_kl                    | 0.001538135 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.6        |\n|    explained_variance           | 0.223       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 33.3        |\n|    n_updates                    | 330         |\n|    policy_gradient_loss         | -0.000144   |\n|    value_loss                   | 55.4        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 130         |\n|    action_queue_updates_total   | 141         |\n|    ice_dug                      | 81          |\n|    water_produced               | 14.8        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.1          |\n| time/                           |               |\n|    fps                          | 882           |\n|    iterations                   | 167           |\n|    time_elapsed                 | 757           |\n|    total_timesteps              | 668000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019238265 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.55         |\n|    explained_variance           | 0.306         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.8          |\n|    n_updates                    | 332           |\n|    policy_gradient_loss         | -0.000152     |\n|    value_loss                   | 33.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 142           |\n|    ice_dug                      | 118           |\n|    water_produced               | 21            |\n---------------------------------------------------\nEval num_timesteps=672000, episode_reward=5.24 +/- 8.85\nEpisode length: 305.00 +/- 8.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 305           |\n|    mean_reward                  | 5.24          |\n| time/                           |               |\n|    total_timesteps              | 672000        |\n| train/                          |               |\n|    approx_kl                    | 0.00022197426 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.64         |\n|    explained_variance           | 0.314         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 21.6          |\n|    n_updates                    | 334           |\n|    policy_gradient_loss         | -0.000167     |\n|    value_loss                   | 38            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 121           |\n|    action_queue_updates_total   | 135           |\n|    ice_dug                      | 82            |\n|    water_produced               | 13.5          |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 19.5     |\n| time/              |          |\n|    fps             | 878      |\n|    iterations      | 168      |\n|    time_elapsed    | 764      |\n|    total_timesteps | 672000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.8         |\n| time/                           |              |\n|    fps                          | 878          |\n|    iterations                   | 169          |\n|    time_elapsed                 | 769          |\n|    total_timesteps              | 676000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006194165 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.306        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15.8         |\n|    n_updates                    | 336          |\n|    policy_gradient_loss         | -2.08e-05    |\n|    value_loss                   | 30.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 122          |\n|    water_produced               | 24.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.6          |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 170           |\n|    time_elapsed                 | 774           |\n|    total_timesteps              | 680000        |\n| train/                          |               |\n|    approx_kl                    | 0.00012223788 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.6          |\n|    explained_variance           | 0.286         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.5          |\n|    n_updates                    | 338           |\n|    policy_gradient_loss         | -0.000216     |\n|    value_loss                   | 45.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 108           |\n|    water_produced               | 19.3          |\n---------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 25.4           |\n| time/                           |                |\n|    fps                          | 878            |\n|    iterations                   | 171            |\n|    time_elapsed                 | 778            |\n|    total_timesteps              | 684000         |\n| train/                          |                |\n|    approx_kl                    | 0.000103289916 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.56          |\n|    explained_variance           | 0.256          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 18.5           |\n|    n_updates                    | 340            |\n|    policy_gradient_loss         | -0.000325      |\n|    value_loss                   | 36.8           |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 136            |\n|    action_queue_updates_total   | 147            |\n|    ice_dug                      | 207            |\n|    water_produced               | 42.2           |\n----------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23.6          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 172           |\n|    time_elapsed                 | 783           |\n|    total_timesteps              | 688000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017056346 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.269         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 41.8          |\n|    n_updates                    | 342           |\n|    policy_gradient_loss         | 7.64e-05      |\n|    value_loss                   | 81.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 138           |\n|    ice_dug                      | 62            |\n|    water_produced               | 12.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.5          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 173           |\n|    time_elapsed                 | 788           |\n|    total_timesteps              | 692000        |\n| train/                          |               |\n|    approx_kl                    | 2.4299025e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.48         |\n|    explained_variance           | 0.315         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.8          |\n|    n_updates                    | 344           |\n|    policy_gradient_loss         | 3.53e-05      |\n|    value_loss                   | 27            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 25            |\n|    water_produced               | 3.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.4         |\n| time/                           |              |\n|    fps                          | 877          |\n|    iterations                   | 174          |\n|    time_elapsed                 | 792          |\n|    total_timesteps              | 696000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011129864 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.577        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.59         |\n|    n_updates                    | 346          |\n|    policy_gradient_loss         | -0.000458    |\n|    value_loss                   | 6.08         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 192          |\n|    water_produced               | 33.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22           |\n| time/                           |              |\n|    fps                          | 877          |\n|    iterations                   | 175          |\n|    time_elapsed                 | 797          |\n|    total_timesteps              | 700000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003357281 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.276        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22.8         |\n|    n_updates                    | 348          |\n|    policy_gradient_loss         | -0.000153    |\n|    value_loss                   | 72           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 109          |\n|    water_produced               | 12           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.2          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 176           |\n|    time_elapsed                 | 802           |\n|    total_timesteps              | 704000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026154923 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.318         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.9          |\n|    n_updates                    | 350           |\n|    policy_gradient_loss         | -0.000216     |\n|    value_loss                   | 28.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 49            |\n|    water_produced               | 9.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15            |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 177           |\n|    time_elapsed                 | 806           |\n|    total_timesteps              | 708000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026825207 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.74         |\n|    explained_variance           | 0.428         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.13          |\n|    n_updates                    | 352           |\n|    policy_gradient_loss         | 0.00012       |\n|    value_loss                   | 12.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 70            |\n|    water_produced               | 12            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.8          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 178           |\n|    time_elapsed                 | 811           |\n|    total_timesteps              | 712000        |\n| train/                          |               |\n|    approx_kl                    | 0.00066508556 |\n|    clip_fraction                | 0.00137       |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.76         |\n|    explained_variance           | 0.359         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.6          |\n|    n_updates                    | 354           |\n|    policy_gradient_loss         | 0.000122      |\n|    value_loss                   | 22.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 158           |\n|    ice_dug                      | 117           |\n|    water_produced               | 21.7          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.3          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 179           |\n|    time_elapsed                 | 815           |\n|    total_timesteps              | 716000        |\n| train/                          |               |\n|    approx_kl                    | 0.00093234255 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.88         |\n|    explained_variance           | 0.368         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.5          |\n|    n_updates                    | 356           |\n|    policy_gradient_loss         | 0.000118      |\n|    value_loss                   | 42.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 109           |\n|    water_produced               | 21.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14.7         |\n| time/                           |              |\n|    fps                          | 877          |\n|    iterations                   | 180          |\n|    time_elapsed                 | 820          |\n|    total_timesteps              | 720000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011250575 |\n|    clip_fraction                | 0.00075      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.31         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22           |\n|    n_updates                    | 358          |\n|    policy_gradient_loss         | -0.000466    |\n|    value_loss                   | 54.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 28           |\n|    water_produced               | 5            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.4         |\n| time/                           |              |\n|    fps                          | 877          |\n|    iterations                   | 181          |\n|    time_elapsed                 | 824          |\n|    total_timesteps              | 724000       |\n| train/                          |              |\n|    approx_kl                    | 0.0038592597 |\n|    clip_fraction                | 0.0161       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.93        |\n|    explained_variance           | 0.741        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.37         |\n|    n_updates                    | 360          |\n|    policy_gradient_loss         | -0.000127    |\n|    value_loss                   | 5.41         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 152          |\n|    water_produced               | 31.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.3          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 182           |\n|    time_elapsed                 | 829           |\n|    total_timesteps              | 728000        |\n| train/                          |               |\n|    approx_kl                    | 0.00011359898 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.68         |\n|    explained_variance           | 0.308         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 37.3          |\n|    n_updates                    | 362           |\n|    policy_gradient_loss         | -4.03e-05     |\n|    value_loss                   | 59.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 162           |\n|    ice_dug                      | 128           |\n|    water_produced               | 16.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.1          |\n| time/                           |               |\n|    fps                          | 877           |\n|    iterations                   | 183           |\n|    time_elapsed                 | 833           |\n|    total_timesteps              | 732000        |\n| train/                          |               |\n|    approx_kl                    | 6.9310365e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.83         |\n|    explained_variance           | 0.397         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 16.5          |\n|    n_updates                    | 364           |\n|    policy_gradient_loss         | 7.25e-06      |\n|    value_loss                   | 33.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 70            |\n|    water_produced               | 11            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.9         |\n| time/                           |              |\n|    fps                          | 878          |\n|    iterations                   | 184          |\n|    time_elapsed                 | 838          |\n|    total_timesteps              | 736000       |\n| train/                          |              |\n|    approx_kl                    | 0.0009790689 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.8         |\n|    explained_variance           | 0.48         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 10.1         |\n|    n_updates                    | 366          |\n|    policy_gradient_loss         | 0.000889     |\n|    value_loss                   | 18.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 238          |\n|    water_produced               | 39.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.2         |\n| time/                           |              |\n|    fps                          | 878          |\n|    iterations                   | 185          |\n|    time_elapsed                 | 842          |\n|    total_timesteps              | 740000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015198134 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.88        |\n|    explained_variance           | 0.389        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 43.4         |\n|    n_updates                    | 368          |\n|    policy_gradient_loss         | -0.000326    |\n|    value_loss                   | 72.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 138          |\n|    water_produced               | 10.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19            |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 186           |\n|    time_elapsed                 | 847           |\n|    total_timesteps              | 744000        |\n| train/                          |               |\n|    approx_kl                    | 0.00016919918 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.91         |\n|    explained_variance           | 0.623         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.58          |\n|    n_updates                    | 370           |\n|    policy_gradient_loss         | -0.000173     |\n|    value_loss                   | 15.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 163           |\n|    ice_dug                      | 68            |\n|    water_produced               | 11.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.8         |\n| time/                           |              |\n|    fps                          | 878          |\n|    iterations                   | 187          |\n|    time_elapsed                 | 851          |\n|    total_timesteps              | 748000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016052086 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.98        |\n|    explained_variance           | 0.625        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 10.4         |\n|    n_updates                    | 372          |\n|    policy_gradient_loss         | -3.54e-05    |\n|    value_loss                   | 18.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 102          |\n|    water_produced               | 20.5         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 19.8        |\n| time/                           |             |\n|    fps                          | 878         |\n|    iterations                   | 188         |\n|    time_elapsed                 | 856         |\n|    total_timesteps              | 752000      |\n| train/                          |             |\n|    approx_kl                    | 0.000202163 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.83       |\n|    explained_variance           | 0.482       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 15.2        |\n|    n_updates                    | 374         |\n|    policy_gradient_loss         | 0.000234    |\n|    value_loss                   | 35.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 138         |\n|    action_queue_updates_total   | 162         |\n|    ice_dug                      | 79          |\n|    water_produced               | 10.8        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.8         |\n| time/                           |              |\n|    fps                          | 878          |\n|    iterations                   | 189          |\n|    time_elapsed                 | 860          |\n|    total_timesteps              | 756000       |\n| train/                          |              |\n|    approx_kl                    | 0.0021645874 |\n|    clip_fraction                | 0.000375     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.96        |\n|    explained_variance           | 0.524        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 7.79         |\n|    n_updates                    | 376          |\n|    policy_gradient_loss         | 3.79e-05     |\n|    value_loss                   | 23.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 66           |\n|    water_produced               | 11.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.4          |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 190           |\n|    time_elapsed                 | 865           |\n|    total_timesteps              | 760000        |\n| train/                          |               |\n|    approx_kl                    | 0.00049861905 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.86         |\n|    explained_variance           | 0.602         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.7          |\n|    n_updates                    | 378           |\n|    policy_gradient_loss         | -4.28e-05     |\n|    value_loss                   | 21.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 40            |\n|    water_produced               | 9             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.1          |\n| time/                           |               |\n|    fps                          | 878           |\n|    iterations                   | 191           |\n|    time_elapsed                 | 869           |\n|    total_timesteps              | 764000        |\n| train/                          |               |\n|    approx_kl                    | 0.00056018506 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.83         |\n|    explained_variance           | 0.633         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11            |\n|    n_updates                    | 380           |\n|    policy_gradient_loss         | -0.000282     |\n|    value_loss                   | 17.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 139           |\n|    water_produced               | 29.2          |\n---------------------------------------------------\nEval num_timesteps=768000, episode_reward=0.00 +/- 0.00\nEpisode length: 301.00 +/- 0.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 301          |\n|    mean_reward                  | 0            |\n| time/                           |              |\n|    total_timesteps              | 768000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015369231 |\n|    clip_fraction                | 0.00262      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.87        |\n|    explained_variance           | 0.458        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 32           |\n|    n_updates                    | 382          |\n|    policy_gradient_loss         | -0.000529    |\n|    value_loss                   | 57.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 158          |\n|    water_produced               | 29.2         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 18.9     |\n| time/              |          |\n|    fps             | 875      |\n|    iterations      | 192      |\n|    time_elapsed    | 876      |\n|    total_timesteps | 768000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.3         |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 193          |\n|    time_elapsed                 | 881          |\n|    total_timesteps              | 772000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011260008 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.402        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 34.6         |\n|    n_updates                    | 384          |\n|    policy_gradient_loss         | -2.04e-05    |\n|    value_loss                   | 61.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 42           |\n|    water_produced               | 8            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.5          |\n| time/                           |               |\n|    fps                          | 875           |\n|    iterations                   | 194           |\n|    time_elapsed                 | 886           |\n|    total_timesteps              | 776000        |\n| train/                          |               |\n|    approx_kl                    | 0.00096253026 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.604         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.78          |\n|    n_updates                    | 386           |\n|    policy_gradient_loss         | 0.000483      |\n|    value_loss                   | 14.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 133           |\n|    water_produced               | 26.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 25.5          |\n| time/                           |               |\n|    fps                          | 875           |\n|    iterations                   | 195           |\n|    time_elapsed                 | 890           |\n|    total_timesteps              | 780000        |\n| train/                          |               |\n|    approx_kl                    | 0.00034384383 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.485         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 21.4          |\n|    n_updates                    | 388           |\n|    policy_gradient_loss         | -6.56e-05     |\n|    value_loss                   | 43.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 137           |\n|    water_produced               | 28.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.5         |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 196          |\n|    time_elapsed                 | 895          |\n|    total_timesteps              | 784000       |\n| train/                          |              |\n|    approx_kl                    | 0.0021298998 |\n|    clip_fraction                | 0.00212      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.362        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 34.3         |\n|    n_updates                    | 390          |\n|    policy_gradient_loss         | -0.000806    |\n|    value_loss                   | 57           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 58           |\n|    water_produced               | 10           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.2          |\n| time/                           |               |\n|    fps                          | 875           |\n|    iterations                   | 197           |\n|    time_elapsed                 | 899           |\n|    total_timesteps              | 788000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014952707 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.81         |\n|    explained_variance           | 0.471         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 9             |\n|    n_updates                    | 392           |\n|    policy_gradient_loss         | -0.000124     |\n|    value_loss                   | 24.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 108           |\n|    water_produced               | 18            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.7          |\n| time/                           |               |\n|    fps                          | 875           |\n|    iterations                   | 198           |\n|    time_elapsed                 | 904           |\n|    total_timesteps              | 792000        |\n| train/                          |               |\n|    approx_kl                    | 0.00022887201 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.69         |\n|    explained_variance           | 0.469         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.4          |\n|    n_updates                    | 394           |\n|    policy_gradient_loss         | -4.75e-05     |\n|    value_loss                   | 28.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 126           |\n|    water_produced               | 14.7          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.9         |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 199          |\n|    time_elapsed                 | 908          |\n|    total_timesteps              | 796000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001751883 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.319        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 18.6         |\n|    n_updates                    | 396          |\n|    policy_gradient_loss         | -0.000129    |\n|    value_loss                   | 41.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 121          |\n|    water_produced               | 27.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.1         |\n| time/                           |              |\n|    fps                          | 875          |\n|    iterations                   | 200          |\n|    time_elapsed                 | 914          |\n|    total_timesteps              | 800000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015025137 |\n|    clip_fraction                | 0.000875     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.75        |\n|    explained_variance           | 0.349        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 30.4         |\n|    n_updates                    | 398          |\n|    policy_gradient_loss         | -0.000833    |\n|    value_loss                   | 62.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 111          |\n|    water_produced               | 20           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.6         |\n| time/                           |              |\n|    fps                          | 874          |\n|    iterations                   | 201          |\n|    time_elapsed                 | 919          |\n|    total_timesteps              | 804000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001693565 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.396        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22.1         |\n|    n_updates                    | 400          |\n|    policy_gradient_loss         | 9.85e-05     |\n|    value_loss                   | 35.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 39           |\n|    water_produced               | 2.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.2         |\n| time/                           |              |\n|    fps                          | 874          |\n|    iterations                   | 202          |\n|    time_elapsed                 | 924          |\n|    total_timesteps              | 808000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015583395 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.68        |\n|    explained_variance           | 0.712        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.72         |\n|    n_updates                    | 402          |\n|    policy_gradient_loss         | -1.11e-05    |\n|    value_loss                   | 6.37         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 144          |\n|    ice_dug                      | 173          |\n|    water_produced               | 30.7         |\n--------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 22.8           |\n| time/                           |                |\n|    fps                          | 873            |\n|    iterations                   | 203            |\n|    time_elapsed                 | 929            |\n|    total_timesteps              | 812000         |\n| train/                          |                |\n|    approx_kl                    | 0.000101483776 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.65          |\n|    explained_variance           | 0.353          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 36.9           |\n|    n_updates                    | 404            |\n|    policy_gradient_loss         | -7.24e-05      |\n|    value_loss                   | 64             |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 130            |\n|    action_queue_updates_total   | 144            |\n|    ice_dug                      | 128            |\n|    water_produced               | 27.7           |\n----------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.9         |\n| time/                           |              |\n|    fps                          | 873          |\n|    iterations                   | 204          |\n|    time_elapsed                 | 934          |\n|    total_timesteps              | 816000       |\n| train/                          |              |\n|    approx_kl                    | 0.0007318262 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.333        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 31.7         |\n|    n_updates                    | 406          |\n|    policy_gradient_loss         | -0.000503    |\n|    value_loss                   | 65.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 145          |\n|    water_produced               | 27.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.9          |\n| time/                           |               |\n|    fps                          | 873           |\n|    iterations                   | 205           |\n|    time_elapsed                 | 939           |\n|    total_timesteps              | 820000        |\n| train/                          |               |\n|    approx_kl                    | 0.00013486322 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.68         |\n|    explained_variance           | 0.363         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 33.7          |\n|    n_updates                    | 408           |\n|    policy_gradient_loss         | -0.000349     |\n|    value_loss                   | 70.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 212           |\n|    water_produced               | 39.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 28.9          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 206           |\n|    time_elapsed                 | 944           |\n|    total_timesteps              | 824000        |\n| train/                          |               |\n|    approx_kl                    | 0.00041726144 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.55         |\n|    explained_variance           | 0.317         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 42.5          |\n|    n_updates                    | 410           |\n|    policy_gradient_loss         | -0.000502     |\n|    value_loss                   | 92.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 58            |\n|    water_produced               | 12            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.3          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 207           |\n|    time_elapsed                 | 948           |\n|    total_timesteps              | 828000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026639557 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.64         |\n|    explained_variance           | 0.41          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.5          |\n|    n_updates                    | 412           |\n|    policy_gradient_loss         | -2.44e-05     |\n|    value_loss                   | 31.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 126           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 99            |\n|    water_produced               | 18.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.5          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 208           |\n|    time_elapsed                 | 953           |\n|    total_timesteps              | 832000        |\n| train/                          |               |\n|    approx_kl                    | 0.00089280866 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.315         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 27            |\n|    n_updates                    | 414           |\n|    policy_gradient_loss         | -8.18e-05     |\n|    value_loss                   | 51.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 143           |\n|    ice_dug                      | 177           |\n|    water_produced               | 28.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 25.5          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 209           |\n|    time_elapsed                 | 958           |\n|    total_timesteps              | 836000        |\n| train/                          |               |\n|    approx_kl                    | 0.00011344401 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.33          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 36            |\n|    n_updates                    | 416           |\n|    policy_gradient_loss         | -0.000251     |\n|    value_loss                   | 86.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 145           |\n|    ice_dug                      | 131           |\n|    water_produced               | 22.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.9          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 210           |\n|    time_elapsed                 | 962           |\n|    total_timesteps              | 840000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014708556 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.368         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 29.8          |\n|    n_updates                    | 418           |\n|    policy_gradient_loss         | -0.000244     |\n|    value_loss                   | 50.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 120           |\n|    action_queue_updates_total   | 133           |\n|    ice_dug                      | 82            |\n|    water_produced               | 7.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.3          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 211           |\n|    time_elapsed                 | 967           |\n|    total_timesteps              | 844000        |\n| train/                          |               |\n|    approx_kl                    | 0.00051919126 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.54         |\n|    explained_variance           | 0.522         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 8.22          |\n|    n_updates                    | 420           |\n|    policy_gradient_loss         | 0.000174      |\n|    value_loss                   | 19.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 145           |\n|    ice_dug                      | 74            |\n|    water_produced               | 8.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.6          |\n| time/                           |               |\n|    fps                          | 872           |\n|    iterations                   | 212           |\n|    time_elapsed                 | 972           |\n|    total_timesteps              | 848000        |\n| train/                          |               |\n|    approx_kl                    | 0.00045394036 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.478         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.6          |\n|    n_updates                    | 422           |\n|    policy_gradient_loss         | 0.000123      |\n|    value_loss                   | 22.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 137           |\n|    ice_dug                      | 117           |\n|    water_produced               | 9.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 14.7          |\n| time/                           |               |\n|    fps                          | 871           |\n|    iterations                   | 213           |\n|    time_elapsed                 | 977           |\n|    total_timesteps              | 852000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015867902 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.63         |\n|    explained_variance           | 0.435         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.1          |\n|    n_updates                    | 424           |\n|    policy_gradient_loss         | -0.000133     |\n|    value_loss                   | 24.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 96            |\n|    water_produced               | 19.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 12.1         |\n| time/                           |              |\n|    fps                          | 871          |\n|    iterations                   | 214          |\n|    time_elapsed                 | 982          |\n|    total_timesteps              | 856000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003449091 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.72        |\n|    explained_variance           | 0.364        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 27.4         |\n|    n_updates                    | 426          |\n|    policy_gradient_loss         | -0.000261    |\n|    value_loss                   | 47.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 122          |\n|    action_queue_updates_total   | 137          |\n|    ice_dug                      | 48           |\n|    water_produced               | 10.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.1          |\n| time/                           |               |\n|    fps                          | 870           |\n|    iterations                   | 215           |\n|    time_elapsed                 | 987           |\n|    total_timesteps              | 860000        |\n| train/                          |               |\n|    approx_kl                    | 0.00028756118 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.424         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.6          |\n|    n_updates                    | 428           |\n|    policy_gradient_loss         | -6.61e-05     |\n|    value_loss                   | 22.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 143           |\n|    ice_dug                      | 179           |\n|    water_produced               | 21.5          |\n---------------------------------------------------\nEval num_timesteps=864000, episode_reward=92.56 +/- 51.51\nEpisode length: 389.00 +/- 49.05\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 389          |\n|    mean_reward                  | 92.6         |\n| time/                           |              |\n|    total_timesteps              | 864000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003211095 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.336        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 23.3         |\n|    n_updates                    | 430          |\n|    policy_gradient_loss         | 6.88e-05     |\n|    value_loss                   | 50.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 156          |\n|    water_produced               | 27           |\n--------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 18.9     |\n| time/              |          |\n|    fps             | 867      |\n|    iterations      | 216      |\n|    time_elapsed    | 995      |\n|    total_timesteps | 864000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.2         |\n| time/                           |              |\n|    fps                          | 867          |\n|    iterations                   | 217          |\n|    time_elapsed                 | 1000         |\n|    total_timesteps              | 868000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003713021 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.35         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 37           |\n|    n_updates                    | 432          |\n|    policy_gradient_loss         | 6.79e-05     |\n|    value_loss                   | 79.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 59           |\n|    water_produced               | 11.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.7         |\n| time/                           |              |\n|    fps                          | 867          |\n|    iterations                   | 218          |\n|    time_elapsed                 | 1005         |\n|    total_timesteps              | 872000       |\n| train/                          |              |\n|    approx_kl                    | 0.0007994392 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.432        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.1         |\n|    n_updates                    | 434          |\n|    policy_gradient_loss         | -0.000415    |\n|    value_loss                   | 25.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 130          |\n|    water_produced               | 22           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.7          |\n| time/                           |               |\n|    fps                          | 866           |\n|    iterations                   | 219           |\n|    time_elapsed                 | 1010          |\n|    total_timesteps              | 876000        |\n| train/                          |               |\n|    approx_kl                    | 4.7599304e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.395         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 23.5          |\n|    n_updates                    | 436           |\n|    policy_gradient_loss         | 8.82e-05      |\n|    value_loss                   | 71            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 147           |\n|    ice_dug                      | 48            |\n|    water_produced               | 10.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20           |\n| time/                           |              |\n|    fps                          | 866          |\n|    iterations                   | 220          |\n|    time_elapsed                 | 1015         |\n|    total_timesteps              | 880000       |\n| train/                          |              |\n|    approx_kl                    | 9.761705e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.85        |\n|    explained_variance           | 0.455        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15.2         |\n|    n_updates                    | 438          |\n|    policy_gradient_loss         | 1.25e-07     |\n|    value_loss                   | 28.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 119          |\n|    action_queue_updates_total   | 131          |\n|    ice_dug                      | 132          |\n|    water_produced               | 23.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.3         |\n| time/                           |              |\n|    fps                          | 866          |\n|    iterations                   | 221          |\n|    time_elapsed                 | 1020         |\n|    total_timesteps              | 884000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008234276 |\n|    clip_fraction                | 0.00075      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.57        |\n|    explained_variance           | 0.366        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 24.9         |\n|    n_updates                    | 440          |\n|    policy_gradient_loss         | -0.000351    |\n|    value_loss                   | 52.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 137          |\n|    ice_dug                      | 56           |\n|    water_produced               | 9.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.7          |\n| time/                           |               |\n|    fps                          | 866           |\n|    iterations                   | 222           |\n|    time_elapsed                 | 1024          |\n|    total_timesteps              | 888000        |\n| train/                          |               |\n|    approx_kl                    | 0.00032900774 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.63         |\n|    explained_variance           | 0.36          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.9          |\n|    n_updates                    | 442           |\n|    policy_gradient_loss         | -0.000486     |\n|    value_loss                   | 23.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 142           |\n|    ice_dug                      | 116           |\n|    water_produced               | 18.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.4          |\n| time/                           |               |\n|    fps                          | 866           |\n|    iterations                   | 223           |\n|    time_elapsed                 | 1029          |\n|    total_timesteps              | 892000        |\n| train/                          |               |\n|    approx_kl                    | 0.00010123962 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.413         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.8          |\n|    n_updates                    | 444           |\n|    policy_gradient_loss         | -0.000133     |\n|    value_loss                   | 36.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 127           |\n|    water_produced               | 25.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 24.1          |\n| time/                           |               |\n|    fps                          | 865           |\n|    iterations                   | 224           |\n|    time_elapsed                 | 1034          |\n|    total_timesteps              | 896000        |\n| train/                          |               |\n|    approx_kl                    | 0.00043650725 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.66         |\n|    explained_variance           | 0.384         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 27.5          |\n|    n_updates                    | 446           |\n|    policy_gradient_loss         | 5.24e-05      |\n|    value_loss                   | 59.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 221           |\n|    water_produced               | 37.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 28.7         |\n| time/                           |              |\n|    fps                          | 865          |\n|    iterations                   | 225          |\n|    time_elapsed                 | 1039         |\n|    total_timesteps              | 900000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003205806 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.54        |\n|    explained_variance           | 0.304        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 47.2         |\n|    n_updates                    | 448          |\n|    policy_gradient_loss         | -0.000135    |\n|    value_loss                   | 107          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 125          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 226          |\n|    water_produced               | 45.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 32.9         |\n| time/                           |              |\n|    fps                          | 865          |\n|    iterations                   | 226          |\n|    time_elapsed                 | 1044         |\n|    total_timesteps              | 904000       |\n| train/                          |              |\n|    approx_kl                    | 0.0010600372 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.397        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 46.3         |\n|    n_updates                    | 450          |\n|    policy_gradient_loss         | 4.57e-06     |\n|    value_loss                   | 97           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 127          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 132          |\n|    water_produced               | 29.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 38.3          |\n| time/                           |               |\n|    fps                          | 865           |\n|    iterations                   | 227           |\n|    time_elapsed                 | 1049          |\n|    total_timesteps              | 908000        |\n| train/                          |               |\n|    approx_kl                    | 0.00043743983 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.7          |\n|    explained_variance           | 0.382         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 35.5          |\n|    n_updates                    | 452           |\n|    policy_gradient_loss         | 5.05e-05      |\n|    value_loss                   | 76.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 209           |\n|    water_produced               | 44            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 36.1          |\n| time/                           |               |\n|    fps                          | 864           |\n|    iterations                   | 228           |\n|    time_elapsed                 | 1054          |\n|    total_timesteps              | 912000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015562904 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.56         |\n|    explained_variance           | 0.33          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 50.8          |\n|    n_updates                    | 454           |\n|    policy_gradient_loss         | -0.000223     |\n|    value_loss                   | 108           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 124           |\n|    action_queue_updates_total   | 127           |\n|    ice_dug                      | 90            |\n|    water_produced               | 14.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 33.2          |\n| time/                           |               |\n|    fps                          | 864           |\n|    iterations                   | 229           |\n|    time_elapsed                 | 1059          |\n|    total_timesteps              | 916000        |\n| train/                          |               |\n|    approx_kl                    | 6.7222936e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.34         |\n|    explained_variance           | 0.315         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.5          |\n|    n_updates                    | 456           |\n|    policy_gradient_loss         | -0.000196     |\n|    value_loss                   | 33.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 112           |\n|    action_queue_updates_total   | 118           |\n|    ice_dug                      | 111           |\n|    water_produced               | 24.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23.9          |\n| time/                           |               |\n|    fps                          | 864           |\n|    iterations                   | 230           |\n|    time_elapsed                 | 1064          |\n|    total_timesteps              | 920000        |\n| train/                          |               |\n|    approx_kl                    | 0.00039507946 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.38         |\n|    explained_variance           | 0.311         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 29.9          |\n|    n_updates                    | 458           |\n|    policy_gradient_loss         | -0.000379     |\n|    value_loss                   | 62            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 34            |\n|    water_produced               | 1             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.5         |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 231          |\n|    time_elapsed                 | 1069         |\n|    total_timesteps              | 924000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003045352 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.743        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.34         |\n|    n_updates                    | 460          |\n|    policy_gradient_loss         | -7.69e-05    |\n|    value_loss                   | 7.32         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 181          |\n|    water_produced               | 22.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.5         |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 232          |\n|    time_elapsed                 | 1074         |\n|    total_timesteps              | 928000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005018158 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.87        |\n|    explained_variance           | 0.431        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21           |\n|    n_updates                    | 462          |\n|    policy_gradient_loss         | -5.04e-05    |\n|    value_loss                   | 43.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 109          |\n|    water_produced               | 10           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.5         |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 233          |\n|    time_elapsed                 | 1079         |\n|    total_timesteps              | 932000       |\n| train/                          |              |\n|    approx_kl                    | 0.0007851451 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.507        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.6         |\n|    n_updates                    | 464          |\n|    policy_gradient_loss         | 1.68e-05     |\n|    value_loss                   | 23.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 122          |\n|    action_queue_updates_total   | 137          |\n|    ice_dug                      | 149          |\n|    water_produced               | 29           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.1         |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 234          |\n|    time_elapsed                 | 1084         |\n|    total_timesteps              | 936000       |\n| train/                          |              |\n|    approx_kl                    | 8.262141e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.413        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 35.8         |\n|    n_updates                    | 466          |\n|    policy_gradient_loss         | -6.09e-05    |\n|    value_loss                   | 55.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 126          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 169          |\n|    water_produced               | 36.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 24.4         |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 235          |\n|    time_elapsed                 | 1088         |\n|    total_timesteps              | 940000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005505717 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.65        |\n|    explained_variance           | 0.395        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 44.4         |\n|    n_updates                    | 468          |\n|    policy_gradient_loss         | -0.000464    |\n|    value_loss                   | 90.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 158          |\n|    water_produced               | 16.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.8         |\n| time/                           |              |\n|    fps                          | 863          |\n|    iterations                   | 236          |\n|    time_elapsed                 | 1093         |\n|    total_timesteps              | 944000       |\n| train/                          |              |\n|    approx_kl                    | 0.0020641664 |\n|    clip_fraction                | 0.00663      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.407        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 18.1         |\n|    n_updates                    | 470          |\n|    policy_gradient_loss         | 0.000386     |\n|    value_loss                   | 36.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 111          |\n|    water_produced               | 20           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 31.7          |\n| time/                           |               |\n|    fps                          | 862           |\n|    iterations                   | 237           |\n|    time_elapsed                 | 1099          |\n|    total_timesteps              | 948000        |\n| train/                          |               |\n|    approx_kl                    | 0.00040785372 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.6          |\n|    explained_variance           | 0.391         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.2          |\n|    n_updates                    | 472           |\n|    policy_gradient_loss         | -3.13e-05     |\n|    value_loss                   | 41.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 255           |\n|    water_produced               | 48            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 36.1         |\n| time/                           |              |\n|    fps                          | 862          |\n|    iterations                   | 238          |\n|    time_elapsed                 | 1104         |\n|    total_timesteps              | 952000       |\n| train/                          |              |\n|    approx_kl                    | 0.0017728902 |\n|    clip_fraction                | 0.00425      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.393        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 46.6         |\n|    n_updates                    | 474          |\n|    policy_gradient_loss         | 2.96e-05     |\n|    value_loss                   | 98.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 220          |\n|    water_produced               | 50           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 29.2         |\n| time/                           |              |\n|    fps                          | 862          |\n|    iterations                   | 239          |\n|    time_elapsed                 | 1108         |\n|    total_timesteps              | 956000       |\n| train/                          |              |\n|    approx_kl                    | 0.0019152559 |\n|    clip_fraction                | 0.000625     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.367        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 53.5         |\n|    n_updates                    | 476          |\n|    policy_gradient_loss         | -0.000712    |\n|    value_loss                   | 122          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 122          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 18           |\n|    water_produced               | 3.75         |\n--------------------------------------------------\nEval num_timesteps=960000, episode_reward=37.60 +/- 75.00\nEpisode length: 337.00 +/- 72.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 337          |\n|    mean_reward                  | 37.6         |\n| time/                           |              |\n|    total_timesteps              | 960000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005348517 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.62        |\n|    explained_variance           | 0.668        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 4.98         |\n|    n_updates                    | 478          |\n|    policy_gradient_loss         | 0.000477     |\n|    value_loss                   | 10.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 309          |\n|    water_produced               | 56.5         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 37.5     |\n| time/              |          |\n|    fps             | 859      |\n|    iterations      | 240      |\n|    time_elapsed    | 1116     |\n|    total_timesteps | 960000   |\n---------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 40.6        |\n| time/                           |             |\n|    fps                          | 859         |\n|    iterations                   | 241         |\n|    time_elapsed                 | 1121        |\n|    total_timesteps              | 964000      |\n| train/                          |             |\n|    approx_kl                    | 3.21278e-05 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.53       |\n|    explained_variance           | 0.356       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 68.6        |\n|    n_updates                    | 480         |\n|    policy_gradient_loss         | -5.95e-05   |\n|    value_loss                   | 138         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 126         |\n|    action_queue_updates_total   | 134         |\n|    ice_dug                      | 180         |\n|    water_produced               | 35          |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 32.1          |\n| time/                           |               |\n|    fps                          | 859           |\n|    iterations                   | 242           |\n|    time_elapsed                 | 1126          |\n|    total_timesteps              | 968000        |\n| train/                          |               |\n|    approx_kl                    | 0.00023631603 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.45         |\n|    explained_variance           | 0.362         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 41.5          |\n|    n_updates                    | 482           |\n|    policy_gradient_loss         | 3.24e-05      |\n|    value_loss                   | 91.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 85            |\n|    water_produced               | 7.25          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23           |\n| time/                           |              |\n|    fps                          | 859          |\n|    iterations                   | 243          |\n|    time_elapsed                 | 1131         |\n|    total_timesteps              | 972000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011677453 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.6          |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.09         |\n|    n_updates                    | 484          |\n|    policy_gradient_loss         | 0.000343     |\n|    value_loss                   | 18.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 141          |\n|    ice_dug                      | 44           |\n|    water_produced               | 6            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 28.7         |\n| time/                           |              |\n|    fps                          | 859          |\n|    iterations                   | 244          |\n|    time_elapsed                 | 1135         |\n|    total_timesteps              | 976000       |\n| train/                          |              |\n|    approx_kl                    | 0.0010476264 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.626        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 5.5          |\n|    n_updates                    | 486          |\n|    policy_gradient_loss         | 0.000564     |\n|    value_loss                   | 11.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 140          |\n|    ice_dug                      | 154          |\n|    water_produced               | 31.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.7          |\n| time/                           |               |\n|    fps                          | 859           |\n|    iterations                   | 245           |\n|    time_elapsed                 | 1140          |\n|    total_timesteps              | 980000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020335914 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.6          |\n|    explained_variance           | 0.431         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 35.5          |\n|    n_updates                    | 488           |\n|    policy_gradient_loss         | -0.000159     |\n|    value_loss                   | 61.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 78            |\n|    water_produced               | 18.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.7         |\n| time/                           |              |\n|    fps                          | 859          |\n|    iterations                   | 246          |\n|    time_elapsed                 | 1145         |\n|    total_timesteps              | 984000       |\n| train/                          |              |\n|    approx_kl                    | 0.0029804884 |\n|    clip_fraction                | 0.013        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.443        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 23.8         |\n|    n_updates                    | 490          |\n|    policy_gradient_loss         | -0.000349    |\n|    value_loss                   | 56.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 150          |\n|    water_produced               | 30.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.4          |\n| time/                           |               |\n|    fps                          | 859           |\n|    iterations                   | 247           |\n|    time_elapsed                 | 1150          |\n|    total_timesteps              | 988000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021298481 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.441         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 48.7          |\n|    n_updates                    | 492           |\n|    policy_gradient_loss         | -0.000215     |\n|    value_loss                   | 88            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 129           |\n|    action_queue_updates_total   | 147           |\n|    ice_dug                      | 60            |\n|    water_produced               | 10.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.9         |\n| time/                           |              |\n|    fps                          | 859          |\n|    iterations                   | 248          |\n|    time_elapsed                 | 1154         |\n|    total_timesteps              | 992000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011334476 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.543        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 11           |\n|    n_updates                    | 494          |\n|    policy_gradient_loss         | 0.000349     |\n|    value_loss                   | 22.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 113          |\n|    water_produced               | 23           |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 20.1        |\n| time/                           |             |\n|    fps                          | 858         |\n|    iterations                   | 249         |\n|    time_elapsed                 | 1160        |\n|    total_timesteps              | 996000      |\n| train/                          |             |\n|    approx_kl                    | 0.000395175 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.76       |\n|    explained_variance           | 0.536       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 23          |\n|    n_updates                    | 496         |\n|    policy_gradient_loss         | -0.000337   |\n|    value_loss                   | 51.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 123         |\n|    action_queue_updates_total   | 136         |\n|    ice_dug                      | 72          |\n|    water_produced               | 13          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.6         |\n| time/                           |              |\n|    fps                          | 858          |\n|    iterations                   | 250          |\n|    time_elapsed                 | 1164         |\n|    total_timesteps              | 1000000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004987839 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.515        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.6         |\n|    n_updates                    | 498          |\n|    policy_gradient_loss         | -1.82e-05    |\n|    value_loss                   | 28.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 61           |\n|    water_produced               | 6.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.5         |\n| time/                           |              |\n|    fps                          | 858          |\n|    iterations                   | 251          |\n|    time_elapsed                 | 1170         |\n|    total_timesteps              | 1004000      |\n| train/                          |              |\n|    approx_kl                    | 0.0014642596 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2           |\n|    explained_variance           | 0.636        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 10.1         |\n|    n_updates                    | 500          |\n|    policy_gradient_loss         | 0.000265     |\n|    value_loss                   | 17.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 98           |\n|    water_produced               | 20.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.8         |\n| time/                           |              |\n|    fps                          | 857          |\n|    iterations                   | 252          |\n|    time_elapsed                 | 1175         |\n|    total_timesteps              | 1008000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010990673 |\n|    clip_fraction                | 0.0015       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.92        |\n|    explained_variance           | 0.509        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 30.2         |\n|    n_updates                    | 502          |\n|    policy_gradient_loss         | -0.000299    |\n|    value_loss                   | 47.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 171          |\n|    water_produced               | 31.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23.9          |\n| time/                           |               |\n|    fps                          | 857           |\n|    iterations                   | 253           |\n|    time_elapsed                 | 1179          |\n|    total_timesteps              | 1012000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031601347 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.73         |\n|    explained_variance           | 0.458         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 32.6          |\n|    n_updates                    | 504           |\n|    policy_gradient_loss         | 0.000166      |\n|    value_loss                   | 72.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 198           |\n|    water_produced               | 42.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 24.9         |\n| time/                           |              |\n|    fps                          | 857          |\n|    iterations                   | 254          |\n|    time_elapsed                 | 1184         |\n|    total_timesteps              | 1016000      |\n| train/                          |              |\n|    approx_kl                    | 0.0003452353 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.436        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 48.7         |\n|    n_updates                    | 506          |\n|    policy_gradient_loss         | -9.78e-05    |\n|    value_loss                   | 107          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 79           |\n|    water_produced               | 18           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 25.9         |\n| time/                           |              |\n|    fps                          | 856          |\n|    iterations                   | 255          |\n|    time_elapsed                 | 1190         |\n|    total_timesteps              | 1020000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005792126 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.84        |\n|    explained_variance           | 0.505        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21           |\n|    n_updates                    | 508          |\n|    policy_gradient_loss         | 0.000189     |\n|    value_loss                   | 50.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 55           |\n|    water_produced               | 11.3         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 26.5         |\n| time/                           |              |\n|    fps                          | 856          |\n|    iterations                   | 256          |\n|    time_elapsed                 | 1195         |\n|    total_timesteps              | 1024000      |\n| train/                          |              |\n|    approx_kl                    | 0.0009578364 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.54         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.6         |\n|    n_updates                    | 510          |\n|    policy_gradient_loss         | -0.00029     |\n|    value_loss                   | 30.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 148          |\n|    water_produced               | 22.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 27.2         |\n| time/                           |              |\n|    fps                          | 856          |\n|    iterations                   | 257          |\n|    time_elapsed                 | 1199         |\n|    total_timesteps              | 1028000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005047376 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.472        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 23.2         |\n|    n_updates                    | 512          |\n|    policy_gradient_loss         | -1.06e-05    |\n|    value_loss                   | 66.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 210          |\n|    water_produced               | 34.5         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 24.9        |\n| time/                           |             |\n|    fps                          | 856         |\n|    iterations                   | 258         |\n|    time_elapsed                 | 1204        |\n|    total_timesteps              | 1032000     |\n| train/                          |             |\n|    approx_kl                    | 0.000624693 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.76       |\n|    explained_variance           | 0.459       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 30.3        |\n|    n_updates                    | 514         |\n|    policy_gradient_loss         | -4.47e-05   |\n|    value_loss                   | 90.4        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 132         |\n|    action_queue_updates_total   | 142         |\n|    ice_dug                      | 199         |\n|    water_produced               | 31.2        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.5         |\n| time/                           |              |\n|    fps                          | 856          |\n|    iterations                   | 259          |\n|    time_elapsed                 | 1208         |\n|    total_timesteps              | 1036000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010614784 |\n|    clip_fraction                | 0.00125      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.463        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 39.1         |\n|    n_updates                    | 516          |\n|    policy_gradient_loss         | 0.000329     |\n|    value_loss                   | 72.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 34           |\n|    water_produced               | 6.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 25.7         |\n| time/                           |              |\n|    fps                          | 856          |\n|    iterations                   | 260          |\n|    time_elapsed                 | 1213         |\n|    total_timesteps              | 1040000      |\n| train/                          |              |\n|    approx_kl                    | 0.0020411522 |\n|    clip_fraction                | 0.00288      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.77        |\n|    explained_variance           | 0.592        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.72         |\n|    n_updates                    | 518          |\n|    policy_gradient_loss         | 0.000491     |\n|    value_loss                   | 21.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 126          |\n|    water_produced               | 26.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 27.3          |\n| time/                           |               |\n|    fps                          | 856           |\n|    iterations                   | 261           |\n|    time_elapsed                 | 1219          |\n|    total_timesteps              | 1044000       |\n| train/                          |               |\n|    approx_kl                    | 0.00035780176 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.81         |\n|    explained_variance           | 0.577         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.8          |\n|    n_updates                    | 520           |\n|    policy_gradient_loss         | -0.000354     |\n|    value_loss                   | 56.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 132           |\n|    water_produced               | 30.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 22.8          |\n| time/                           |               |\n|    fps                          | 855           |\n|    iterations                   | 262           |\n|    time_elapsed                 | 1224          |\n|    total_timesteps              | 1048000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031716534 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.505         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 51.5          |\n|    n_updates                    | 522           |\n|    policy_gradient_loss         | 5.17e-05      |\n|    value_loss                   | 90.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 71            |\n|    water_produced               | 13.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.5         |\n| time/                           |              |\n|    fps                          | 855          |\n|    iterations                   | 263          |\n|    time_elapsed                 | 1229         |\n|    total_timesteps              | 1052000      |\n| train/                          |              |\n|    approx_kl                    | 7.243102e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.81        |\n|    explained_variance           | 0.559        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 20           |\n|    n_updates                    | 524          |\n|    policy_gradient_loss         | 3e-05        |\n|    value_loss                   | 39.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 142          |\n|    water_produced               | 30.2         |\n--------------------------------------------------\nEval num_timesteps=1056000, episode_reward=26.40 +/- 52.80\nEpisode length: 326.00 +/- 50.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 326           |\n|    mean_reward                  | 26.4          |\n| time/                           |               |\n|    total_timesteps              | 1056000       |\n| train/                          |               |\n|    approx_kl                    | 0.00055517023 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.89         |\n|    explained_variance           | 0.57          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 31.5          |\n|    n_updates                    | 526           |\n|    policy_gradient_loss         | -0.00036      |\n|    value_loss                   | 70.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 142           |\n|    ice_dug                      | 136           |\n|    water_produced               | 29.8          |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 27.4     |\n| time/              |          |\n|    fps             | 852      |\n|    iterations      | 264      |\n|    time_elapsed    | 1238     |\n|    total_timesteps | 1056000  |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.8          |\n| time/                           |               |\n|    fps                          | 852           |\n|    iterations                   | 265           |\n|    time_elapsed                 | 1243          |\n|    total_timesteps              | 1060000       |\n| train/                          |               |\n|    approx_kl                    | 0.00027754993 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.64         |\n|    explained_variance           | 0.526         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 34            |\n|    n_updates                    | 528           |\n|    policy_gradient_loss         | -7.44e-05     |\n|    value_loss                   | 70.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 129           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 120           |\n|    water_produced               | 23.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.1         |\n| time/                           |              |\n|    fps                          | 852          |\n|    iterations                   | 266          |\n|    time_elapsed                 | 1247         |\n|    total_timesteps              | 1064000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002875963 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.93        |\n|    explained_variance           | 0.551        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 27.7         |\n|    n_updates                    | 530          |\n|    policy_gradient_loss         | -6.99e-05    |\n|    value_loss                   | 55.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 143          |\n|    ice_dug                      | 54           |\n|    water_produced               | 8.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.1         |\n| time/                           |              |\n|    fps                          | 852          |\n|    iterations                   | 267          |\n|    time_elapsed                 | 1252         |\n|    total_timesteps              | 1068000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004328352 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.75        |\n|    explained_variance           | 0.701        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.6         |\n|    n_updates                    | 532          |\n|    policy_gradient_loss         | -0.000385    |\n|    value_loss                   | 19.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 71           |\n|    water_produced               | 8.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.3         |\n| time/                           |              |\n|    fps                          | 852          |\n|    iterations                   | 268          |\n|    time_elapsed                 | 1257         |\n|    total_timesteps              | 1072000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006023676 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.87        |\n|    explained_variance           | 0.629        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22.1         |\n|    n_updates                    | 534          |\n|    policy_gradient_loss         | -0.000301    |\n|    value_loss                   | 27.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 144          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 144          |\n|    water_produced               | 26.3         |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 18.2       |\n| time/                           |            |\n|    fps                          | 852        |\n|    iterations                   | 269        |\n|    time_elapsed                 | 1261       |\n|    total_timesteps              | 1076000    |\n| train/                          |            |\n|    approx_kl                    | 0.00113333 |\n|    clip_fraction                | 0.000625   |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.81      |\n|    explained_variance           | 0.532      |\n|    learning_rate                | 0.0003     |\n|    loss                         | 26.5       |\n|    n_updates                    | 536        |\n|    policy_gradient_loss         | -0.000391  |\n|    value_loss                   | 51.8       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 143        |\n|    action_queue_updates_total   | 159        |\n|    ice_dug                      | 121        |\n|    water_produced               | 19.5       |\n------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.5         |\n| time/                           |              |\n|    fps                          | 852          |\n|    iterations                   | 270          |\n|    time_elapsed                 | 1266         |\n|    total_timesteps              | 1080000      |\n| train/                          |              |\n|    approx_kl                    | 0.0020851283 |\n|    clip_fraction                | 0.0035       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.9         |\n|    explained_variance           | 0.473        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21.1         |\n|    n_updates                    | 538          |\n|    policy_gradient_loss         | 0.000479     |\n|    value_loss                   | 49.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 68           |\n|    water_produced               | 15.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.7          |\n| time/                           |               |\n|    fps                          | 852           |\n|    iterations                   | 271           |\n|    time_elapsed                 | 1271          |\n|    total_timesteps              | 1084000       |\n| train/                          |               |\n|    approx_kl                    | 0.00051485084 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.92         |\n|    explained_variance           | 0.591         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.2          |\n|    n_updates                    | 540           |\n|    policy_gradient_loss         | 0.000279      |\n|    value_loss                   | 32.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 161           |\n|    ice_dug                      | 49            |\n|    water_produced               | 9.25          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.8         |\n| time/                           |              |\n|    fps                          | 852          |\n|    iterations                   | 272          |\n|    time_elapsed                 | 1276         |\n|    total_timesteps              | 1088000      |\n| train/                          |              |\n|    approx_kl                    | 0.0016538281 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.01        |\n|    explained_variance           | 0.593        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.26         |\n|    n_updates                    | 542          |\n|    policy_gradient_loss         | 0.000684     |\n|    value_loss                   | 21.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 103          |\n|    water_produced               | 18.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.2          |\n| time/                           |               |\n|    fps                          | 852           |\n|    iterations                   | 273           |\n|    time_elapsed                 | 1281          |\n|    total_timesteps              | 1092000       |\n| train/                          |               |\n|    approx_kl                    | 0.00017962238 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.02         |\n|    explained_variance           | 0.566         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 23.8          |\n|    n_updates                    | 544           |\n|    policy_gradient_loss         | 0.000107      |\n|    value_loss                   | 47.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 131           |\n|    water_produced               | 23.8          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 16.4        |\n| time/                           |             |\n|    fps                          | 852         |\n|    iterations                   | 274         |\n|    time_elapsed                 | 1286        |\n|    total_timesteps              | 1096000     |\n| train/                          |             |\n|    approx_kl                    | 0.000459477 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.85       |\n|    explained_variance           | 0.541       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 22.6        |\n|    n_updates                    | 546         |\n|    policy_gradient_loss         | 0.00017     |\n|    value_loss                   | 48.3        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 134         |\n|    action_queue_updates_total   | 153         |\n|    ice_dug                      | 51          |\n|    water_produced               | 10.8        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.5         |\n| time/                           |              |\n|    fps                          | 851          |\n|    iterations                   | 275          |\n|    time_elapsed                 | 1291         |\n|    total_timesteps              | 1100000      |\n| train/                          |              |\n|    approx_kl                    | 0.0014787544 |\n|    clip_fraction                | 0.000625     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.86        |\n|    explained_variance           | 0.71         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 7.43         |\n|    n_updates                    | 548          |\n|    policy_gradient_loss         | 0.000458     |\n|    value_loss                   | 15.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 85           |\n|    water_produced               | 16.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.3          |\n| time/                           |               |\n|    fps                          | 851           |\n|    iterations                   | 276           |\n|    time_elapsed                 | 1296          |\n|    total_timesteps              | 1104000       |\n| train/                          |               |\n|    approx_kl                    | 0.00017726843 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.96         |\n|    explained_variance           | 0.631         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.3          |\n|    n_updates                    | 550           |\n|    policy_gradient_loss         | 0.000169      |\n|    value_loss                   | 33.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 159           |\n|    ice_dug                      | 53            |\n|    water_produced               | 8             |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 14.7        |\n| time/                           |             |\n|    fps                          | 851         |\n|    iterations                   | 277         |\n|    time_elapsed                 | 1301        |\n|    total_timesteps              | 1108000     |\n| train/                          |             |\n|    approx_kl                    | 0.002192241 |\n|    clip_fraction                | 0.000875    |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.99       |\n|    explained_variance           | 0.706       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 12          |\n|    n_updates                    | 552         |\n|    policy_gradient_loss         | 0.000555    |\n|    value_loss                   | 18.7        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 134         |\n|    action_queue_updates_total   | 165         |\n|    ice_dug                      | 77          |\n|    water_produced               | 10.7        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.4          |\n| time/                           |               |\n|    fps                          | 851           |\n|    iterations                   | 278           |\n|    time_elapsed                 | 1306          |\n|    total_timesteps              | 1112000       |\n| train/                          |               |\n|    approx_kl                    | 0.00027118268 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.15         |\n|    explained_variance           | 0.655         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 9.21          |\n|    n_updates                    | 554           |\n|    policy_gradient_loss         | 0.000142      |\n|    value_loss                   | 21.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 125           |\n|    water_produced               | 27.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.2         |\n| time/                           |              |\n|    fps                          | 851          |\n|    iterations                   | 279          |\n|    time_elapsed                 | 1311         |\n|    total_timesteps              | 1116000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005089751 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.96        |\n|    explained_variance           | 0.581        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 28.1         |\n|    n_updates                    | 556          |\n|    policy_gradient_loss         | 0.000359     |\n|    value_loss                   | 57.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 52           |\n|    water_produced               | 9.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14.2         |\n| time/                           |              |\n|    fps                          | 850          |\n|    iterations                   | 280          |\n|    time_elapsed                 | 1316         |\n|    total_timesteps              | 1120000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006080299 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.05        |\n|    explained_variance           | 0.68         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.4         |\n|    n_updates                    | 558          |\n|    policy_gradient_loss         | 0.00016      |\n|    value_loss                   | 26.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 62           |\n|    water_produced               | 11.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.7          |\n| time/                           |               |\n|    fps                          | 850           |\n|    iterations                   | 281           |\n|    time_elapsed                 | 1320          |\n|    total_timesteps              | 1124000       |\n| train/                          |               |\n|    approx_kl                    | 0.00016689653 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.06         |\n|    explained_variance           | 0.726         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.3          |\n|    n_updates                    | 560           |\n|    policy_gradient_loss         | -6.36e-05     |\n|    value_loss                   | 20.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 159           |\n|    ice_dug                      | 80            |\n|    water_produced               | 15.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14.2         |\n| time/                           |              |\n|    fps                          | 850          |\n|    iterations                   | 282          |\n|    time_elapsed                 | 1325         |\n|    total_timesteps              | 1128000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007549894 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.02        |\n|    explained_variance           | 0.705        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15           |\n|    n_updates                    | 562          |\n|    policy_gradient_loss         | -0.000409    |\n|    value_loss                   | 27           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 164          |\n|    ice_dug                      | 17           |\n|    water_produced               | 3.75         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 10.9        |\n| time/                           |             |\n|    fps                          | 850         |\n|    iterations                   | 283         |\n|    time_elapsed                 | 1330        |\n|    total_timesteps              | 1132000     |\n| train/                          |             |\n|    approx_kl                    | 0.000624061 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.17       |\n|    explained_variance           | 0.792       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 4.12        |\n|    n_updates                    | 564         |\n|    policy_gradient_loss         | -0.000774   |\n|    value_loss                   | 8.38        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 130         |\n|    action_queue_updates_total   | 164         |\n|    ice_dug                      | 60          |\n|    water_produced               | 12          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 10.5         |\n| time/                           |              |\n|    fps                          | 850          |\n|    iterations                   | 284          |\n|    time_elapsed                 | 1335         |\n|    total_timesteps              | 1136000      |\n| train/                          |              |\n|    approx_kl                    | 0.0003749789 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.16        |\n|    explained_variance           | 0.693        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.8         |\n|    n_updates                    | 566          |\n|    policy_gradient_loss         | -0.000243    |\n|    value_loss                   | 31.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 42           |\n|    water_produced               | 7.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 9.92         |\n| time/                           |              |\n|    fps                          | 850          |\n|    iterations                   | 285          |\n|    time_elapsed                 | 1340         |\n|    total_timesteps              | 1140000      |\n| train/                          |              |\n|    approx_kl                    | 0.0017598458 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.16        |\n|    explained_variance           | 0.721        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.79         |\n|    n_updates                    | 568          |\n|    policy_gradient_loss         | 0.000238     |\n|    value_loss                   | 15.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 59           |\n|    water_produced               | 8.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.71          |\n| time/                           |               |\n|    fps                          | 850           |\n|    iterations                   | 286           |\n|    time_elapsed                 | 1345          |\n|    total_timesteps              | 1144000       |\n| train/                          |               |\n|    approx_kl                    | 0.00020297637 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.1          |\n|    explained_variance           | 0.754         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.99          |\n|    n_updates                    | 570           |\n|    policy_gradient_loss         | 0.000163      |\n|    value_loss                   | 15.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 169           |\n|    ice_dug                      | 2             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7.02         |\n| time/                           |              |\n|    fps                          | 849          |\n|    iterations                   | 287          |\n|    time_elapsed                 | 1350         |\n|    total_timesteps              | 1148000      |\n| train/                          |              |\n|    approx_kl                    | 0.0020938807 |\n|    clip_fraction                | 0.00413      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.2         |\n|    explained_variance           | 0.822        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.5          |\n|    n_updates                    | 572          |\n|    policy_gradient_loss         | 0.000942     |\n|    value_loss                   | 3.31         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 170          |\n|    ice_dug                      | 46           |\n|    water_produced               | 5            |\n--------------------------------------------------\nEval num_timesteps=1152000, episode_reward=1.08 +/- 2.16\nEpisode length: 302.00 +/- 2.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 302          |\n|    mean_reward                  | 1.08         |\n| time/                           |              |\n|    total_timesteps              | 1152000      |\n| train/                          |              |\n|    approx_kl                    | 0.0008510974 |\n|    clip_fraction                | 0.000875     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.22        |\n|    explained_variance           | 0.682        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.03         |\n|    n_updates                    | 574          |\n|    policy_gradient_loss         | 0.000453     |\n|    value_loss                   | 12           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 171          |\n|    ice_dug                      | 77           |\n|    water_produced               | 12.2         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 7.1      |\n| time/              |          |\n|    fps             | 847      |\n|    iterations      | 288      |\n|    time_elapsed    | 1358     |\n|    total_timesteps | 1152000  |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 8.72         |\n| time/                           |              |\n|    fps                          | 847          |\n|    iterations                   | 289          |\n|    time_elapsed                 | 1363         |\n|    total_timesteps              | 1156000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005210367 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.22        |\n|    explained_variance           | 0.715        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.5         |\n|    n_updates                    | 576          |\n|    policy_gradient_loss         | 2.57e-05     |\n|    value_loss                   | 21           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 77           |\n|    water_produced               | 15.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.8           |\n| time/                           |               |\n|    fps                          | 847           |\n|    iterations                   | 290           |\n|    time_elapsed                 | 1368          |\n|    total_timesteps              | 1160000       |\n| train/                          |               |\n|    approx_kl                    | 0.00044472553 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.22         |\n|    explained_variance           | 0.69          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.5          |\n|    n_updates                    | 578           |\n|    policy_gradient_loss         | 0.000183      |\n|    value_loss                   | 25.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 158           |\n|    ice_dug                      | 49            |\n|    water_produced               | 9             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.5          |\n| time/                           |               |\n|    fps                          | 847           |\n|    iterations                   | 291           |\n|    time_elapsed                 | 1373          |\n|    total_timesteps              | 1164000       |\n| train/                          |               |\n|    approx_kl                    | 0.00062468735 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.01         |\n|    explained_variance           | 0.74          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 9.14          |\n|    n_updates                    | 580           |\n|    policy_gradient_loss         | 0.000129      |\n|    value_loss                   | 16.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 159           |\n|    ice_dug                      | 75            |\n|    water_produced               | 12.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.4          |\n| time/                           |               |\n|    fps                          | 847           |\n|    iterations                   | 292           |\n|    time_elapsed                 | 1378          |\n|    total_timesteps              | 1168000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031951623 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.02         |\n|    explained_variance           | 0.632         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.1          |\n|    n_updates                    | 582           |\n|    policy_gradient_loss         | -0.000335     |\n|    value_loss                   | 23.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 170           |\n|    ice_dug                      | 20            |\n|    water_produced               | 4.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11            |\n| time/                           |               |\n|    fps                          | 847           |\n|    iterations                   | 293           |\n|    time_elapsed                 | 1383          |\n|    total_timesteps              | 1172000       |\n| train/                          |               |\n|    approx_kl                    | 0.00011024615 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.25         |\n|    explained_variance           | 0.78          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.8           |\n|    n_updates                    | 584           |\n|    policy_gradient_loss         | -0.000296     |\n|    value_loss                   | 8.43          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 43            |\n|    water_produced               | 10.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.2          |\n| time/                           |               |\n|    fps                          | 847           |\n|    iterations                   | 294           |\n|    time_elapsed                 | 1388          |\n|    total_timesteps              | 1176000       |\n| train/                          |               |\n|    approx_kl                    | 0.00043498678 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.22         |\n|    explained_variance           | 0.67          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.84          |\n|    n_updates                    | 586           |\n|    policy_gradient_loss         | 0.000312      |\n|    value_loss                   | 17.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 168           |\n|    ice_dug                      | 159           |\n|    water_produced               | 25.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.9         |\n| time/                           |              |\n|    fps                          | 846          |\n|    iterations                   | 295          |\n|    time_elapsed                 | 1393         |\n|    total_timesteps              | 1180000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010667145 |\n|    clip_fraction                | 0.000375     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.08        |\n|    explained_variance           | 0.683        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 25.5         |\n|    n_updates                    | 588          |\n|    policy_gradient_loss         | 0.000221     |\n|    value_loss                   | 55.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 103          |\n|    water_produced               | 22.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.6         |\n| time/                           |              |\n|    fps                          | 846          |\n|    iterations                   | 296          |\n|    time_elapsed                 | 1398         |\n|    total_timesteps              | 1184000      |\n| train/                          |              |\n|    approx_kl                    | 0.0008863056 |\n|    clip_fraction                | 0.00075      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.12        |\n|    explained_variance           | 0.693        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15.3         |\n|    n_updates                    | 590          |\n|    policy_gradient_loss         | -9.44e-05    |\n|    value_loss                   | 41.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 79           |\n|    water_produced               | 11           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.3          |\n| time/                           |               |\n|    fps                          | 846           |\n|    iterations                   | 297           |\n|    time_elapsed                 | 1404          |\n|    total_timesteps              | 1188000       |\n| train/                          |               |\n|    approx_kl                    | 0.00032515073 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.01         |\n|    explained_variance           | 0.795         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.3          |\n|    n_updates                    | 592           |\n|    policy_gradient_loss         | -0.000181     |\n|    value_loss                   | 20.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 171           |\n|    ice_dug                      | 104           |\n|    water_produced               | 22.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.7         |\n| time/                           |              |\n|    fps                          | 845          |\n|    iterations                   | 298          |\n|    time_elapsed                 | 1409         |\n|    total_timesteps              | 1192000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004766675 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.19        |\n|    explained_variance           | 0.684        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 29.4         |\n|    n_updates                    | 594          |\n|    policy_gradient_loss         | 9.84e-05     |\n|    value_loss                   | 59.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 115          |\n|    action_queue_updates_total   | 155          |\n|    ice_dug                      | 34           |\n|    water_produced               | 7.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.3          |\n| time/                           |               |\n|    fps                          | 845           |\n|    iterations                   | 299           |\n|    time_elapsed                 | 1413          |\n|    total_timesteps              | 1196000       |\n| train/                          |               |\n|    approx_kl                    | 0.00021486692 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.05         |\n|    explained_variance           | 0.82          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 9.98          |\n|    n_updates                    | 596           |\n|    policy_gradient_loss         | -3.63e-05     |\n|    value_loss                   | 18.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 169           |\n|    ice_dug                      | 8             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.3         |\n| time/                           |              |\n|    fps                          | 845          |\n|    iterations                   | 300          |\n|    time_elapsed                 | 1418         |\n|    total_timesteps              | 1200000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004866078 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.23        |\n|    explained_variance           | 0.86         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.61         |\n|    n_updates                    | 598          |\n|    policy_gradient_loss         | -0.000574    |\n|    value_loss                   | 4.02         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 120          |\n|    water_produced               | 22.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 12.5          |\n| time/                           |               |\n|    fps                          | 845           |\n|    iterations                   | 301           |\n|    time_elapsed                 | 1423          |\n|    total_timesteps              | 1204000       |\n| train/                          |               |\n|    approx_kl                    | 0.00045920373 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.96         |\n|    explained_variance           | 0.687         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 22.6          |\n|    n_updates                    | 600           |\n|    policy_gradient_loss         | -0.000318     |\n|    value_loss                   | 49.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 162           |\n|    ice_dug                      | 30            |\n|    water_produced               | 7.25          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 8.25         |\n| time/                           |              |\n|    fps                          | 845          |\n|    iterations                   | 302          |\n|    time_elapsed                 | 1428         |\n|    total_timesteps              | 1208000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005177533 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.07        |\n|    explained_variance           | 0.764        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.45         |\n|    n_updates                    | 602          |\n|    policy_gradient_loss         | -0.000422    |\n|    value_loss                   | 18.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 168          |\n|    ice_dug                      | 10           |\n|    water_produced               | 2            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 7.44          |\n| time/                           |               |\n|    fps                          | 845           |\n|    iterations                   | 303           |\n|    time_elapsed                 | 1433          |\n|    total_timesteps              | 1212000       |\n| train/                          |               |\n|    approx_kl                    | 0.00035707653 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.2          |\n|    explained_variance           | 0.845         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.43          |\n|    n_updates                    | 604           |\n|    policy_gradient_loss         | -4.12e-05     |\n|    value_loss                   | 6.32          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 168           |\n|    ice_dug                      | 26            |\n|    water_produced               | 3.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 12.4         |\n| time/                           |              |\n|    fps                          | 845          |\n|    iterations                   | 304          |\n|    time_elapsed                 | 1437         |\n|    total_timesteps              | 1216000      |\n| train/                          |              |\n|    approx_kl                    | 9.287884e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.2         |\n|    explained_variance           | 0.824        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 3.74         |\n|    n_updates                    | 606          |\n|    policy_gradient_loss         | -0.000353    |\n|    value_loss                   | 7.66         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 134          |\n|    water_produced               | 23.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.3          |\n| time/                           |               |\n|    fps                          | 845           |\n|    iterations                   | 305           |\n|    time_elapsed                 | 1442          |\n|    total_timesteps              | 1220000       |\n| train/                          |               |\n|    approx_kl                    | 0.00015605132 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.04         |\n|    explained_variance           | 0.596         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 34.3          |\n|    n_updates                    | 608           |\n|    policy_gradient_loss         | -0.000346     |\n|    value_loss                   | 75            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 169           |\n|    ice_dug                      | 53            |\n|    water_produced               | 12.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.8         |\n| time/                           |              |\n|    fps                          | 845          |\n|    iterations                   | 306          |\n|    time_elapsed                 | 1448         |\n|    total_timesteps              | 1224000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002167614 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.19        |\n|    explained_variance           | 0.684        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 16.4         |\n|    n_updates                    | 610          |\n|    policy_gradient_loss         | 0.000101     |\n|    value_loss                   | 30.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 170          |\n|    ice_dug                      | 97           |\n|    water_produced               | 24.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.7         |\n| time/                           |              |\n|    fps                          | 845          |\n|    iterations                   | 307          |\n|    time_elapsed                 | 1452         |\n|    total_timesteps              | 1228000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002460958 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.2         |\n|    explained_variance           | 0.631        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 26.6         |\n|    n_updates                    | 612          |\n|    policy_gradient_loss         | 0.000139     |\n|    value_loss                   | 66.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 144          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 86           |\n|    water_produced               | 16           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.8          |\n| time/                           |               |\n|    fps                          | 845           |\n|    iterations                   | 308           |\n|    time_elapsed                 | 1457          |\n|    total_timesteps              | 1232000       |\n| train/                          |               |\n|    approx_kl                    | 0.00046287593 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.04         |\n|    explained_variance           | 0.68          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 12.7          |\n|    n_updates                    | 614           |\n|    policy_gradient_loss         | 0.000273      |\n|    value_loss                   | 31            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 167           |\n|    ice_dug                      | 60            |\n|    water_produced               | 13.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.3         |\n| time/                           |              |\n|    fps                          | 844          |\n|    iterations                   | 309          |\n|    time_elapsed                 | 1462         |\n|    total_timesteps              | 1236000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006344596 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.16        |\n|    explained_variance           | 0.655        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 33.3         |\n|    n_updates                    | 616          |\n|    policy_gradient_loss         | -0.000188    |\n|    value_loss                   | 44.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 127          |\n|    action_queue_updates_total   | 170          |\n|    ice_dug                      | 71           |\n|    water_produced               | 17           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.2         |\n| time/                           |              |\n|    fps                          | 844          |\n|    iterations                   | 310          |\n|    time_elapsed                 | 1467         |\n|    total_timesteps              | 1240000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002316423 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.2         |\n|    explained_variance           | 0.739        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.85         |\n|    n_updates                    | 618          |\n|    policy_gradient_loss         | 7.57e-06     |\n|    value_loss                   | 30.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 165          |\n|    ice_dug                      | 115          |\n|    water_produced               | 16.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18           |\n| time/                           |              |\n|    fps                          | 844          |\n|    iterations                   | 311          |\n|    time_elapsed                 | 1472         |\n|    total_timesteps              | 1244000      |\n| train/                          |              |\n|    approx_kl                    | 0.0009554931 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.09        |\n|    explained_variance           | 0.669        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22.2         |\n|    n_updates                    | 620          |\n|    policy_gradient_loss         | 0.000797     |\n|    value_loss                   | 44.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 164          |\n|    ice_dug                      | 103          |\n|    water_produced               | 23           |\n--------------------------------------------------\nEval num_timesteps=1248000, episode_reward=1.88 +/- 3.76\nEpisode length: 301.00 +/- 0.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 301           |\n|    mean_reward                  | 1.88          |\n| time/                           |               |\n|    total_timesteps              | 1248000       |\n| train/                          |               |\n|    approx_kl                    | 0.00045636072 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.06         |\n|    explained_variance           | 0.676         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 35.6          |\n|    n_updates                    | 622           |\n|    policy_gradient_loss         | -0.000332     |\n|    value_loss                   | 53.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 21            |\n|    water_produced               | 2.25          |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 15.1     |\n| time/              |          |\n|    fps             | 843      |\n|    iterations      | 312      |\n|    time_elapsed    | 1480     |\n|    total_timesteps | 1248000  |\n---------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 16.8        |\n| time/                           |             |\n|    fps                          | 842         |\n|    iterations                   | 313         |\n|    time_elapsed                 | 1485        |\n|    total_timesteps              | 1252000     |\n| train/                          |             |\n|    approx_kl                    | 0.002095736 |\n|    clip_fraction                | 0.00188     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2          |\n|    explained_variance           | 0.841       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 4.02        |\n|    n_updates                    | 624         |\n|    policy_gradient_loss         | -0.000134   |\n|    value_loss                   | 9           |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 135         |\n|    action_queue_updates_total   | 159         |\n|    ice_dug                      | 136         |\n|    water_produced               | 21          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.2         |\n| time/                           |              |\n|    fps                          | 842          |\n|    iterations                   | 314          |\n|    time_elapsed                 | 1490         |\n|    total_timesteps              | 1256000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011950241 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.98        |\n|    explained_variance           | 0.691        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21.5         |\n|    n_updates                    | 626          |\n|    policy_gradient_loss         | -0.000476    |\n|    value_loss                   | 52.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 175          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.5          |\n| time/                           |               |\n|    fps                          | 842           |\n|    iterations                   | 315           |\n|    time_elapsed                 | 1495          |\n|    total_timesteps              | 1260000       |\n| train/                          |               |\n|    approx_kl                    | 0.00046619904 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.32         |\n|    explained_variance           | 0.849         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.98          |\n|    n_updates                    | 628           |\n|    policy_gradient_loss         | -0.000144     |\n|    value_loss                   | 2.52          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 16            |\n|    water_produced               | 3.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.86          |\n| time/                           |               |\n|    fps                          | 842           |\n|    iterations                   | 316           |\n|    time_elapsed                 | 1500          |\n|    total_timesteps              | 1264000       |\n| train/                          |               |\n|    approx_kl                    | 0.00029291006 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.28         |\n|    explained_variance           | 0.828         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.18          |\n|    n_updates                    | 630           |\n|    policy_gradient_loss         | -0.000677     |\n|    value_loss                   | 6.11          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 171           |\n|    ice_dug                      | 5             |\n|    water_produced               | 1             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7.63         |\n| time/                           |              |\n|    fps                          | 842          |\n|    iterations                   | 317          |\n|    time_elapsed                 | 1505         |\n|    total_timesteps              | 1268000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011448052 |\n|    clip_fraction                | 0.00225      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.23        |\n|    explained_variance           | 0.843        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.62         |\n|    n_updates                    | 632          |\n|    policy_gradient_loss         | 0.000466     |\n|    value_loss                   | 3.45         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 176          |\n|    ice_dug                      | 56           |\n|    water_produced               | 10.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.98          |\n| time/                           |               |\n|    fps                          | 842           |\n|    iterations                   | 318           |\n|    time_elapsed                 | 1509          |\n|    total_timesteps              | 1272000       |\n| train/                          |               |\n|    approx_kl                    | 0.00020211484 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.23         |\n|    explained_variance           | 0.684         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 12.6          |\n|    n_updates                    | 634           |\n|    policy_gradient_loss         | -0.000292     |\n|    value_loss                   | 24            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 36            |\n|    water_produced               | 8.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.6           |\n| time/                           |               |\n|    fps                          | 842           |\n|    iterations                   | 319           |\n|    time_elapsed                 | 1514          |\n|    total_timesteps              | 1276000       |\n| train/                          |               |\n|    approx_kl                    | 0.00010213704 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.19         |\n|    explained_variance           | 0.706         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.6          |\n|    n_updates                    | 636           |\n|    policy_gradient_loss         | -0.000202     |\n|    value_loss                   | 21.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 38            |\n|    water_produced               | 7.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.92          |\n| time/                           |               |\n|    fps                          | 842           |\n|    iterations                   | 320           |\n|    time_elapsed                 | 1520          |\n|    total_timesteps              | 1280000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031237624 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.22         |\n|    explained_variance           | 0.772         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.48          |\n|    n_updates                    | 638           |\n|    policy_gradient_loss         | 0.000327      |\n|    value_loss                   | 11            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 5.71         |\n| time/                           |              |\n|    fps                          | 841          |\n|    iterations                   | 321          |\n|    time_elapsed                 | 1525         |\n|    total_timesteps              | 1284000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011285143 |\n|    clip_fraction                | 0.000625     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.17        |\n|    explained_variance           | 0.861        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.23         |\n|    n_updates                    | 640          |\n|    policy_gradient_loss         | 0.000108     |\n|    value_loss                   | 2.8          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 5.71         |\n| time/                           |              |\n|    fps                          | 841          |\n|    iterations                   | 322          |\n|    time_elapsed                 | 1530         |\n|    total_timesteps              | 1288000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006486765 |\n|    clip_fraction                | 0.0015       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.34        |\n|    explained_variance           | 0.85         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.279        |\n|    n_updates                    | 642          |\n|    policy_gradient_loss         | 0.000494     |\n|    value_loss                   | 0.772        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 179          |\n|    ice_dug                      | 58           |\n|    water_produced               | 10.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 5.09         |\n| time/                           |              |\n|    fps                          | 841          |\n|    iterations                   | 323          |\n|    time_elapsed                 | 1535         |\n|    total_timesteps              | 1292000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002622258 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.25        |\n|    explained_variance           | 0.713        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.06         |\n|    n_updates                    | 644          |\n|    policy_gradient_loss         | -0.000331    |\n|    value_loss                   | 16.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 25           |\n|    water_produced               | 5.75         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.19          |\n| time/                           |               |\n|    fps                          | 841           |\n|    iterations                   | 324           |\n|    time_elapsed                 | 1540          |\n|    total_timesteps              | 1296000       |\n| train/                          |               |\n|    approx_kl                    | 0.00036467708 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.688         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.82          |\n|    n_updates                    | 646           |\n|    policy_gradient_loss         | 0.000228      |\n|    value_loss                   | 15            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 174           |\n|    ice_dug                      | 14            |\n|    water_produced               | 3.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 4.67         |\n| time/                           |              |\n|    fps                          | 841          |\n|    iterations                   | 325          |\n|    time_elapsed                 | 1545         |\n|    total_timesteps              | 1300000      |\n| train/                          |              |\n|    approx_kl                    | 0.0012498202 |\n|    clip_fraction                | 0.00188      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.27        |\n|    explained_variance           | 0.745        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 5.83         |\n|    n_updates                    | 648          |\n|    policy_gradient_loss         | 0.000499     |\n|    value_loss                   | 7.78         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 176          |\n|    ice_dug                      | 12           |\n|    water_produced               | 2.25         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.79          |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 326           |\n|    time_elapsed                 | 1550          |\n|    total_timesteps              | 1304000       |\n| train/                          |               |\n|    approx_kl                    | 0.00040356535 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.33         |\n|    explained_variance           | 0.785         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.41          |\n|    n_updates                    | 650           |\n|    policy_gradient_loss         | -0.000147     |\n|    value_loss                   | 2.61          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 174           |\n|    ice_dug                      | 61            |\n|    water_produced               | 10            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.94          |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 327           |\n|    time_elapsed                 | 1555          |\n|    total_timesteps              | 1308000       |\n| train/                          |               |\n|    approx_kl                    | 0.00036326153 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.25         |\n|    explained_variance           | 0.609         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.5          |\n|    n_updates                    | 652           |\n|    policy_gradient_loss         | -0.00071      |\n|    value_loss                   | 23            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 8             |\n|    water_produced               | 2             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.37          |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 328           |\n|    time_elapsed                 | 1560          |\n|    total_timesteps              | 1312000       |\n| train/                          |               |\n|    approx_kl                    | 0.00085185573 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.27         |\n|    explained_variance           | 0.75          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.36          |\n|    n_updates                    | 654           |\n|    policy_gradient_loss         | -0.000123     |\n|    value_loss                   | 4.03          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 38            |\n|    water_produced               | 7.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.69          |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 329           |\n|    time_elapsed                 | 1565          |\n|    total_timesteps              | 1316000       |\n| train/                          |               |\n|    approx_kl                    | 0.00029530743 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.31         |\n|    explained_variance           | 0.666         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.53          |\n|    n_updates                    | 656           |\n|    policy_gradient_loss         | 0.000131      |\n|    value_loss                   | 19.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 1             |\n|    water_produced               | 0.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.1           |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 330           |\n|    time_elapsed                 | 1570          |\n|    total_timesteps              | 1320000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031200075 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.27         |\n|    explained_variance           | 0.901         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.397         |\n|    n_updates                    | 658           |\n|    policy_gradient_loss         | 0.00057       |\n|    value_loss                   | 1.23          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 94            |\n|    water_produced               | 18.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 6.09         |\n| time/                           |              |\n|    fps                          | 840          |\n|    iterations                   | 331          |\n|    time_elapsed                 | 1575         |\n|    total_timesteps              | 1324000      |\n| train/                          |              |\n|    approx_kl                    | 0.0013709879 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.26        |\n|    explained_variance           | 0.67         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 20.4         |\n|    n_updates                    | 660          |\n|    policy_gradient_loss         | -0.000149    |\n|    value_loss                   | 41.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 4            |\n|    water_produced               | 0.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 7.66          |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 332           |\n|    time_elapsed                 | 1580          |\n|    total_timesteps              | 1328000       |\n| train/                          |               |\n|    approx_kl                    | 0.00023335408 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.33         |\n|    explained_variance           | 0.79          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.576         |\n|    n_updates                    | 662           |\n|    policy_gradient_loss         | 0.000437      |\n|    value_loss                   | 1.34          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 178           |\n|    ice_dug                      | 41            |\n|    water_produced               | 9.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.24          |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 333           |\n|    time_elapsed                 | 1585          |\n|    total_timesteps              | 1332000       |\n| train/                          |               |\n|    approx_kl                    | 0.00015113055 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.23         |\n|    explained_variance           | 0.648         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 8.93          |\n|    n_updates                    | 664           |\n|    policy_gradient_loss         | 1.84e-05      |\n|    value_loss                   | 22.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 178           |\n|    ice_dug                      | 7             |\n|    water_produced               | 1             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 7.13          |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 334           |\n|    time_elapsed                 | 1589          |\n|    total_timesteps              | 1336000       |\n| train/                          |               |\n|    approx_kl                    | 0.00029954236 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.837         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.535         |\n|    n_updates                    | 666           |\n|    policy_gradient_loss         | -0.000175     |\n|    value_loss                   | 1.15          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 20            |\n|    water_produced               | 4.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.35          |\n| time/                           |               |\n|    fps                          | 840           |\n|    iterations                   | 335           |\n|    time_elapsed                 | 1594          |\n|    total_timesteps              | 1340000       |\n| train/                          |               |\n|    approx_kl                    | 0.00051399204 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.27         |\n|    explained_variance           | 0.714         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.04          |\n|    n_updates                    | 668           |\n|    policy_gradient_loss         | 0.000122      |\n|    value_loss                   | 6.67          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 150           |\n|    action_queue_updates_total   | 175           |\n|    ice_dug                      | 29            |\n|    water_produced               | 5.25          |\n---------------------------------------------------\nEval num_timesteps=1344000, episode_reward=0.00 +/- 0.00\nEpisode length: 301.00 +/- 0.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 301           |\n|    mean_reward                  | 0             |\n| time/                           |               |\n|    total_timesteps              | 1344000       |\n| train/                          |               |\n|    approx_kl                    | 0.00013514272 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.28         |\n|    explained_variance           | 0.712         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.1           |\n|    n_updates                    | 670           |\n|    policy_gradient_loss         | -6.45e-05     |\n|    value_loss                   | 7.63          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 175           |\n|    ice_dug                      | 11            |\n|    water_produced               | 1.5           |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 4.57     |\n| time/              |          |\n|    fps             | 838      |\n|    iterations      | 336      |\n|    time_elapsed    | 1602     |\n|    total_timesteps | 1344000  |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 2.96         |\n| time/                           |              |\n|    fps                          | 838          |\n|    iterations                   | 337          |\n|    time_elapsed                 | 1607         |\n|    total_timesteps              | 1348000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007206025 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.27        |\n|    explained_variance           | 0.771        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.27         |\n|    n_updates                    | 672          |\n|    policy_gradient_loss         | 0.00075      |\n|    value_loss                   | 2.98         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 151          |\n|    action_queue_updates_total   | 176          |\n|    ice_dug                      | 11           |\n|    water_produced               | 1.75         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 2.74          |\n| time/                           |               |\n|    fps                          | 838           |\n|    iterations                   | 338           |\n|    time_elapsed                 | 1612          |\n|    total_timesteps              | 1352000       |\n| train/                          |               |\n|    approx_kl                    | 0.00046364925 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.33         |\n|    explained_variance           | 0.706         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.01          |\n|    n_updates                    | 674           |\n|    policy_gradient_loss         | 0.000686      |\n|    value_loss                   | 2.67          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 148           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 1.97          |\n| time/                           |               |\n|    fps                          | 838           |\n|    iterations                   | 339           |\n|    time_elapsed                 | 1617          |\n|    total_timesteps              | 1356000       |\n| train/                          |               |\n|    approx_kl                    | 0.00054471765 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.34         |\n|    explained_variance           | 0.786         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.183         |\n|    n_updates                    | 676           |\n|    policy_gradient_loss         | -0.000361     |\n|    value_loss                   | 0.426         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 157           |\n|    action_queue_updates_total   | 178           |\n|    ice_dug                      | 8             |\n|    water_produced               | 0.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.75          |\n| time/                           |               |\n|    fps                          | 838           |\n|    iterations                   | 340           |\n|    time_elapsed                 | 1621          |\n|    total_timesteps              | 1360000       |\n| train/                          |               |\n|    approx_kl                    | 0.00054264773 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.34         |\n|    explained_variance           | 0.456         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.568         |\n|    n_updates                    | 678           |\n|    policy_gradient_loss         | -2.82e-05     |\n|    value_loss                   | 0.885         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 159           |\n|    action_queue_updates_total   | 174           |\n|    ice_dug                      | 95            |\n|    water_produced               | 18.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7            |\n| time/                           |              |\n|    fps                          | 838          |\n|    iterations                   | 341          |\n|    time_elapsed                 | 1626         |\n|    total_timesteps              | 1364000      |\n| train/                          |              |\n|    approx_kl                    | 0.0038056516 |\n|    clip_fraction                | 0.0174       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.28        |\n|    explained_variance           | 0.579        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.9         |\n|    n_updates                    | 680          |\n|    policy_gradient_loss         | -0.00272     |\n|    value_loss                   | 30.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 153          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 63           |\n|    water_produced               | 12.3         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.79          |\n| time/                           |               |\n|    fps                          | 838           |\n|    iterations                   | 342           |\n|    time_elapsed                 | 1631          |\n|    total_timesteps              | 1368000       |\n| train/                          |               |\n|    approx_kl                    | 0.00065781863 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.22         |\n|    explained_variance           | 0.547         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 12.9          |\n|    n_updates                    | 682           |\n|    policy_gradient_loss         | -0.00121      |\n|    value_loss                   | 29.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 155           |\n|    action_queue_updates_total   | 174           |\n|    ice_dug                      | 4             |\n|    water_produced               | 0.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7.12         |\n| time/                           |              |\n|    fps                          | 838          |\n|    iterations                   | 343          |\n|    time_elapsed                 | 1636         |\n|    total_timesteps              | 1372000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011751932 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.3         |\n|    explained_variance           | 0.753        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.363        |\n|    n_updates                    | 684          |\n|    policy_gradient_loss         | -6.03e-05    |\n|    value_loss                   | 0.897        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 159          |\n|    action_queue_updates_total   | 172          |\n|    ice_dug                      | 15           |\n|    water_produced               | 1.5          |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 7.11       |\n| time/                           |            |\n|    fps                          | 838        |\n|    iterations                   | 344        |\n|    time_elapsed                 | 1641       |\n|    total_timesteps              | 1376000    |\n| train/                          |            |\n|    approx_kl                    | 0.00062722 |\n|    clip_fraction                | 0          |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -2.28      |\n|    explained_variance           | 0.51       |\n|    learning_rate                | 0.0003     |\n|    loss                         | 1.68       |\n|    n_updates                    | 686        |\n|    policy_gradient_loss         | 0.000408   |\n|    value_loss                   | 2.47       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 150        |\n|    action_queue_updates_total   | 172        |\n|    ice_dug                      | 4          |\n|    water_produced               | 0.75       |\n------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 3.44          |\n| time/                           |               |\n|    fps                          | 838           |\n|    iterations                   | 345           |\n|    time_elapsed                 | 1646          |\n|    total_timesteps              | 1380000       |\n| train/                          |               |\n|    approx_kl                    | 0.00057513604 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.758         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.232         |\n|    n_updates                    | 688           |\n|    policy_gradient_loss         | 0.00033       |\n|    value_loss                   | 0.693         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 160           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 11            |\n|    water_produced               | 1             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 2.13          |\n| time/                           |               |\n|    fps                          | 838           |\n|    iterations                   | 346           |\n|    time_elapsed                 | 1650          |\n|    total_timesteps              | 1384000       |\n| train/                          |               |\n|    approx_kl                    | 0.00044133878 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.638         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.76          |\n|    n_updates                    | 690           |\n|    policy_gradient_loss         | 0.000335      |\n|    value_loss                   | 1.4           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 160           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 32            |\n|    water_produced               | 6             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 2             |\n| time/                           |               |\n|    fps                          | 838           |\n|    iterations                   | 347           |\n|    time_elapsed                 | 1655          |\n|    total_timesteps              | 1388000       |\n| train/                          |               |\n|    approx_kl                    | 0.00040451757 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.724         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.55          |\n|    n_updates                    | 692           |\n|    policy_gradient_loss         | -0.00046      |\n|    value_loss                   | 7.14          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 161           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 13            |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 2.39          |\n| time/                           |               |\n|    fps                          | 838           |\n|    iterations                   | 348           |\n|    time_elapsed                 | 1660          |\n|    total_timesteps              | 1392000       |\n| train/                          |               |\n|    approx_kl                    | 0.00014301736 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.32         |\n|    explained_variance           | 0.635         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2             |\n|    n_updates                    | 694           |\n|    policy_gradient_loss         | -0.000138     |\n|    value_loss                   | 2.84          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 155           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 37            |\n|    water_produced               | 3.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 3.02          |\n| time/                           |               |\n|    fps                          | 837           |\n|    iterations                   | 349           |\n|    time_elapsed                 | 1666          |\n|    total_timesteps              | 1396000       |\n| train/                          |               |\n|    approx_kl                    | 0.00038259962 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.28         |\n|    explained_variance           | 0.633         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.54          |\n|    n_updates                    | 696           |\n|    policy_gradient_loss         | -0.00137      |\n|    value_loss                   | 6.75          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 152           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 40            |\n|    water_produced               | 3.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.15          |\n| time/                           |               |\n|    fps                          | 837           |\n|    iterations                   | 350           |\n|    time_elapsed                 | 1671          |\n|    total_timesteps              | 1400000       |\n| train/                          |               |\n|    approx_kl                    | 0.00021880404 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.24         |\n|    explained_variance           | 0.683         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.58          |\n|    n_updates                    | 698           |\n|    policy_gradient_loss         | -0.000548     |\n|    value_loss                   | 9.2           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 158           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 29            |\n|    water_produced               | 6.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.19          |\n| time/                           |               |\n|    fps                          | 837           |\n|    iterations                   | 351           |\n|    time_elapsed                 | 1675          |\n|    total_timesteps              | 1404000       |\n| train/                          |               |\n|    approx_kl                    | 0.00025216496 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.25         |\n|    explained_variance           | 0.586         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.37          |\n|    n_updates                    | 700           |\n|    policy_gradient_loss         | -0.000681     |\n|    value_loss                   | 18.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 159           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 74            |\n|    water_produced               | 15.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 11.2         |\n| time/                           |              |\n|    fps                          | 837          |\n|    iterations                   | 352          |\n|    time_elapsed                 | 1680         |\n|    total_timesteps              | 1408000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010115396 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.23        |\n|    explained_variance           | 0.577        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 17.8         |\n|    n_updates                    | 702          |\n|    policy_gradient_loss         | -0.000462    |\n|    value_loss                   | 49.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 167          |\n|    action_queue_updates_total   | 172          |\n|    ice_dug                      | 114          |\n|    water_produced               | 24           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.3          |\n| time/                           |               |\n|    fps                          | 837           |\n|    iterations                   | 353           |\n|    time_elapsed                 | 1685          |\n|    total_timesteps              | 1412000       |\n| train/                          |               |\n|    approx_kl                    | 0.00075706496 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.14         |\n|    explained_variance           | 0.49          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 69.6          |\n|    n_updates                    | 704           |\n|    policy_gradient_loss         | -2.89e-05     |\n|    value_loss                   | 94.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 155           |\n|    action_queue_updates_total   | 169           |\n|    ice_dug                      | 170           |\n|    water_produced               | 27.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.7         |\n| time/                           |              |\n|    fps                          | 837          |\n|    iterations                   | 354          |\n|    time_elapsed                 | 1690         |\n|    total_timesteps              | 1416000      |\n| train/                          |              |\n|    approx_kl                    | 0.0032165137 |\n|    clip_fraction                | 0.0149       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.09        |\n|    explained_variance           | 0.565        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 34.9         |\n|    n_updates                    | 706          |\n|    policy_gradient_loss         | -0.000829    |\n|    value_loss                   | 79.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 164          |\n|    action_queue_updates_total   | 168          |\n|    ice_dug                      | 225          |\n|    water_produced               | 38.8         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 28.4        |\n| time/                           |             |\n|    fps                          | 837         |\n|    iterations                   | 355         |\n|    time_elapsed                 | 1695        |\n|    total_timesteps              | 1420000     |\n| train/                          |             |\n|    approx_kl                    | 0.005030956 |\n|    clip_fraction                | 0.0268      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.96       |\n|    explained_variance           | 0.577       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 38.1        |\n|    n_updates                    | 708         |\n|    policy_gradient_loss         | -0.00144    |\n|    value_loss                   | 72.9        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 147         |\n|    action_queue_updates_total   | 161         |\n|    ice_dug                      | 206         |\n|    water_produced               | 28.2        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 42.2        |\n| time/                           |             |\n|    fps                          | 837         |\n|    iterations                   | 356         |\n|    time_elapsed                 | 1700        |\n|    total_timesteps              | 1424000     |\n| train/                          |             |\n|    approx_kl                    | 0.004134828 |\n|    clip_fraction                | 0.0155      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.93       |\n|    explained_variance           | 0.592       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 48.7        |\n|    n_updates                    | 710         |\n|    policy_gradient_loss         | -0.000834   |\n|    value_loss                   | 73.2        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 146         |\n|    action_queue_updates_total   | 162         |\n|    ice_dug                      | 412         |\n|    water_produced               | 81.2        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 46.9         |\n| time/                           |              |\n|    fps                          | 837          |\n|    iterations                   | 357          |\n|    time_elapsed                 | 1705         |\n|    total_timesteps              | 1428000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011539629 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.83        |\n|    explained_variance           | 0.524        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 126          |\n|    n_updates                    | 712          |\n|    policy_gradient_loss         | -0.000592    |\n|    value_loss                   | 331          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 253          |\n|    water_produced               | 46           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 49           |\n| time/                           |              |\n|    fps                          | 836          |\n|    iterations                   | 358          |\n|    time_elapsed                 | 1710         |\n|    total_timesteps              | 1432000      |\n| train/                          |              |\n|    approx_kl                    | 0.0038175047 |\n|    clip_fraction                | 0.0161       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.69        |\n|    explained_variance           | 0.502        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 102          |\n|    n_updates                    | 714          |\n|    policy_gradient_loss         | -0.00116     |\n|    value_loss                   | 161          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 285          |\n|    water_produced               | 37           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 49.8          |\n| time/                           |               |\n|    fps                          | 836           |\n|    iterations                   | 359           |\n|    time_elapsed                 | 1715          |\n|    total_timesteps              | 1436000       |\n| train/                          |               |\n|    approx_kl                    | 0.00055099506 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.555         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 65.4          |\n|    n_updates                    | 716           |\n|    policy_gradient_loss         | -0.000637     |\n|    value_loss                   | 106           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 143           |\n|    ice_dug                      | 209           |\n|    water_produced               | 42.7          |\n---------------------------------------------------\nEval num_timesteps=1440000, episode_reward=100.96 +/- 131.28\nEpisode length: 398.00 +/- 126.24\n-------------------------------------------------\n| eval/                           |             |\n|    mean_ep_length               | 398         |\n|    mean_reward                  | 101         |\n| time/                           |             |\n|    total_timesteps              | 1440000     |\n| train/                          |             |\n|    approx_kl                    | 0.001383229 |\n|    clip_fraction                | 0.00075     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.6        |\n|    explained_variance           | 0.518       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 44.7        |\n|    n_updates                    | 718         |\n|    policy_gradient_loss         | -0.000926   |\n|    value_loss                   | 101         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 141         |\n|    action_queue_updates_total   | 151         |\n|    ice_dug                      | 185         |\n|    water_produced               | 30.8        |\n-------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 50.2     |\n| time/              |          |\n|    fps             | 835      |\n|    iterations      | 360      |\n|    time_elapsed    | 1723     |\n|    total_timesteps | 1440000  |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 51.2          |\n| time/                           |               |\n|    fps                          | 835           |\n|    iterations                   | 361           |\n|    time_elapsed                 | 1728          |\n|    total_timesteps              | 1444000       |\n| train/                          |               |\n|    approx_kl                    | 0.00087729207 |\n|    clip_fraction                | 0.00363       |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.76         |\n|    explained_variance           | 0.538         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 51            |\n|    n_updates                    | 720           |\n|    policy_gradient_loss         | -0.000333     |\n|    value_loss                   | 92.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 151           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 411           |\n|    water_produced               | 86.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 55.3         |\n| time/                           |              |\n|    fps                          | 835          |\n|    iterations                   | 362          |\n|    time_elapsed                 | 1733         |\n|    total_timesteps              | 1448000      |\n| train/                          |              |\n|    approx_kl                    | 0.0047625764 |\n|    clip_fraction                | 0.0252       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.66        |\n|    explained_variance           | 0.536        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 138          |\n|    n_updates                    | 722          |\n|    policy_gradient_loss         | 0.000488     |\n|    value_loss                   | 244          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 338          |\n|    water_produced               | 65.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 60.7         |\n| time/                           |              |\n|    fps                          | 835          |\n|    iterations                   | 363          |\n|    time_elapsed                 | 1738         |\n|    total_timesteps              | 1452000      |\n| train/                          |              |\n|    approx_kl                    | 0.0066222725 |\n|    clip_fraction                | 0.0409       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.65        |\n|    explained_variance           | 0.467        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 130          |\n|    n_updates                    | 724          |\n|    policy_gradient_loss         | -0.000971    |\n|    value_loss                   | 239          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 123          |\n|    action_queue_updates_total   | 127          |\n|    ice_dug                      | 289          |\n|    water_produced               | 64           |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 68.9        |\n| time/                           |             |\n|    fps                          | 835         |\n|    iterations                   | 364         |\n|    time_elapsed                 | 1742        |\n|    total_timesteps              | 1456000     |\n| train/                          |             |\n|    approx_kl                    | 0.001119815 |\n|    clip_fraction                | 0.000625    |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.3        |\n|    explained_variance           | 0.451       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 108         |\n|    n_updates                    | 726         |\n|    policy_gradient_loss         | 0.000359    |\n|    value_loss                   | 207         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 137         |\n|    action_queue_updates_total   | 148         |\n|    ice_dug                      | 459         |\n|    water_produced               | 81.3        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 74.4         |\n| time/                           |              |\n|    fps                          | 835          |\n|    iterations                   | 365          |\n|    time_elapsed                 | 1747         |\n|    total_timesteps              | 1460000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010913487 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.45        |\n|    explained_variance           | 0.431        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 162          |\n|    n_updates                    | 728          |\n|    policy_gradient_loss         | -0.000715    |\n|    value_loss                   | 307          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 118          |\n|    action_queue_updates_total   | 128          |\n|    ice_dug                      | 255          |\n|    water_produced               | 57.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 62.1         |\n| time/                           |              |\n|    fps                          | 835          |\n|    iterations                   | 366          |\n|    time_elapsed                 | 1752         |\n|    total_timesteps              | 1464000      |\n| train/                          |              |\n|    approx_kl                    | 0.0001394096 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.29        |\n|    explained_variance           | 0.447        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 76.5         |\n|    n_updates                    | 730          |\n|    policy_gradient_loss         | -0.000304    |\n|    value_loss                   | 189          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 123          |\n|    action_queue_updates_total   | 127          |\n|    ice_dug                      | 129          |\n|    water_produced               | 27.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 65.5         |\n| time/                           |              |\n|    fps                          | 835          |\n|    iterations                   | 367          |\n|    time_elapsed                 | 1758         |\n|    total_timesteps              | 1468000      |\n| train/                          |              |\n|    approx_kl                    | 0.0029549673 |\n|    clip_fraction                | 0.0136       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.37        |\n|    explained_variance           | 0.421        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 67.7         |\n|    n_updates                    | 732          |\n|    policy_gradient_loss         | -0.000472    |\n|    value_loss                   | 123          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 416          |\n|    water_produced               | 81.8         |\n--------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 68.5           |\n| time/                           |                |\n|    fps                          | 834            |\n|    iterations                   | 368            |\n|    time_elapsed                 | 1763           |\n|    total_timesteps              | 1472000        |\n| train/                          |                |\n|    approx_kl                    | 0.000114817034 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.47          |\n|    explained_variance           | 0.48           |\n|    learning_rate                | 0.0003         |\n|    loss                         | 127            |\n|    n_updates                    | 734            |\n|    policy_gradient_loss         | -9.22e-05      |\n|    value_loss                   | 289            |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 133            |\n|    action_queue_updates_total   | 140            |\n|    ice_dug                      | 397            |\n|    water_produced               | 77.5           |\n----------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 65.7        |\n| time/                           |             |\n|    fps                          | 834         |\n|    iterations                   | 369         |\n|    time_elapsed                 | 1768        |\n|    total_timesteps              | 1476000     |\n| train/                          |             |\n|    approx_kl                    | 0.002678215 |\n|    clip_fraction                | 0.00625     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.4        |\n|    explained_variance           | 0.473       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 94.7        |\n|    n_updates                    | 736         |\n|    policy_gradient_loss         | 2.16e-06    |\n|    value_loss                   | 224         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 129         |\n|    action_queue_updates_total   | 143         |\n|    ice_dug                      | 327         |\n|    water_produced               | 68.8        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 67.5         |\n| time/                           |              |\n|    fps                          | 834          |\n|    iterations                   | 370          |\n|    time_elapsed                 | 1773         |\n|    total_timesteps              | 1480000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002475088 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.45        |\n|    explained_variance           | 0.472        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 71.7         |\n|    n_updates                    | 738          |\n|    policy_gradient_loss         | 0.000381     |\n|    value_loss                   | 208          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 125          |\n|    action_queue_updates_total   | 133          |\n|    ice_dug                      | 401          |\n|    water_produced               | 65.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 70.7          |\n| time/                           |               |\n|    fps                          | 834           |\n|    iterations                   | 371           |\n|    time_elapsed                 | 1777          |\n|    total_timesteps              | 1484000       |\n| train/                          |               |\n|    approx_kl                    | 0.00083621533 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.28         |\n|    explained_variance           | 0.462         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 89.1          |\n|    n_updates                    | 740           |\n|    policy_gradient_loss         | -0.00068      |\n|    value_loss                   | 185           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 127           |\n|    ice_dug                      | 189           |\n|    water_produced               | 43            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 62.4         |\n| time/                           |              |\n|    fps                          | 834          |\n|    iterations                   | 372          |\n|    time_elapsed                 | 1783         |\n|    total_timesteps              | 1488000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011812802 |\n|    clip_fraction                | 0.00675      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.3         |\n|    explained_variance           | 0.424        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 62.7         |\n|    n_updates                    | 742          |\n|    policy_gradient_loss         | -0.000737    |\n|    value_loss                   | 145          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 248          |\n|    water_produced               | 41.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 55.3         |\n| time/                           |              |\n|    fps                          | 834          |\n|    iterations                   | 373          |\n|    time_elapsed                 | 1788         |\n|    total_timesteps              | 1492000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007848259 |\n|    clip_fraction                | 0.00275      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.41        |\n|    explained_variance           | 0.477        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 48           |\n|    n_updates                    | 744          |\n|    policy_gradient_loss         | -0.000524    |\n|    value_loss                   | 134          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 233          |\n|    water_produced               | 44           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 47.5         |\n| time/                           |              |\n|    fps                          | 834          |\n|    iterations                   | 374          |\n|    time_elapsed                 | 1793         |\n|    total_timesteps              | 1496000      |\n| train/                          |              |\n|    approx_kl                    | 0.0015707914 |\n|    clip_fraction                | 0.00288      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.45        |\n|    explained_variance           | 0.453        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 69.8         |\n|    n_updates                    | 746          |\n|    policy_gradient_loss         | -0.000135    |\n|    value_loss                   | 158          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 137          |\n|    water_produced               | 31.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 53.1         |\n| time/                           |              |\n|    fps                          | 834          |\n|    iterations                   | 375          |\n|    time_elapsed                 | 1797         |\n|    total_timesteps              | 1500000      |\n| train/                          |              |\n|    approx_kl                    | 0.0036763034 |\n|    clip_fraction                | 0.0166       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.51         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 42.8         |\n|    n_updates                    | 748          |\n|    policy_gradient_loss         | -0.000443    |\n|    value_loss                   | 82.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 148          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 394          |\n|    water_produced               | 93.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 55.1         |\n| time/                           |              |\n|    fps                          | 834          |\n|    iterations                   | 376          |\n|    time_elapsed                 | 1802         |\n|    total_timesteps              | 1504000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007597172 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.51        |\n|    explained_variance           | 0.516        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 135          |\n|    n_updates                    | 750          |\n|    policy_gradient_loss         | -0.000129    |\n|    value_loss                   | 250          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 422          |\n|    water_produced               | 50.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 48.9         |\n| time/                           |              |\n|    fps                          | 834          |\n|    iterations                   | 377          |\n|    time_elapsed                 | 1807         |\n|    total_timesteps              | 1508000      |\n| train/                          |              |\n|    approx_kl                    | 0.0015540597 |\n|    clip_fraction                | 0.000375     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.56        |\n|    explained_variance           | 0.497        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 70.1         |\n|    n_updates                    | 752          |\n|    policy_gradient_loss         | -0.000361    |\n|    value_loss                   | 152          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 65           |\n|    water_produced               | 12.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 51.8         |\n| time/                           |              |\n|    fps                          | 834          |\n|    iterations                   | 378          |\n|    time_elapsed                 | 1812         |\n|    total_timesteps              | 1512000      |\n| train/                          |              |\n|    approx_kl                    | 0.0051081693 |\n|    clip_fraction                | 0.0294       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.584        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.6         |\n|    n_updates                    | 754          |\n|    policy_gradient_loss         | 0.000933     |\n|    value_loss                   | 27.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 330          |\n|    water_produced               | 57.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 63            |\n| time/                           |               |\n|    fps                          | 834           |\n|    iterations                   | 379           |\n|    time_elapsed                 | 1817          |\n|    total_timesteps              | 1516000       |\n| train/                          |               |\n|    approx_kl                    | 0.00017399379 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.535         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 89.7          |\n|    n_updates                    | 756           |\n|    policy_gradient_loss         | -0.000171     |\n|    value_loss                   | 179           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 139           |\n|    ice_dug                      | 390           |\n|    water_produced               | 84.2          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 60.9        |\n| time/                           |             |\n|    fps                          | 833         |\n|    iterations                   | 380         |\n|    time_elapsed                 | 1822        |\n|    total_timesteps              | 1520000     |\n| train/                          |             |\n|    approx_kl                    | 0.004943449 |\n|    clip_fraction                | 0.0246      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.35       |\n|    explained_variance           | 0.47        |\n|    learning_rate                | 0.0003      |\n|    loss                         | 99.5        |\n|    n_updates                    | 758         |\n|    policy_gradient_loss         | 0.00129     |\n|    value_loss                   | 230         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 129         |\n|    action_queue_updates_total   | 136         |\n|    ice_dug                      | 465         |\n|    water_produced               | 82.8        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 50.8        |\n| time/                           |             |\n|    fps                          | 833         |\n|    iterations                   | 381         |\n|    time_elapsed                 | 1827        |\n|    total_timesteps              | 1524000     |\n| train/                          |             |\n|    approx_kl                    | 0.002567702 |\n|    clip_fraction                | 0.00988     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.32       |\n|    explained_variance           | 0.486       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 115         |\n|    n_updates                    | 760         |\n|    policy_gradient_loss         | -0.000433   |\n|    value_loss                   | 229         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 127         |\n|    action_queue_updates_total   | 137         |\n|    ice_dug                      | 17          |\n|    water_produced               | 4           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 61.8         |\n| time/                           |              |\n|    fps                          | 833          |\n|    iterations                   | 382          |\n|    time_elapsed                 | 1832         |\n|    total_timesteps              | 1528000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011406463 |\n|    clip_fraction                | 0.00412      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.5         |\n|    explained_variance           | 0.537        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 5.79         |\n|    n_updates                    | 762          |\n|    policy_gradient_loss         | -0.000714    |\n|    value_loss                   | 17.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 284          |\n|    water_produced               | 65.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 54.7         |\n| time/                           |              |\n|    fps                          | 833          |\n|    iterations                   | 383          |\n|    time_elapsed                 | 1837         |\n|    total_timesteps              | 1532000      |\n| train/                          |              |\n|    approx_kl                    | 0.0014206026 |\n|    clip_fraction                | 0.0045       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.51        |\n|    explained_variance           | 0.453        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 109          |\n|    n_updates                    | 764          |\n|    policy_gradient_loss         | -0.000753    |\n|    value_loss                   | 254          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 102          |\n|    water_produced               | 24.7         |\n--------------------------------------------------\nEval num_timesteps=1536000, episode_reward=148.20 +/- 180.83\nEpisode length: 442.00 +/- 173.76\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 442          |\n|    mean_reward                  | 148          |\n| time/                           |              |\n|    total_timesteps              | 1536000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005775105 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.522        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 34.6         |\n|    n_updates                    | 766          |\n|    policy_gradient_loss         | -0.000153    |\n|    value_loss                   | 69.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 144          |\n|    action_queue_updates_total   | 159          |\n|    ice_dug                      | 172          |\n|    water_produced               | 42.8         |\n--------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 46       |\n| time/              |          |\n|    fps             | 831      |\n|    iterations      | 384      |\n|    time_elapsed    | 1848     |\n|    total_timesteps | 1536000  |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 35           |\n| time/                           |              |\n|    fps                          | 831          |\n|    iterations                   | 385          |\n|    time_elapsed                 | 1853         |\n|    total_timesteps              | 1540000      |\n| train/                          |              |\n|    approx_kl                    | 0.0022543606 |\n|    clip_fraction                | 0.001        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.84        |\n|    explained_variance           | 0.557        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 57.2         |\n|    n_updates                    | 768          |\n|    policy_gradient_loss         | 0.00069      |\n|    value_loss                   | 117          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 267          |\n|    water_produced               | 29.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 46.6         |\n| time/                           |              |\n|    fps                          | 830          |\n|    iterations                   | 386          |\n|    time_elapsed                 | 1858         |\n|    total_timesteps              | 1544000      |\n| train/                          |              |\n|    approx_kl                    | 0.0012980937 |\n|    clip_fraction                | 0.001        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.521        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 45.9         |\n|    n_updates                    | 770          |\n|    policy_gradient_loss         | 0.000118     |\n|    value_loss                   | 99.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 151          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 246          |\n|    water_produced               | 59.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 40.9         |\n| time/                           |              |\n|    fps                          | 830          |\n|    iterations                   | 387          |\n|    time_elapsed                 | 1863         |\n|    total_timesteps              | 1548000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007145946 |\n|    clip_fraction                | 0.00175      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.8         |\n|    explained_variance           | 0.534        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 95.4         |\n|    n_updates                    | 772          |\n|    policy_gradient_loss         | 0.000478     |\n|    value_loss                   | 190          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 148          |\n|    action_queue_updates_total   | 165          |\n|    ice_dug                      | 331          |\n|    water_produced               | 36.5         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 41.6        |\n| time/                           |             |\n|    fps                          | 830         |\n|    iterations                   | 388         |\n|    time_elapsed                 | 1867        |\n|    total_timesteps              | 1552000     |\n| train/                          |             |\n|    approx_kl                    | 0.001006267 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.84       |\n|    explained_variance           | 0.537       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 62.9        |\n|    n_updates                    | 774         |\n|    policy_gradient_loss         | 0.000207    |\n|    value_loss                   | 122         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 139         |\n|    action_queue_updates_total   | 144         |\n|    ice_dug                      | 158         |\n|    water_produced               | 27.5        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 43.3         |\n| time/                           |              |\n|    fps                          | 830          |\n|    iterations                   | 389          |\n|    time_elapsed                 | 1872         |\n|    total_timesteps              | 1556000      |\n| train/                          |              |\n|    approx_kl                    | 0.0034312743 |\n|    clip_fraction                | 0.0119       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.544        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 39.5         |\n|    n_updates                    | 776          |\n|    policy_gradient_loss         | -0.000533    |\n|    value_loss                   | 67.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 155          |\n|    ice_dug                      | 247          |\n|    water_produced               | 50.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 50.3          |\n| time/                           |               |\n|    fps                          | 830           |\n|    iterations                   | 390           |\n|    time_elapsed                 | 1878          |\n|    total_timesteps              | 1560000       |\n| train/                          |               |\n|    approx_kl                    | 0.00073186384 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.538         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 92.2          |\n|    n_updates                    | 778           |\n|    policy_gradient_loss         | 0.000255      |\n|    value_loss                   | 157           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 292           |\n|    water_produced               | 64.5          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 52.2        |\n| time/                           |             |\n|    fps                          | 830         |\n|    iterations                   | 391         |\n|    time_elapsed                 | 1883        |\n|    total_timesteps              | 1564000     |\n| train/                          |             |\n|    approx_kl                    | 0.002189424 |\n|    clip_fraction                | 0.00388     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.6        |\n|    explained_variance           | 0.566       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 103         |\n|    n_updates                    | 780         |\n|    policy_gradient_loss         | -0.000697   |\n|    value_loss                   | 177         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 148         |\n|    action_queue_updates_total   | 154         |\n|    ice_dug                      | 326         |\n|    water_produced               | 68.2        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 61.1        |\n| time/                           |             |\n|    fps                          | 830         |\n|    iterations                   | 392         |\n|    time_elapsed                 | 1888        |\n|    total_timesteps              | 1568000     |\n| train/                          |             |\n|    approx_kl                    | 0.001144874 |\n|    clip_fraction                | 0.000375    |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.64       |\n|    explained_variance           | 0.549       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 89.8        |\n|    n_updates                    | 782         |\n|    policy_gradient_loss         | -9.89e-05   |\n|    value_loss                   | 204         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 138         |\n|    action_queue_updates_total   | 145         |\n|    ice_dug                      | 439         |\n|    water_produced               | 80.3        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 61.7         |\n| time/                           |              |\n|    fps                          | 830          |\n|    iterations                   | 393          |\n|    time_elapsed                 | 1893         |\n|    total_timesteps              | 1572000      |\n| train/                          |              |\n|    approx_kl                    | 0.0023118933 |\n|    clip_fraction                | 0.00625      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.48        |\n|    explained_variance           | 0.525        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 123          |\n|    n_updates                    | 784          |\n|    policy_gradient_loss         | -0.000882    |\n|    value_loss                   | 231          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 152          |\n|    water_produced               | 30.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 68.8          |\n| time/                           |               |\n|    fps                          | 830           |\n|    iterations                   | 394           |\n|    time_elapsed                 | 1898          |\n|    total_timesteps              | 1576000       |\n| train/                          |               |\n|    approx_kl                    | 0.00097276305 |\n|    clip_fraction                | 0.0005        |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.503         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 68.6          |\n|    n_updates                    | 786           |\n|    policy_gradient_loss         | -0.000294     |\n|    value_loss                   | 106           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 364           |\n|    water_produced               | 85            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 59.5         |\n| time/                           |              |\n|    fps                          | 830          |\n|    iterations                   | 395          |\n|    time_elapsed                 | 1903         |\n|    total_timesteps              | 1580000      |\n| train/                          |              |\n|    approx_kl                    | 0.0001081898 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.586        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 113          |\n|    n_updates                    | 788          |\n|    policy_gradient_loss         | -3e-05       |\n|    value_loss                   | 224          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 91           |\n|    water_produced               | 20.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 55.3          |\n| time/                           |               |\n|    fps                          | 829           |\n|    iterations                   | 396           |\n|    time_elapsed                 | 1909          |\n|    total_timesteps              | 1584000       |\n| train/                          |               |\n|    approx_kl                    | 0.00019883896 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.492         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 39.1          |\n|    n_updates                    | 790           |\n|    policy_gradient_loss         | 5.21e-05      |\n|    value_loss                   | 87.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 206           |\n|    water_produced               | 48            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 59.2         |\n| time/                           |              |\n|    fps                          | 829          |\n|    iterations                   | 397          |\n|    time_elapsed                 | 1913         |\n|    total_timesteps              | 1588000      |\n| train/                          |              |\n|    approx_kl                    | 0.0020753383 |\n|    clip_fraction                | 0.0045       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.538        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 70.2         |\n|    n_updates                    | 792          |\n|    policy_gradient_loss         | 0.000274     |\n|    value_loss                   | 153          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 471          |\n|    water_produced               | 99.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 68.6         |\n| time/                           |              |\n|    fps                          | 829          |\n|    iterations                   | 398          |\n|    time_elapsed                 | 1918         |\n|    total_timesteps              | 1592000      |\n| train/                          |              |\n|    approx_kl                    | 0.0043610996 |\n|    clip_fraction                | 0.0207       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.53        |\n|    explained_variance           | 0.548        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 141          |\n|    n_updates                    | 794          |\n|    policy_gradient_loss         | 0.00124      |\n|    value_loss                   | 286          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 147          |\n|    ice_dug                      | 315          |\n|    water_produced               | 75.7         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 64.5         |\n| time/                           |              |\n|    fps                          | 829          |\n|    iterations                   | 399          |\n|    time_elapsed                 | 1923         |\n|    total_timesteps              | 1596000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007994406 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.47        |\n|    explained_variance           | 0.529        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 116          |\n|    n_updates                    | 796          |\n|    policy_gradient_loss         | -5.19e-05    |\n|    value_loss                   | 226          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 285          |\n|    water_produced               | 65.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 71.2          |\n| time/                           |               |\n|    fps                          | 830           |\n|    iterations                   | 400           |\n|    time_elapsed                 | 1927          |\n|    total_timesteps              | 1600000       |\n| train/                          |               |\n|    approx_kl                    | 0.00010545462 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.57         |\n|    explained_variance           | 0.499         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 133           |\n|    n_updates                    | 798           |\n|    policy_gradient_loss         | -0.000163     |\n|    value_loss                   | 217           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 145           |\n|    ice_dug                      | 348           |\n|    water_produced               | 51.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 73.5          |\n| time/                           |               |\n|    fps                          | 830           |\n|    iterations                   | 401           |\n|    time_elapsed                 | 1932          |\n|    total_timesteps              | 1604000       |\n| train/                          |               |\n|    approx_kl                    | 0.00010798282 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.51         |\n|    explained_variance           | 0.521         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 58            |\n|    n_updates                    | 800           |\n|    policy_gradient_loss         | -1.56e-05     |\n|    value_loss                   | 144           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 279           |\n|    water_produced               | 58.7          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 74.6          |\n| time/                           |               |\n|    fps                          | 829           |\n|    iterations                   | 402           |\n|    time_elapsed                 | 1937          |\n|    total_timesteps              | 1608000       |\n| train/                          |               |\n|    approx_kl                    | 0.00044857958 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.54         |\n|    explained_variance           | 0.554         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 83.7          |\n|    n_updates                    | 802           |\n|    policy_gradient_loss         | 0.000212      |\n|    value_loss                   | 142           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 142           |\n|    ice_dug                      | 517           |\n|    water_produced               | 104           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 69           |\n| time/                           |              |\n|    fps                          | 829          |\n|    iterations                   | 403          |\n|    time_elapsed                 | 1942         |\n|    total_timesteps              | 1612000      |\n| train/                          |              |\n|    approx_kl                    | 0.0042897267 |\n|    clip_fraction                | 0.0211       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.36        |\n|    explained_variance           | 0.471        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 150          |\n|    n_updates                    | 804          |\n|    policy_gradient_loss         | -0.000508    |\n|    value_loss                   | 309          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 147          |\n|    ice_dug                      | 274          |\n|    water_produced               | 48.2         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 66.5        |\n| time/                           |             |\n|    fps                          | 829         |\n|    iterations                   | 404         |\n|    time_elapsed                 | 1947        |\n|    total_timesteps              | 1616000     |\n| train/                          |             |\n|    approx_kl                    | 0.000497521 |\n|    clip_fraction                | 0.00075     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.46       |\n|    explained_variance           | 0.473       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 84.3        |\n|    n_updates                    | 806         |\n|    policy_gradient_loss         | -0.000273   |\n|    value_loss                   | 164         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 132         |\n|    action_queue_updates_total   | 139         |\n|    ice_dug                      | 253         |\n|    water_produced               | 53.5        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 67.4        |\n| time/                           |             |\n|    fps                          | 829         |\n|    iterations                   | 405         |\n|    time_elapsed                 | 1952        |\n|    total_timesteps              | 1620000     |\n| train/                          |             |\n|    approx_kl                    | 0.003651478 |\n|    clip_fraction                | 0.0194      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.44       |\n|    explained_variance           | 0.476       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 69.6        |\n|    n_updates                    | 808         |\n|    policy_gradient_loss         | 5.27e-05    |\n|    value_loss                   | 153         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 140         |\n|    action_queue_updates_total   | 147         |\n|    ice_dug                      | 271         |\n|    water_produced               | 56.2        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 81.1          |\n| time/                           |               |\n|    fps                          | 829           |\n|    iterations                   | 406           |\n|    time_elapsed                 | 1957          |\n|    total_timesteps              | 1624000       |\n| train/                          |               |\n|    approx_kl                    | 0.00018070725 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.52         |\n|    explained_variance           | 0.509         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 69.5          |\n|    n_updates                    | 810           |\n|    policy_gradient_loss         | -0.000265     |\n|    value_loss                   | 153           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 652           |\n|    water_produced               | 124           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 76.9          |\n| time/                           |               |\n|    fps                          | 829           |\n|    iterations                   | 407           |\n|    time_elapsed                 | 1962          |\n|    total_timesteps              | 1628000       |\n| train/                          |               |\n|    approx_kl                    | 0.00078471424 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.48         |\n|    explained_variance           | 0.523         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 183           |\n|    n_updates                    | 812           |\n|    policy_gradient_loss         | -0.000334     |\n|    value_loss                   | 325           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 417           |\n|    water_produced               | 84            |\n---------------------------------------------------\nEval num_timesteps=1632000, episode_reward=133.24 +/- 205.73\nEpisode length: 428.00 +/- 197.52\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 428          |\n|    mean_reward                  | 133          |\n| time/                           |              |\n|    total_timesteps              | 1632000      |\n| train/                          |              |\n|    approx_kl                    | 0.0036631408 |\n|    clip_fraction                | 0.0126       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.43        |\n|    explained_variance           | 0.518        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 111          |\n|    n_updates                    | 814          |\n|    policy_gradient_loss         | 0.00101      |\n|    value_loss                   | 217          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 138          |\n|    ice_dug                      | 141          |\n|    water_produced               | 21.8         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 71.3     |\n| time/              |          |\n|    fps             | 827      |\n|    iterations      | 408      |\n|    time_elapsed    | 1971     |\n|    total_timesteps | 1632000  |\n---------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 72.6        |\n| time/                           |             |\n|    fps                          | 827         |\n|    iterations                   | 409         |\n|    time_elapsed                 | 1976        |\n|    total_timesteps              | 1636000     |\n| train/                          |             |\n|    approx_kl                    | 0.004190961 |\n|    clip_fraction                | 0.0254      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.38       |\n|    explained_variance           | 0.537       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 29.6        |\n|    n_updates                    | 816         |\n|    policy_gradient_loss         | -0.0011     |\n|    value_loss                   | 67.5        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 144         |\n|    action_queue_updates_total   | 152         |\n|    ice_dug                      | 279         |\n|    water_produced               | 59.8        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 80.9          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 410           |\n|    time_elapsed                 | 1981          |\n|    total_timesteps              | 1640000       |\n| train/                          |               |\n|    approx_kl                    | 0.00051521737 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.56         |\n|    explained_variance           | 0.546         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 95            |\n|    n_updates                    | 818           |\n|    policy_gradient_loss         | 9.86e-05      |\n|    value_loss                   | 182           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 150           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 457           |\n|    water_produced               | 96            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 63.5         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 411          |\n|    time_elapsed                 | 1986         |\n|    total_timesteps              | 1644000      |\n| train/                          |              |\n|    approx_kl                    | 0.0015219627 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.5         |\n|    explained_variance           | 0.52         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 151          |\n|    n_updates                    | 820          |\n|    policy_gradient_loss         | 0.000269     |\n|    value_loss                   | 287          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 188          |\n|    water_produced               | 41.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 60.7         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 412          |\n|    time_elapsed                 | 1991         |\n|    total_timesteps              | 1648000      |\n| train/                          |              |\n|    approx_kl                    | 0.0035028257 |\n|    clip_fraction                | 0.0134       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.57        |\n|    explained_variance           | 0.521        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 64.9         |\n|    n_updates                    | 822          |\n|    policy_gradient_loss         | 0.000154     |\n|    value_loss                   | 122          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 326          |\n|    water_produced               | 70.7         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 73.9          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 413           |\n|    time_elapsed                 | 1996          |\n|    total_timesteps              | 1652000       |\n| train/                          |               |\n|    approx_kl                    | 0.00025205294 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.563         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 116           |\n|    n_updates                    | 824           |\n|    policy_gradient_loss         | 0.000153      |\n|    value_loss                   | 193           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 367           |\n|    water_produced               | 85.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 69.4          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 414           |\n|    time_elapsed                 | 2001          |\n|    total_timesteps              | 1656000       |\n| train/                          |               |\n|    approx_kl                    | 0.00068352977 |\n|    clip_fraction                | 0.000125      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.53         |\n|    explained_variance           | 0.582         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 99            |\n|    n_updates                    | 826           |\n|    policy_gradient_loss         | 0.000217      |\n|    value_loss                   | 218           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 139           |\n|    ice_dug                      | 209           |\n|    water_produced               | 38            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 60.1          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 415           |\n|    time_elapsed                 | 2006          |\n|    total_timesteps              | 1660000       |\n| train/                          |               |\n|    approx_kl                    | 0.00059676403 |\n|    clip_fraction                | 0.001         |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.43         |\n|    explained_variance           | 0.545         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 48            |\n|    n_updates                    | 828           |\n|    policy_gradient_loss         | -7.39e-05     |\n|    value_loss                   | 97.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 301           |\n|    water_produced               | 51.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 69.8          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 416           |\n|    time_elapsed                 | 2011          |\n|    total_timesteps              | 1664000       |\n| train/                          |               |\n|    approx_kl                    | 0.00035647216 |\n|    clip_fraction                | 0.001         |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.47         |\n|    explained_variance           | 0.532         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 71.6          |\n|    n_updates                    | 830           |\n|    policy_gradient_loss         | -0.000322     |\n|    value_loss                   | 141           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 158           |\n|    ice_dug                      | 497           |\n|    water_produced               | 86.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 62.3         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 417          |\n|    time_elapsed                 | 2016         |\n|    total_timesteps              | 1668000      |\n| train/                          |              |\n|    approx_kl                    | 0.0032437718 |\n|    clip_fraction                | 0.00762      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.577        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 94.7         |\n|    n_updates                    | 832          |\n|    policy_gradient_loss         | -0.000675    |\n|    value_loss                   | 232          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 201          |\n|    water_produced               | 34.8         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 63.5        |\n| time/                           |             |\n|    fps                          | 827         |\n|    iterations                   | 418         |\n|    time_elapsed                 | 2021        |\n|    total_timesteps              | 1672000     |\n| train/                          |             |\n|    approx_kl                    | 0.001079922 |\n|    clip_fraction                | 0.00138     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.5        |\n|    explained_variance           | 0.577       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 44.2        |\n|    n_updates                    | 834         |\n|    policy_gradient_loss         | -0.00025    |\n|    value_loss                   | 107         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 138         |\n|    action_queue_updates_total   | 144         |\n|    ice_dug                      | 406         |\n|    water_produced               | 91          |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 68.9          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 419           |\n|    time_elapsed                 | 2026          |\n|    total_timesteps              | 1676000       |\n| train/                          |               |\n|    approx_kl                    | 0.00015473705 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.41         |\n|    explained_variance           | 0.542         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 138           |\n|    n_updates                    | 836           |\n|    policy_gradient_loss         | -0.000189     |\n|    value_loss                   | 244           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 284           |\n|    water_produced               | 64.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 83.8          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 420           |\n|    time_elapsed                 | 2031          |\n|    total_timesteps              | 1680000       |\n| train/                          |               |\n|    approx_kl                    | 0.00018151515 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.57         |\n|    explained_variance           | 0.562         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 104           |\n|    n_updates                    | 838           |\n|    policy_gradient_loss         | 0.000307      |\n|    value_loss                   | 188           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 606           |\n|    water_produced               | 123           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 75.1         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 421          |\n|    time_elapsed                 | 2036         |\n|    total_timesteps              | 1684000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006172243 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.46        |\n|    explained_variance           | 0.521        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 166          |\n|    n_updates                    | 840          |\n|    policy_gradient_loss         | -0.000672    |\n|    value_loss                   | 362          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 260          |\n|    water_produced               | 45           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 81           |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 422          |\n|    time_elapsed                 | 2041         |\n|    total_timesteps              | 1688000      |\n| train/                          |              |\n|    approx_kl                    | 8.720688e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.47        |\n|    explained_variance           | 0.504        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 116          |\n|    n_updates                    | 842          |\n|    policy_gradient_loss         | 7.55e-05     |\n|    value_loss                   | 186          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 297          |\n|    water_produced               | 63.3         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 68.7         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 423          |\n|    time_elapsed                 | 2046         |\n|    total_timesteps              | 1692000      |\n| train/                          |              |\n|    approx_kl                    | 0.0012443729 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.55        |\n|    explained_variance           | 0.545        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 93.1         |\n|    n_updates                    | 844          |\n|    policy_gradient_loss         | -0.00114     |\n|    value_loss                   | 181          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 189          |\n|    water_produced               | 31.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 59.6         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 424          |\n|    time_elapsed                 | 2051         |\n|    total_timesteps              | 1696000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010985421 |\n|    clip_fraction                | 0.00163      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.556        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 49.9         |\n|    n_updates                    | 846          |\n|    policy_gradient_loss         | -0.00045     |\n|    value_loss                   | 99.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 155          |\n|    ice_dug                      | 184          |\n|    water_produced               | 19.7         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 66.8        |\n| time/                           |             |\n|    fps                          | 826         |\n|    iterations                   | 425         |\n|    time_elapsed                 | 2055        |\n|    total_timesteps              | 1700000     |\n| train/                          |             |\n|    approx_kl                    | 0.005727559 |\n|    clip_fraction                | 0.0261      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.7        |\n|    explained_variance           | 0.566       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 37.4        |\n|    n_updates                    | 848         |\n|    policy_gradient_loss         | 0.000242    |\n|    value_loss                   | 72.5        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 154         |\n|    action_queue_updates_total   | 159         |\n|    ice_dug                      | 682         |\n|    water_produced               | 158         |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 73.8         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 426          |\n|    time_elapsed                 | 2060         |\n|    total_timesteps              | 1704000      |\n| train/                          |              |\n|    approx_kl                    | 0.0020301088 |\n|    clip_fraction                | 0.00425      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.49        |\n|    explained_variance           | 0.59         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 200          |\n|    n_updates                    | 850          |\n|    policy_gradient_loss         | 0.000347     |\n|    value_loss                   | 420          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 477          |\n|    water_produced               | 77.8         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 70.3        |\n| time/                           |             |\n|    fps                          | 826         |\n|    iterations                   | 427         |\n|    time_elapsed                 | 2066        |\n|    total_timesteps              | 1708000     |\n| train/                          |             |\n|    approx_kl                    | 0.004778543 |\n|    clip_fraction                | 0.0247      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.42       |\n|    explained_variance           | 0.538       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 109         |\n|    n_updates                    | 852         |\n|    policy_gradient_loss         | -0.000212   |\n|    value_loss                   | 197         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 140         |\n|    action_queue_updates_total   | 146         |\n|    ice_dug                      | 271         |\n|    water_produced               | 46          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 69.6         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 428          |\n|    time_elapsed                 | 2071         |\n|    total_timesteps              | 1712000      |\n| train/                          |              |\n|    approx_kl                    | 0.0016137175 |\n|    clip_fraction                | 0.00563      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.45        |\n|    explained_variance           | 0.517        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 86.7         |\n|    n_updates                    | 854          |\n|    policy_gradient_loss         | -0.00033     |\n|    value_loss                   | 140          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 191          |\n|    water_produced               | 28.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 82.5         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 429          |\n|    time_elapsed                 | 2076         |\n|    total_timesteps              | 1716000      |\n| train/                          |              |\n|    approx_kl                    | 0.0033905834 |\n|    clip_fraction                | 0.0171       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.54        |\n|    explained_variance           | 0.579        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 65.5         |\n|    n_updates                    | 856          |\n|    policy_gradient_loss         | 0.00019      |\n|    value_loss                   | 94.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 153          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 365          |\n|    water_produced               | 82.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 66           |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 430          |\n|    time_elapsed                 | 2081         |\n|    total_timesteps              | 1720000      |\n| train/                          |              |\n|    approx_kl                    | 0.0014947278 |\n|    clip_fraction                | 0.001        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.592        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 119          |\n|    n_updates                    | 858          |\n|    policy_gradient_loss         | 1.67e-06     |\n|    value_loss                   | 222          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 150          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 335          |\n|    water_produced               | 79.3         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 62.7         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 431          |\n|    time_elapsed                 | 2085         |\n|    total_timesteps              | 1724000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007636421 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.49        |\n|    explained_variance           | 0.544        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 92.2         |\n|    n_updates                    | 860          |\n|    policy_gradient_loss         | 0.000152     |\n|    value_loss                   | 207          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 284          |\n|    water_produced               | 63           |\n--------------------------------------------------\nEval num_timesteps=1728000, episode_reward=69.88 +/- 83.42\nEpisode length: 367.00 +/- 81.08\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 367          |\n|    mean_reward                  | 69.9         |\n| time/                           |              |\n|    total_timesteps              | 1728000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004504029 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.53        |\n|    explained_variance           | 0.495        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 89.1         |\n|    n_updates                    | 862          |\n|    policy_gradient_loss         | -0.000301    |\n|    value_loss                   | 199          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 361          |\n|    water_produced               | 77           |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 69.1     |\n| time/              |          |\n|    fps             | 825      |\n|    iterations      | 432      |\n|    time_elapsed    | 2093     |\n|    total_timesteps | 1728000  |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 83.1         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 433          |\n|    time_elapsed                 | 2098         |\n|    total_timesteps              | 1732000      |\n| train/                          |              |\n|    approx_kl                    | 0.0013612735 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.46        |\n|    explained_variance           | 0.546        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 87.7         |\n|    n_updates                    | 864          |\n|    policy_gradient_loss         | 0.000347     |\n|    value_loss                   | 182          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 143          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 409          |\n|    water_produced               | 96.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 79.3          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 434           |\n|    time_elapsed                 | 2103          |\n|    total_timesteps              | 1736000       |\n| train/                          |               |\n|    approx_kl                    | 0.00043552456 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.49         |\n|    explained_variance           | 0.539         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 146           |\n|    n_updates                    | 866           |\n|    policy_gradient_loss         | 0.000361      |\n|    value_loss                   | 276           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 282           |\n|    water_produced               | 64.3          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 81.1          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 435           |\n|    time_elapsed                 | 2107          |\n|    total_timesteps              | 1740000       |\n| train/                          |               |\n|    approx_kl                    | 0.00030206534 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.48         |\n|    explained_variance           | 0.521         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 85.3          |\n|    n_updates                    | 868           |\n|    policy_gradient_loss         | 0.000212      |\n|    value_loss                   | 170           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 387           |\n|    water_produced               | 87.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 79.5          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 436           |\n|    time_elapsed                 | 2112          |\n|    total_timesteps              | 1744000       |\n| train/                          |               |\n|    approx_kl                    | 0.00037216608 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.55         |\n|    explained_variance           | 0.54          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 121           |\n|    n_updates                    | 870           |\n|    policy_gradient_loss         | -3.02e-05     |\n|    value_loss                   | 260           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 257           |\n|    water_produced               | 55.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 82.9         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 437          |\n|    time_elapsed                 | 2117         |\n|    total_timesteps              | 1748000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010058836 |\n|    clip_fraction                | 0.0005       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.52        |\n|    explained_variance           | 0.545        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 81.2         |\n|    n_updates                    | 872          |\n|    policy_gradient_loss         | -0.000306    |\n|    value_loss                   | 171          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 525          |\n|    water_produced               | 92.3         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 77.2          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 438           |\n|    time_elapsed                 | 2122          |\n|    total_timesteps              | 1752000       |\n| train/                          |               |\n|    approx_kl                    | 0.00043419338 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.48         |\n|    explained_variance           | 0.512         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 155           |\n|    n_updates                    | 874           |\n|    policy_gradient_loss         | 0.000169      |\n|    value_loss                   | 294           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 404           |\n|    water_produced               | 68            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 78.8          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 439           |\n|    time_elapsed                 | 2127          |\n|    total_timesteps              | 1756000       |\n| train/                          |               |\n|    approx_kl                    | 0.00019644527 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.49         |\n|    explained_variance           | 0.524         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 114           |\n|    n_updates                    | 876           |\n|    policy_gradient_loss         | 0.000132      |\n|    value_loss                   | 227           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 367           |\n|    water_produced               | 71.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 76.7          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 440           |\n|    time_elapsed                 | 2132          |\n|    total_timesteps              | 1760000       |\n| train/                          |               |\n|    approx_kl                    | 0.00020844644 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.45         |\n|    explained_variance           | 0.559         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 86.2          |\n|    n_updates                    | 878           |\n|    policy_gradient_loss         | -9.26e-05     |\n|    value_loss                   | 175           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 400           |\n|    water_produced               | 77            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 95.7          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 441           |\n|    time_elapsed                 | 2137          |\n|    total_timesteps              | 1764000       |\n| train/                          |               |\n|    approx_kl                    | 0.00032559113 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.42         |\n|    explained_variance           | 0.533         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 109           |\n|    n_updates                    | 880           |\n|    policy_gradient_loss         | -0.000255     |\n|    value_loss                   | 206           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 693           |\n|    water_produced               | 146           |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 78.4        |\n| time/                           |             |\n|    fps                          | 825         |\n|    iterations                   | 442         |\n|    time_elapsed                 | 2142        |\n|    total_timesteps              | 1768000     |\n| train/                          |             |\n|    approx_kl                    | 0.002587312 |\n|    clip_fraction                | 0.00987     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.29       |\n|    explained_variance           | 0.539       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 187         |\n|    n_updates                    | 882         |\n|    policy_gradient_loss         | 0.000409    |\n|    value_loss                   | 379         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 138         |\n|    action_queue_updates_total   | 146         |\n|    ice_dug                      | 84          |\n|    water_produced               | 10.2        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 83.7          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 443           |\n|    time_elapsed                 | 2147          |\n|    total_timesteps              | 1772000       |\n| train/                          |               |\n|    approx_kl                    | 0.00018191138 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.42         |\n|    explained_variance           | 0.668         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 24.5          |\n|    n_updates                    | 884           |\n|    policy_gradient_loss         | 0.000452      |\n|    value_loss                   | 55.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 413           |\n|    water_produced               | 94            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 85           |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 444          |\n|    time_elapsed                 | 2152         |\n|    total_timesteps              | 1776000      |\n| train/                          |              |\n|    approx_kl                    | 0.0009189613 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.4         |\n|    explained_variance           | 0.559        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 151          |\n|    n_updates                    | 886          |\n|    policy_gradient_loss         | -0.00119     |\n|    value_loss                   | 279          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 441          |\n|    water_produced               | 77.3         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 74.2         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 445          |\n|    time_elapsed                 | 2157         |\n|    total_timesteps              | 1780000      |\n| train/                          |              |\n|    approx_kl                    | 0.0018954424 |\n|    clip_fraction                | 0.00162      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.41        |\n|    explained_variance           | 0.515        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 111          |\n|    n_updates                    | 888          |\n|    policy_gradient_loss         | 0.000834     |\n|    value_loss                   | 211          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 144          |\n|    ice_dug                      | 119          |\n|    water_produced               | 26           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 67.3         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 446          |\n|    time_elapsed                 | 2162         |\n|    total_timesteps              | 1784000      |\n| train/                          |              |\n|    approx_kl                    | 0.0025546248 |\n|    clip_fraction                | 0.0106       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.47        |\n|    explained_variance           | 0.576        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 31.9         |\n|    n_updates                    | 890          |\n|    policy_gradient_loss         | -0.0011      |\n|    value_loss                   | 73.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 144          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 576          |\n|    water_produced               | 112          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 80.5          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 447           |\n|    time_elapsed                 | 2167          |\n|    total_timesteps              | 1788000       |\n| train/                          |               |\n|    approx_kl                    | 0.00011663725 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.34         |\n|    explained_variance           | 0.483         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 153           |\n|    n_updates                    | 892           |\n|    policy_gradient_loss         | -0.000155     |\n|    value_loss                   | 334           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 412           |\n|    water_produced               | 73.3          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 76.2          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 448           |\n|    time_elapsed                 | 2171          |\n|    total_timesteps              | 1792000       |\n| train/                          |               |\n|    approx_kl                    | 0.00039348943 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.47         |\n|    explained_variance           | 0.505         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 127           |\n|    n_updates                    | 894           |\n|    policy_gradient_loss         | 8.15e-05      |\n|    value_loss                   | 228           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 150           |\n|    action_queue_updates_total   | 160           |\n|    ice_dug                      | 399           |\n|    water_produced               | 72.8          |\n---------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 76.9           |\n| time/                           |                |\n|    fps                          | 825            |\n|    iterations                   | 449            |\n|    time_elapsed                 | 2176           |\n|    total_timesteps              | 1796000        |\n| train/                          |                |\n|    approx_kl                    | 0.000120795776 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.5           |\n|    explained_variance           | 0.561          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 105            |\n|    n_updates                    | 896            |\n|    policy_gradient_loss         | 0.00012        |\n|    value_loss                   | 193            |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 146            |\n|    action_queue_updates_total   | 156            |\n|    ice_dug                      | 452            |\n|    water_produced               | 80.5           |\n----------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 82.1          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 450           |\n|    time_elapsed                 | 2181          |\n|    total_timesteps              | 1800000       |\n| train/                          |               |\n|    approx_kl                    | 0.00026793292 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.43         |\n|    explained_variance           | 0.533         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 116           |\n|    n_updates                    | 898           |\n|    policy_gradient_loss         | 0.000474      |\n|    value_loss                   | 223           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 141           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 271           |\n|    water_produced               | 50.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 68           |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 451          |\n|    time_elapsed                 | 2186         |\n|    total_timesteps              | 1804000      |\n| train/                          |              |\n|    approx_kl                    | 0.0036859326 |\n|    clip_fraction                | 0.0171       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.55        |\n|    explained_variance           | 0.631        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 67           |\n|    n_updates                    | 900          |\n|    policy_gradient_loss         | -0.000209    |\n|    value_loss                   | 179          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 324          |\n|    water_produced               | 44.3         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 64.3         |\n| time/                           |              |\n|    fps                          | 824          |\n|    iterations                   | 452          |\n|    time_elapsed                 | 2191         |\n|    total_timesteps              | 1808000      |\n| train/                          |              |\n|    approx_kl                    | 0.0018166773 |\n|    clip_fraction                | 0.00225      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.51        |\n|    explained_variance           | 0.572        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 74.9         |\n|    n_updates                    | 902          |\n|    policy_gradient_loss         | 0.000275     |\n|    value_loss                   | 150          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 308          |\n|    water_produced               | 55.7         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 59.1          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 453           |\n|    time_elapsed                 | 2196          |\n|    total_timesteps              | 1812000       |\n| train/                          |               |\n|    approx_kl                    | 0.00060997525 |\n|    clip_fraction                | 0.00025       |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.56         |\n|    explained_variance           | 0.489         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 75.4          |\n|    n_updates                    | 904           |\n|    policy_gradient_loss         | -2.32e-05     |\n|    value_loss                   | 146           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 350           |\n|    water_produced               | 47.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 57            |\n| time/                           |               |\n|    fps                          | 824           |\n|    iterations                   | 454           |\n|    time_elapsed                 | 2201          |\n|    total_timesteps              | 1816000       |\n| train/                          |               |\n|    approx_kl                    | 0.00084881595 |\n|    clip_fraction                | 0.000625      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.62         |\n|    explained_variance           | 0.549         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 83.7          |\n|    n_updates                    | 906           |\n|    policy_gradient_loss         | 0.000132      |\n|    value_loss                   | 161           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 161           |\n|    ice_dug                      | 330           |\n|    water_produced               | 71.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 66.1          |\n| time/                           |               |\n|    fps                          | 824           |\n|    iterations                   | 455           |\n|    time_elapsed                 | 2206          |\n|    total_timesteps              | 1820000       |\n| train/                          |               |\n|    approx_kl                    | 0.00041178885 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.665         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 87.2          |\n|    n_updates                    | 908           |\n|    policy_gradient_loss         | -0.000216     |\n|    value_loss                   | 191           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 160           |\n|    ice_dug                      | 499           |\n|    water_produced               | 94            |\n---------------------------------------------------\nEval num_timesteps=1824000, episode_reward=136.60 +/- 165.94\nEpisode length: 430.00 +/- 158.06\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 430          |\n|    mean_reward                  | 137          |\n| time/                           |              |\n|    total_timesteps              | 1824000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010346528 |\n|    clip_fraction                | 0.00175      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.6         |\n|    explained_variance           | 0.597        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 120          |\n|    n_updates                    | 910          |\n|    policy_gradient_loss         | 0.000347     |\n|    value_loss                   | 253          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 155          |\n|    ice_dug                      | 221          |\n|    water_produced               | 45.3         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 66.1     |\n| time/              |          |\n|    fps             | 823      |\n|    iterations      | 456      |\n|    time_elapsed    | 2215     |\n|    total_timesteps | 1824000  |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 70.9          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 457           |\n|    time_elapsed                 | 2220          |\n|    total_timesteps              | 1828000       |\n| train/                          |               |\n|    approx_kl                    | 0.00016431455 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.555         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 57.1          |\n|    n_updates                    | 912           |\n|    policy_gradient_loss         | -0.000173     |\n|    value_loss                   | 121           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 147           |\n|    ice_dug                      | 385           |\n|    water_produced               | 79            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 71.1          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 458           |\n|    time_elapsed                 | 2224          |\n|    total_timesteps              | 1832000       |\n| train/                          |               |\n|    approx_kl                    | 0.00034296457 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.45         |\n|    explained_variance           | 0.535         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 98.2          |\n|    n_updates                    | 914           |\n|    policy_gradient_loss         | -2.82e-05     |\n|    value_loss                   | 245           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 240           |\n|    water_produced               | 49.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 73.4         |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 459          |\n|    time_elapsed                 | 2229         |\n|    total_timesteps              | 1836000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005815922 |\n|    clip_fraction                | 0.001        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.52        |\n|    explained_variance           | 0.551        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 65.2         |\n|    n_updates                    | 916          |\n|    policy_gradient_loss         | -0.000518    |\n|    value_loss                   | 136          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 143          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 435          |\n|    water_produced               | 81.7         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 68           |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 460          |\n|    time_elapsed                 | 2234         |\n|    total_timesteps              | 1840000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011707498 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.47        |\n|    explained_variance           | 0.581        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 127          |\n|    n_updates                    | 918          |\n|    policy_gradient_loss         | -1.23e-05    |\n|    value_loss                   | 213          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 148          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 349          |\n|    water_produced               | 68.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 74.4          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 461           |\n|    time_elapsed                 | 2239          |\n|    total_timesteps              | 1844000       |\n| train/                          |               |\n|    approx_kl                    | 0.00027483905 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.48         |\n|    explained_variance           | 0.533         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 114           |\n|    n_updates                    | 920           |\n|    policy_gradient_loss         | -0.000133     |\n|    value_loss                   | 223           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 384           |\n|    water_produced               | 76            |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 79.2        |\n| time/                           |             |\n|    fps                          | 823         |\n|    iterations                   | 462         |\n|    time_elapsed                 | 2243        |\n|    total_timesteps              | 1848000     |\n| train/                          |             |\n|    approx_kl                    | 0.000451012 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.54       |\n|    explained_variance           | 0.547       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 96.5        |\n|    n_updates                    | 922         |\n|    policy_gradient_loss         | -0.000174   |\n|    value_loss                   | 237         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 150         |\n|    action_queue_updates_total   | 162         |\n|    ice_dug                      | 519         |\n|    water_produced               | 101         |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 92          |\n| time/                           |             |\n|    fps                          | 823         |\n|    iterations                   | 463         |\n|    time_elapsed                 | 2248        |\n|    total_timesteps              | 1852000     |\n| train/                          |             |\n|    approx_kl                    | 0.001407727 |\n|    clip_fraction                | 0.00162     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.56       |\n|    explained_variance           | 0.558       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 128         |\n|    n_updates                    | 924         |\n|    policy_gradient_loss         | -0.000251   |\n|    value_loss                   | 262         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 151         |\n|    action_queue_updates_total   | 160         |\n|    ice_dug                      | 584         |\n|    water_produced               | 110         |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 86.3         |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 464          |\n|    time_elapsed                 | 2253         |\n|    total_timesteps              | 1856000      |\n| train/                          |              |\n|    approx_kl                    | 0.0042222636 |\n|    clip_fraction                | 0.0174       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.48        |\n|    explained_variance           | 0.571        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 146          |\n|    n_updates                    | 926          |\n|    policy_gradient_loss         | 0.00146      |\n|    value_loss                   | 292          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 138          |\n|    ice_dug                      | 232          |\n|    water_produced               | 55.3         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 83.1         |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 465          |\n|    time_elapsed                 | 2258         |\n|    total_timesteps              | 1860000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011341423 |\n|    clip_fraction                | 0.00263      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.3         |\n|    explained_variance           | 0.473        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 65.1         |\n|    n_updates                    | 928          |\n|    policy_gradient_loss         | 0.000123     |\n|    value_loss                   | 150          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 144          |\n|    ice_dug                      | 279          |\n|    water_produced               | 53           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 85.1         |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 466          |\n|    time_elapsed                 | 2262         |\n|    total_timesteps              | 1864000      |\n| train/                          |              |\n|    approx_kl                    | 0.0064335777 |\n|    clip_fraction                | 0.0404       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.44        |\n|    explained_variance           | 0.522        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 99.3         |\n|    n_updates                    | 930          |\n|    policy_gradient_loss         | -7.53e-05    |\n|    value_loss                   | 164          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 149          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 422          |\n|    water_produced               | 85.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 72.2         |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 467          |\n|    time_elapsed                 | 2268         |\n|    total_timesteps              | 1868000      |\n| train/                          |              |\n|    approx_kl                    | 0.0009797883 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.53        |\n|    explained_variance           | 0.629        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 113          |\n|    n_updates                    | 932          |\n|    policy_gradient_loss         | -0.000218    |\n|    value_loss                   | 222          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 183          |\n|    water_produced               | 40.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 61.5         |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 468          |\n|    time_elapsed                 | 2272         |\n|    total_timesteps              | 1872000      |\n| train/                          |              |\n|    approx_kl                    | 0.0033152208 |\n|    clip_fraction                | 0.0155       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.53        |\n|    explained_variance           | 0.556        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 84.4         |\n|    n_updates                    | 934          |\n|    policy_gradient_loss         | 0.000314     |\n|    value_loss                   | 145          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 149          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 268          |\n|    water_produced               | 59.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 66.8         |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 469          |\n|    time_elapsed                 | 2277         |\n|    total_timesteps              | 1876000      |\n| train/                          |              |\n|    approx_kl                    | 0.0009402173 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.685        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 96.5         |\n|    n_updates                    | 936          |\n|    policy_gradient_loss         | -0.000678    |\n|    value_loss                   | 175          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 149          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 330          |\n|    water_produced               | 80.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 72.7          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 470           |\n|    time_elapsed                 | 2282          |\n|    total_timesteps              | 1880000       |\n| train/                          |               |\n|    approx_kl                    | 0.00041009142 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.613         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 115           |\n|    n_updates                    | 938           |\n|    policy_gradient_loss         | 9.32e-05      |\n|    value_loss                   | 241           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 155           |\n|    action_queue_updates_total   | 167           |\n|    ice_dug                      | 440           |\n|    water_produced               | 81            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 64.8          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 471           |\n|    time_elapsed                 | 2287          |\n|    total_timesteps              | 1884000       |\n| train/                          |               |\n|    approx_kl                    | 0.00075543846 |\n|    clip_fraction                | 0.000375      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.57          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 139           |\n|    n_updates                    | 940           |\n|    policy_gradient_loss         | 3.84e-06      |\n|    value_loss                   | 270           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 219           |\n|    water_produced               | 48            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 71.4          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 472           |\n|    time_elapsed                 | 2292          |\n|    total_timesteps              | 1888000       |\n| train/                          |               |\n|    approx_kl                    | 0.00024510163 |\n|    clip_fraction                | 0.000125      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.5          |\n|    explained_variance           | 0.527         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 74.1          |\n|    n_updates                    | 942           |\n|    policy_gradient_loss         | 2.64e-05      |\n|    value_loss                   | 146           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 158           |\n|    action_queue_updates_total   | 162           |\n|    ice_dug                      | 453           |\n|    water_produced               | 70.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 70            |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 473           |\n|    time_elapsed                 | 2297          |\n|    total_timesteps              | 1892000       |\n| train/                          |               |\n|    approx_kl                    | 0.00032139808 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.562         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 95.4          |\n|    n_updates                    | 944           |\n|    policy_gradient_loss         | -8.95e-05     |\n|    value_loss                   | 209           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 277           |\n|    water_produced               | 52.7          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 70.6          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 474           |\n|    time_elapsed                 | 2302          |\n|    total_timesteps              | 1896000       |\n| train/                          |               |\n|    approx_kl                    | 0.00019400133 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.568         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 73.1          |\n|    n_updates                    | 946           |\n|    policy_gradient_loss         | 4.46e-05      |\n|    value_loss                   | 152           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 376           |\n|    water_produced               | 83            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 59.4          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 475           |\n|    time_elapsed                 | 2306          |\n|    total_timesteps              | 1900000       |\n| train/                          |               |\n|    approx_kl                    | 0.00017442036 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.5          |\n|    explained_variance           | 0.541         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 124           |\n|    n_updates                    | 948           |\n|    policy_gradient_loss         | 0.000144      |\n|    value_loss                   | 227           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 141           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 187           |\n|    water_produced               | 27.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 60.5          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 476           |\n|    time_elapsed                 | 2311          |\n|    total_timesteps              | 1904000       |\n| train/                          |               |\n|    approx_kl                    | 0.00016221762 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.573         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 51.2          |\n|    n_updates                    | 950           |\n|    policy_gradient_loss         | -4.32e-05     |\n|    value_loss                   | 111           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 310           |\n|    water_produced               | 52.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 64.4         |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 477          |\n|    time_elapsed                 | 2317         |\n|    total_timesteps              | 1908000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010714286 |\n|    clip_fraction                | 0.001        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.49        |\n|    explained_variance           | 0.538        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 87           |\n|    n_updates                    | 952          |\n|    policy_gradient_loss         | -0.000912    |\n|    value_loss                   | 190          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 153          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 434          |\n|    water_produced               | 90.2         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 63.1        |\n| time/                           |             |\n|    fps                          | 823         |\n|    iterations                   | 478         |\n|    time_elapsed                 | 2321        |\n|    total_timesteps              | 1912000     |\n| train/                          |             |\n|    approx_kl                    | 0.005110504 |\n|    clip_fraction                | 0.0207      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.57       |\n|    explained_variance           | 0.608       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 124         |\n|    n_updates                    | 954         |\n|    policy_gradient_loss         | 0.00151     |\n|    value_loss                   | 225         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 145         |\n|    action_queue_updates_total   | 152         |\n|    ice_dug                      | 304         |\n|    water_produced               | 46          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 75           |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 479          |\n|    time_elapsed                 | 2326         |\n|    total_timesteps              | 1916000      |\n| train/                          |              |\n|    approx_kl                    | 0.0016839042 |\n|    clip_fraction                | 0.00213      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.54        |\n|    explained_variance           | 0.547        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 70.8         |\n|    n_updates                    | 956          |\n|    policy_gradient_loss         | -0.00066     |\n|    value_loss                   | 146          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 615          |\n|    water_produced               | 140          |\n--------------------------------------------------\nEval num_timesteps=1920000, episode_reward=322.60 +/- 369.76\nEpisode length: 573.80 +/- 301.74\n-------------------------------------------------\n| eval/                           |             |\n|    mean_ep_length               | 574         |\n|    mean_reward                  | 323         |\n| time/                           |             |\n|    total_timesteps              | 1920000     |\n| train/                          |             |\n|    approx_kl                    | 0.002281469 |\n|    clip_fraction                | 0.00512     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.41       |\n|    explained_variance           | 0.542       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 214         |\n|    n_updates                    | 958         |\n|    policy_gradient_loss         | -0.000138   |\n|    value_loss                   | 428         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 148         |\n|    action_queue_updates_total   | 154         |\n|    ice_dug                      | 301         |\n|    water_produced               | 31          |\n-------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 76       |\n| time/              |          |\n|    fps             | 821      |\n|    iterations      | 480      |\n|    time_elapsed    | 2336     |\n|    total_timesteps | 1920000  |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 91.3         |\n| time/                           |              |\n|    fps                          | 821          |\n|    iterations                   | 481          |\n|    time_elapsed                 | 2341         |\n|    total_timesteps              | 1924000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002494093 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.46        |\n|    explained_variance           | 0.487        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 66.3         |\n|    n_updates                    | 960          |\n|    policy_gradient_loss         | 0.000605     |\n|    value_loss                   | 142          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 147          |\n|    ice_dug                      | 597          |\n|    water_produced               | 126          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 82.5          |\n| time/                           |               |\n|    fps                          | 821           |\n|    iterations                   | 482           |\n|    time_elapsed                 | 2346          |\n|    total_timesteps              | 1928000       |\n| train/                          |               |\n|    approx_kl                    | 0.00020361264 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.3          |\n|    explained_variance           | 0.54          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 161           |\n|    n_updates                    | 962           |\n|    policy_gradient_loss         | -0.000398     |\n|    value_loss                   | 347           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 369           |\n|    water_produced               | 47            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 83.5          |\n| time/                           |               |\n|    fps                          | 821           |\n|    iterations                   | 483           |\n|    time_elapsed                 | 2351          |\n|    total_timesteps              | 1932000       |\n| train/                          |               |\n|    approx_kl                    | 0.00012013786 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.39         |\n|    explained_variance           | 0.482         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 75.2          |\n|    n_updates                    | 964           |\n|    policy_gradient_loss         | -1.85e-05     |\n|    value_loss                   | 174           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 141           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 255           |\n|    water_produced               | 51.8          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 67.8        |\n| time/                           |             |\n|    fps                          | 821         |\n|    iterations                   | 484         |\n|    time_elapsed                 | 2356        |\n|    total_timesteps              | 1936000     |\n| train/                          |             |\n|    approx_kl                    | 0.006209714 |\n|    clip_fraction                | 0.0354      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.51       |\n|    explained_variance           | 0.578       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 95.3        |\n|    n_updates                    | 966         |\n|    policy_gradient_loss         | 0.000136    |\n|    value_loss                   | 184         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 151         |\n|    action_queue_updates_total   | 159         |\n|    ice_dug                      | 352         |\n|    water_produced               | 64.5        |\n-------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 75.2           |\n| time/                           |                |\n|    fps                          | 821            |\n|    iterations                   | 485            |\n|    time_elapsed                 | 2361           |\n|    total_timesteps              | 1940000        |\n| train/                          |                |\n|    approx_kl                    | 0.000117454445 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.53          |\n|    explained_variance           | 0.604          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 91.3           |\n|    n_updates                    | 968            |\n|    policy_gradient_loss         | 1.14e-05       |\n|    value_loss                   | 171            |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 151            |\n|    action_queue_updates_total   | 159            |\n|    ice_dug                      | 404            |\n|    water_produced               | 66.7           |\n----------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 67.4          |\n| time/                           |               |\n|    fps                          | 821           |\n|    iterations                   | 486           |\n|    time_elapsed                 | 2366          |\n|    total_timesteps              | 1944000       |\n| train/                          |               |\n|    approx_kl                    | 0.00013222871 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.56         |\n|    explained_variance           | 0.578         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 82.5          |\n|    n_updates                    | 970           |\n|    policy_gradient_loss         | -5.87e-05     |\n|    value_loss                   | 198           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 156           |\n|    action_queue_updates_total   | 166           |\n|    ice_dug                      | 422           |\n|    water_produced               | 89            |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 69.3        |\n| time/                           |             |\n|    fps                          | 821         |\n|    iterations                   | 487         |\n|    time_elapsed                 | 2371        |\n|    total_timesteps              | 1948000     |\n| train/                          |             |\n|    approx_kl                    | 0.001270597 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.61       |\n|    explained_variance           | 0.561       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 173         |\n|    n_updates                    | 972         |\n|    policy_gradient_loss         | -4.4e-05    |\n|    value_loss                   | 314         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 144         |\n|    action_queue_updates_total   | 151         |\n|    ice_dug                      | 308         |\n|    water_produced               | 57          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 75.8         |\n| time/                           |              |\n|    fps                          | 821          |\n|    iterations                   | 488          |\n|    time_elapsed                 | 2376         |\n|    total_timesteps              | 1952000      |\n| train/                          |              |\n|    approx_kl                    | 0.0021535524 |\n|    clip_fraction                | 0.005        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.43        |\n|    explained_variance           | 0.501        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 93.6         |\n|    n_updates                    | 974          |\n|    policy_gradient_loss         | 0.000245     |\n|    value_loss                   | 193          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 401          |\n|    water_produced               | 82.8         |\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Process ForkServerProcess-5:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-8:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-6:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-2:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-3:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-1:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-7:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-4:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 30, in _worker\n    observation, reward, done, info = env.step(data)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/monitor.py\", line 94, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/wrappers/time_limit.py\", line 18, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/tmp/ipykernel_27/2513648194.py\", line 23, in step\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/tmp/ipykernel_27/3552070691.py\", line 95, in step\n  File \"/opt/conda/lib/python3.7/site-packages/gym/wrappers/order_enforcing.py\", line 11, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/luxai_s2/env.py\", line 985, in step\n    self.state.board.lichen_strains, strain_ids\n  File \"<__array_function__ internals>\", line 6, in isin\n  File \"/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py\", line 736, in isin\n    invert=invert).reshape(element.shape)\n  File \"<__array_function__ internals>\", line 6, in in1d\n  File \"/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py\", line 608, in in1d\n    mask |= (ar1 == a)\nKeyboardInterrupt\n","output_type":"stream"},{"name":"stdout","text":"pygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\npygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\npygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\npygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\npygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\npygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\npygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\npygame 2.3.0 (SDL 2.24.2, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3377558824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.learn(\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensorboardCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_metrics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/latest_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Packaging and Submission\n\nWe now have a trained policy. In order to make it submittable to the competition we recommend you write code on separate files and only use kaggle notebooks for training as it can get very messy to program an RL agent just using a Kaggle notebook interface. The starter kit that was downloaded earlier has all of the code above written already and organized into separate files and folders. The observation wrapper and controller written here are saved to the `wrappers` folder. The SB3Wrapper is not in the kit, but is a part of the official luxai_s2 package and you can import it with\n\n```\nfrom luxai_s2.wrappers import SB3Wrapper\n```\n\nThe main file to take note of is `agent.py` which defines your agent's behavior. It will load the policy from`MODEL_WEIGHTS_RELATIVE_PATH` which can be changed at the top of `agent.py`.\n\n`agent.py` also uses the actions_mask function to invalidate some actions so that the policy only generates valid actions, which is a easy way to improve performance.","metadata":{}},{"cell_type":"code","source":"# if running on kaggle, run below to copy the rl starter kit files to the working directory\n!cp -r ../input/luxai-s2-rl-sb3-kit/* .\n!mv best_model.dontunzipme best_model.zip # kaggle auto unzips files but we don't want it to here so we do this","metadata":{"execution":{"iopub.status.busy":"2023-02-01T07:40:23.554448Z","iopub.execute_input":"2023-02-01T07:40:23.555045Z","iopub.status.idle":"2023-02-01T07:40:25.919537Z","shell.execute_reply.started":"2023-02-01T07:40:23.555001Z","shell.execute_reply":"2023-02-01T07:40:25.917314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if you trained an actual agent, copy its model weights here\n!mv logs/exp_1/models/best_model.zip best_model.zip","metadata":{"execution":{"iopub.status.busy":"2023-02-01T07:40:25.921667Z","iopub.execute_input":"2023-02-01T07:40:25.922141Z","iopub.status.idle":"2023-02-01T07:40:27.045959Z","shell.execute_reply.started":"2023-02-01T07:40:25.9221Z","shell.execute_reply":"2023-02-01T07:40:27.044353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nTo submit your trained agent create a .tar.gz file. You can download the submission.tar.gz file from the right and submit it to the competition directly.","metadata":{}},{"cell_type":"code","source":"!tar -cvzf submission.tar.gz *","metadata":{"execution":{"iopub.status.busy":"2023-02-01T07:40:27.048011Z","iopub.execute_input":"2023-02-01T07:40:27.048719Z","iopub.status.idle":"2023-02-01T07:40:28.328749Z","shell.execute_reply.started":"2023-02-01T07:40:27.048677Z","shell.execute_reply":"2023-02-01T07:40:28.326997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tips for Improving your Agent\n\nThis tutorial agent will train a policy that can efficiently control a single heavy robot that learns to pickup power, constantly dig ice, and transfer ice back to the factory and survive the full 1000 turns in the game. A simple improvement would be to add lichen planting to the action space / controller or program it directly as a rule in the agent.py file, allowing you to score points by the end of the game as well as generate more power.\n\nAnother easy idea is to modify the `agent.py` code so that you spawn multiple factories and multiple heavy robots, and simply run the trained policy on each heavy robot.\n\n\nIf you want to look into more scalable solutions, it's critical to first figure out how to model multiple units at once. This kit shows you how to control a single heavy robot effectively but not multiple. Another thing to consider is what observations and features would be the most useful. Finally, you can always try and develop a more complex action controller in addition to developing better reward functions.\n\nIf you feel you are experienced enough, you can take a look at [last season's winning solution by team Toad Brigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) or [our paper: Emergent collective intelligence from massive-agent cooperation and competition](https://arxiv.org/abs/2301.01609) which show how to use convolutional neural nets and various other techniques (e.g. invalid action masking) to control a massive number of units at once.","metadata":{}}]}